{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "teste.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OTIfXkXWRqQzB5bjk_szR3q8lbAmTdz9",
      "authorship_tag": "ABX9TyNa5Lre4WmlA/+rpwyhM2BY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustavoGatti/BreastCancerSiameseNeuralNetwork/blob/main/teste.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rghciu6x_Cfq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "0ccd68a7-2785-4398-cf7d-ea4c7aa6e088"
      },
      "source": [
        "!pip install tensorflow-gpu==2.2.0-rc0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.2.0-rc0 in /usr/local/lib/python3.6/dist-packages (2.2.0rc0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (1.15.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (1.4.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (0.10.0)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (2.1.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (0.35.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (2.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.2.0-rc0) (3.12.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (50.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (2.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (3.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.2.0-rc0) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D6IRaT-_Gag"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv1D, MaxPool1D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import ndarray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXVrBGMFhJAO"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpD9YPUOHbUx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "febb9a4c-e9ad-47ff-d580-dc47b76fadb1"
      },
      "source": [
        "cancer = pd.read_csv('/content/drive/My Drive/Colab Notebooks/dataR234.csv')\n",
        "x = pd.DataFrame(data = cancer)\n",
        "x.drop('Classification', inplace=True, axis=1)\n",
        "aux = x.iloc[21]\n",
        "#aux = np.asarray(aux)\n",
        "aux"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Age             34.000000\n",
              "BMI             31.975015\n",
              "Glucose         87.000000\n",
              "Insulin          4.530000\n",
              "HOMA             0.972138\n",
              "Leptin          28.750200\n",
              "Adiponectin      7.642760\n",
              "Resistin         5.625920\n",
              "MCP.1          572.783000\n",
              "Name: 21, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22wKX9G5HklF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "51b73cab-c5a8-4231-cc34-f59dd6d6b408"
      },
      "source": [
        "y = cancer.Classification\n",
        "y\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "111    1\n",
              "112    1\n",
              "113    1\n",
              "114    1\n",
              "115    1\n",
              "Name: Classification, Length: 116, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sdnaSx6H6Y3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4bc0679e-ec3c-43fd-85b7-c8fbf70aeaf2"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0, stratify = y)\n",
        "x_train = np.asarray(x_train)\n",
        "y_train = np.asarray(y_train)\n",
        "x_test = np.asarray(x_test)\n",
        "y_test = np.asarray(y_test)\n",
        "test_isolado = x_test[2]\n",
        "print(test_isolado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 78.        25.3       60.         3.508      0.519184   6.633\n",
            "  10.567295   4.6638   209.749   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klCG3YeAIGae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "28d07e46-b2d1-4e24-8949-613f8cfa761a"
      },
      "source": [
        "print(x_test[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 78.        25.3       60.         3.508      0.519184   6.633\n",
            "  10.567295   4.6638   209.749   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVyCv0ZwIRwx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e15602b-456f-4a22-a592-4db4c6da84d2"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwZKSCZWItiy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "885d5276-04af-4e98-b6d2-ba2bd5975f61"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "print(x_test[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.28105086e+00 -4.28229801e-01 -1.82290405e+00 -6.06770235e-01\n",
            " -5.44698050e-01 -1.02439202e+00 -5.72133378e-04 -8.29524843e-01\n",
            " -9.21193560e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HobCmmb6IxP7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "437741e5-6e65-4c46-85c1-c309709ea2e3"
      },
      "source": [
        "x_train = x_train.reshape(92,9,1)\n",
        "x_test = x_test.reshape(24,9,1)\n",
        "\n",
        "x_test[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.28105086e+00],\n",
              "       [-4.28229801e-01],\n",
              "       [-1.82290405e+00],\n",
              "       [-6.06770235e-01],\n",
              "       [-5.44698050e-01],\n",
              "       [-1.02439202e+00],\n",
              "       [-5.72133378e-04],\n",
              "       [-8.29524843e-01],\n",
              "       [-9.21193560e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBK4DOa9I4Es",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8d592f03-b82c-401a-f74e-244d212f90f4"
      },
      "source": [
        "'''\n",
        "epochs = 50\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=32, kernel_size=2, activation = 'relu', input_shape = (30,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nepochs = 50\\nmodel = Sequential()\\nmodel.add(Conv1D(filters=32, kernel_size=2, activation = 'relu', input_shape = (30,1)))\\nmodel.add(BatchNormalization())\\nmodel.add(Dropout(0.2))\\n\\n\\nmodel.add(Conv1D(filters=64, kernel_size=2, activation = 'relu'))\\nmodel.add(BatchNormalization())\\nmodel.add(Dropout(0.5))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(64, activation = 'relu'))\\nmodel.add(Dropout(0.5))\\n\\nmodel.add(Dense(1, activation = 'sigmoid'))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJhdS57gXMAS"
      },
      "source": [
        "epochs = 200\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=32, kernel_size=2, activation = 'relu', input_shape = (9,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(2, activation = 'softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFxNEL03JM27"
      },
      "source": [
        "#model.compile(optimizer=Adam(lr=0.00005), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYggw63XWya"
      },
      "source": [
        "model.compile(optimizer=Adam(lr=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpnFA3JXJSMh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7713f132-2384-472d-fa53-a4b81c062114"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d61e0f28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d61e0f28>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d61e0f28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d61e0f28>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 2s 20ms/sample - loss: 1.5553 - accuracy: 0.4783 - val_loss: 0.6668 - val_accuracy: 0.7083\n",
            "Epoch 2/200\n",
            "92/92 [==============================] - 0s 351us/sample - loss: 0.9433 - accuracy: 0.5652 - val_loss: 0.6530 - val_accuracy: 0.7917\n",
            "Epoch 3/200\n",
            "92/92 [==============================] - 0s 338us/sample - loss: 1.2895 - accuracy: 0.5652 - val_loss: 0.6383 - val_accuracy: 0.7917\n",
            "Epoch 4/200\n",
            "92/92 [==============================] - 0s 328us/sample - loss: 0.9557 - accuracy: 0.5761 - val_loss: 0.6287 - val_accuracy: 0.7500\n",
            "Epoch 5/200\n",
            "92/92 [==============================] - 0s 327us/sample - loss: 1.2359 - accuracy: 0.5761 - val_loss: 0.6206 - val_accuracy: 0.8750\n",
            "Epoch 6/200\n",
            "92/92 [==============================] - 0s 355us/sample - loss: 0.9301 - accuracy: 0.5870 - val_loss: 0.6147 - val_accuracy: 0.8750\n",
            "Epoch 7/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.9596 - accuracy: 0.6087 - val_loss: 0.6109 - val_accuracy: 0.8333\n",
            "Epoch 8/200\n",
            "92/92 [==============================] - 0s 463us/sample - loss: 0.9716 - accuracy: 0.5870 - val_loss: 0.6077 - val_accuracy: 0.7500\n",
            "Epoch 9/200\n",
            "92/92 [==============================] - 0s 351us/sample - loss: 1.2348 - accuracy: 0.5435 - val_loss: 0.6061 - val_accuracy: 0.7500\n",
            "Epoch 10/200\n",
            "92/92 [==============================] - 0s 328us/sample - loss: 0.9204 - accuracy: 0.6087 - val_loss: 0.6031 - val_accuracy: 0.7500\n",
            "Epoch 11/200\n",
            "92/92 [==============================] - 0s 319us/sample - loss: 0.8615 - accuracy: 0.6630 - val_loss: 0.6007 - val_accuracy: 0.7500\n",
            "Epoch 12/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.7807 - accuracy: 0.7283 - val_loss: 0.5976 - val_accuracy: 0.7500\n",
            "Epoch 13/200\n",
            "92/92 [==============================] - 0s 355us/sample - loss: 0.7977 - accuracy: 0.6413 - val_loss: 0.5933 - val_accuracy: 0.7083\n",
            "Epoch 14/200\n",
            "92/92 [==============================] - 0s 382us/sample - loss: 0.9520 - accuracy: 0.6304 - val_loss: 0.5896 - val_accuracy: 0.7083\n",
            "Epoch 15/200\n",
            "92/92 [==============================] - 0s 373us/sample - loss: 0.7951 - accuracy: 0.6304 - val_loss: 0.5864 - val_accuracy: 0.6667\n",
            "Epoch 16/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.6293 - accuracy: 0.6739 - val_loss: 0.5836 - val_accuracy: 0.6667\n",
            "Epoch 17/200\n",
            "92/92 [==============================] - 0s 346us/sample - loss: 0.7767 - accuracy: 0.6630 - val_loss: 0.5823 - val_accuracy: 0.6667\n",
            "Epoch 18/200\n",
            "92/92 [==============================] - 0s 369us/sample - loss: 0.8202 - accuracy: 0.6848 - val_loss: 0.5824 - val_accuracy: 0.6667\n",
            "Epoch 19/200\n",
            "92/92 [==============================] - 0s 343us/sample - loss: 0.8246 - accuracy: 0.6087 - val_loss: 0.5820 - val_accuracy: 0.6667\n",
            "Epoch 20/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.9463 - accuracy: 0.6630 - val_loss: 0.5818 - val_accuracy: 0.6250\n",
            "Epoch 21/200\n",
            "92/92 [==============================] - 0s 319us/sample - loss: 0.7095 - accuracy: 0.6739 - val_loss: 0.5807 - val_accuracy: 0.6250\n",
            "Epoch 22/200\n",
            "92/92 [==============================] - 0s 341us/sample - loss: 0.5112 - accuracy: 0.7826 - val_loss: 0.5804 - val_accuracy: 0.6250\n",
            "Epoch 23/200\n",
            "92/92 [==============================] - 0s 338us/sample - loss: 0.7436 - accuracy: 0.6196 - val_loss: 0.5797 - val_accuracy: 0.6250\n",
            "Epoch 24/200\n",
            "92/92 [==============================] - 0s 344us/sample - loss: 0.6875 - accuracy: 0.6739 - val_loss: 0.5792 - val_accuracy: 0.6250\n",
            "Epoch 25/200\n",
            "92/92 [==============================] - 0s 308us/sample - loss: 0.7378 - accuracy: 0.6957 - val_loss: 0.5785 - val_accuracy: 0.5833\n",
            "Epoch 26/200\n",
            "92/92 [==============================] - 0s 317us/sample - loss: 0.8657 - accuracy: 0.6196 - val_loss: 0.5761 - val_accuracy: 0.5833\n",
            "Epoch 27/200\n",
            "92/92 [==============================] - 0s 319us/sample - loss: 0.5134 - accuracy: 0.7283 - val_loss: 0.5735 - val_accuracy: 0.5833\n",
            "Epoch 28/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.6879 - accuracy: 0.7283 - val_loss: 0.5705 - val_accuracy: 0.5833\n",
            "Epoch 29/200\n",
            "92/92 [==============================] - 0s 362us/sample - loss: 0.6420 - accuracy: 0.7065 - val_loss: 0.5674 - val_accuracy: 0.5833\n",
            "Epoch 30/200\n",
            "92/92 [==============================] - 0s 351us/sample - loss: 0.6024 - accuracy: 0.7283 - val_loss: 0.5646 - val_accuracy: 0.5833\n",
            "Epoch 31/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.6043 - accuracy: 0.7174 - val_loss: 0.5620 - val_accuracy: 0.5833\n",
            "Epoch 32/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.7921 - accuracy: 0.6630 - val_loss: 0.5602 - val_accuracy: 0.5833\n",
            "Epoch 33/200\n",
            "92/92 [==============================] - 0s 350us/sample - loss: 0.6537 - accuracy: 0.6848 - val_loss: 0.5583 - val_accuracy: 0.5833\n",
            "Epoch 34/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.5993 - accuracy: 0.7174 - val_loss: 0.5569 - val_accuracy: 0.5833\n",
            "Epoch 35/200\n",
            "92/92 [==============================] - 0s 349us/sample - loss: 0.5651 - accuracy: 0.7174 - val_loss: 0.5557 - val_accuracy: 0.5833\n",
            "Epoch 36/200\n",
            "92/92 [==============================] - 0s 330us/sample - loss: 0.7747 - accuracy: 0.7174 - val_loss: 0.5545 - val_accuracy: 0.5833\n",
            "Epoch 37/200\n",
            "92/92 [==============================] - 0s 408us/sample - loss: 0.5528 - accuracy: 0.7174 - val_loss: 0.5551 - val_accuracy: 0.5833\n",
            "Epoch 38/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.7343 - accuracy: 0.7065 - val_loss: 0.5547 - val_accuracy: 0.6250\n",
            "Epoch 39/200\n",
            "92/92 [==============================] - 0s 567us/sample - loss: 0.5971 - accuracy: 0.6848 - val_loss: 0.5545 - val_accuracy: 0.6250\n",
            "Epoch 40/200\n",
            "92/92 [==============================] - 0s 375us/sample - loss: 0.4750 - accuracy: 0.7500 - val_loss: 0.5527 - val_accuracy: 0.6250\n",
            "Epoch 41/200\n",
            "92/92 [==============================] - 0s 368us/sample - loss: 0.5008 - accuracy: 0.7391 - val_loss: 0.5517 - val_accuracy: 0.6250\n",
            "Epoch 42/200\n",
            "92/92 [==============================] - 0s 344us/sample - loss: 0.7499 - accuracy: 0.6957 - val_loss: 0.5498 - val_accuracy: 0.6250\n",
            "Epoch 43/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.7511 - accuracy: 0.6522 - val_loss: 0.5484 - val_accuracy: 0.6250\n",
            "Epoch 44/200\n",
            "92/92 [==============================] - 0s 361us/sample - loss: 0.7353 - accuracy: 0.7717 - val_loss: 0.5459 - val_accuracy: 0.6250\n",
            "Epoch 45/200\n",
            "92/92 [==============================] - 0s 334us/sample - loss: 0.6565 - accuracy: 0.7065 - val_loss: 0.5418 - val_accuracy: 0.6250\n",
            "Epoch 46/200\n",
            "92/92 [==============================] - 0s 360us/sample - loss: 0.5314 - accuracy: 0.7500 - val_loss: 0.5396 - val_accuracy: 0.6250\n",
            "Epoch 47/200\n",
            "92/92 [==============================] - 0s 396us/sample - loss: 0.5832 - accuracy: 0.7935 - val_loss: 0.5371 - val_accuracy: 0.6250\n",
            "Epoch 48/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.5881 - accuracy: 0.7174 - val_loss: 0.5335 - val_accuracy: 0.6250\n",
            "Epoch 49/200\n",
            "92/92 [==============================] - 0s 352us/sample - loss: 0.5865 - accuracy: 0.7500 - val_loss: 0.5295 - val_accuracy: 0.5833\n",
            "Epoch 50/200\n",
            "92/92 [==============================] - 0s 432us/sample - loss: 0.6043 - accuracy: 0.7391 - val_loss: 0.5259 - val_accuracy: 0.5833\n",
            "Epoch 51/200\n",
            "92/92 [==============================] - 0s 362us/sample - loss: 0.6499 - accuracy: 0.7283 - val_loss: 0.5229 - val_accuracy: 0.6250\n",
            "Epoch 52/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.6950 - accuracy: 0.7283 - val_loss: 0.5206 - val_accuracy: 0.6250\n",
            "Epoch 53/200\n",
            "92/92 [==============================] - 0s 343us/sample - loss: 0.7519 - accuracy: 0.7826 - val_loss: 0.5185 - val_accuracy: 0.6250\n",
            "Epoch 54/200\n",
            "92/92 [==============================] - 0s 340us/sample - loss: 0.5867 - accuracy: 0.7717 - val_loss: 0.5156 - val_accuracy: 0.6250\n",
            "Epoch 55/200\n",
            "92/92 [==============================] - 0s 433us/sample - loss: 0.4180 - accuracy: 0.8478 - val_loss: 0.5133 - val_accuracy: 0.6250\n",
            "Epoch 56/200\n",
            "92/92 [==============================] - 0s 389us/sample - loss: 0.5449 - accuracy: 0.7826 - val_loss: 0.5116 - val_accuracy: 0.6250\n",
            "Epoch 57/200\n",
            "92/92 [==============================] - 0s 375us/sample - loss: 0.4368 - accuracy: 0.8043 - val_loss: 0.5110 - val_accuracy: 0.6250\n",
            "Epoch 58/200\n",
            "92/92 [==============================] - 0s 334us/sample - loss: 0.5639 - accuracy: 0.6957 - val_loss: 0.5070 - val_accuracy: 0.6667\n",
            "Epoch 59/200\n",
            "92/92 [==============================] - 0s 335us/sample - loss: 0.6559 - accuracy: 0.6957 - val_loss: 0.5039 - val_accuracy: 0.6667\n",
            "Epoch 60/200\n",
            "92/92 [==============================] - 0s 373us/sample - loss: 0.5080 - accuracy: 0.8043 - val_loss: 0.5000 - val_accuracy: 0.6667\n",
            "Epoch 61/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.5081 - accuracy: 0.7609 - val_loss: 0.4969 - val_accuracy: 0.6667\n",
            "Epoch 62/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.5334 - accuracy: 0.7500 - val_loss: 0.4931 - val_accuracy: 0.6667\n",
            "Epoch 63/200\n",
            "92/92 [==============================] - 0s 355us/sample - loss: 0.5964 - accuracy: 0.6630 - val_loss: 0.4894 - val_accuracy: 0.7083\n",
            "Epoch 64/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.4730 - accuracy: 0.7391 - val_loss: 0.4862 - val_accuracy: 0.7083\n",
            "Epoch 65/200\n",
            "92/92 [==============================] - 0s 348us/sample - loss: 0.6116 - accuracy: 0.7391 - val_loss: 0.4830 - val_accuracy: 0.7083\n",
            "Epoch 66/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.5181 - accuracy: 0.8587 - val_loss: 0.4781 - val_accuracy: 0.7083\n",
            "Epoch 67/200\n",
            "92/92 [==============================] - 0s 436us/sample - loss: 0.4901 - accuracy: 0.7609 - val_loss: 0.4748 - val_accuracy: 0.7083\n",
            "Epoch 68/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.6128 - accuracy: 0.7826 - val_loss: 0.4716 - val_accuracy: 0.7083\n",
            "Epoch 69/200\n",
            "92/92 [==============================] - 0s 330us/sample - loss: 0.4786 - accuracy: 0.8152 - val_loss: 0.4671 - val_accuracy: 0.7083\n",
            "Epoch 70/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.4547 - accuracy: 0.7935 - val_loss: 0.4638 - val_accuracy: 0.7083\n",
            "Epoch 71/200\n",
            "92/92 [==============================] - 0s 412us/sample - loss: 0.5591 - accuracy: 0.7065 - val_loss: 0.4620 - val_accuracy: 0.7083\n",
            "Epoch 72/200\n",
            "92/92 [==============================] - 0s 370us/sample - loss: 0.4027 - accuracy: 0.7826 - val_loss: 0.4605 - val_accuracy: 0.7083\n",
            "Epoch 73/200\n",
            "92/92 [==============================] - 0s 405us/sample - loss: 0.5269 - accuracy: 0.7391 - val_loss: 0.4581 - val_accuracy: 0.7083\n",
            "Epoch 74/200\n",
            "92/92 [==============================] - 0s 341us/sample - loss: 0.4521 - accuracy: 0.7826 - val_loss: 0.4547 - val_accuracy: 0.7083\n",
            "Epoch 75/200\n",
            "92/92 [==============================] - 0s 335us/sample - loss: 0.5859 - accuracy: 0.7935 - val_loss: 0.4517 - val_accuracy: 0.7083\n",
            "Epoch 76/200\n",
            "92/92 [==============================] - 0s 327us/sample - loss: 0.5426 - accuracy: 0.7500 - val_loss: 0.4488 - val_accuracy: 0.7083\n",
            "Epoch 77/200\n",
            "92/92 [==============================] - 0s 328us/sample - loss: 0.4220 - accuracy: 0.7935 - val_loss: 0.4458 - val_accuracy: 0.7083\n",
            "Epoch 78/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.4721 - accuracy: 0.8261 - val_loss: 0.4431 - val_accuracy: 0.7083\n",
            "Epoch 79/200\n",
            "92/92 [==============================] - 0s 364us/sample - loss: 0.5676 - accuracy: 0.8043 - val_loss: 0.4393 - val_accuracy: 0.7083\n",
            "Epoch 80/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.4201 - accuracy: 0.8152 - val_loss: 0.4362 - val_accuracy: 0.7083\n",
            "Epoch 81/200\n",
            "92/92 [==============================] - 0s 441us/sample - loss: 0.5557 - accuracy: 0.7174 - val_loss: 0.4333 - val_accuracy: 0.7083\n",
            "Epoch 82/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.4108 - accuracy: 0.7609 - val_loss: 0.4312 - val_accuracy: 0.7083\n",
            "Epoch 83/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.4658 - accuracy: 0.7609 - val_loss: 0.4283 - val_accuracy: 0.7500\n",
            "Epoch 84/200\n",
            "92/92 [==============================] - 0s 380us/sample - loss: 0.4318 - accuracy: 0.8370 - val_loss: 0.4255 - val_accuracy: 0.7500\n",
            "Epoch 85/200\n",
            "92/92 [==============================] - 0s 361us/sample - loss: 0.4086 - accuracy: 0.8261 - val_loss: 0.4229 - val_accuracy: 0.7500\n",
            "Epoch 86/200\n",
            "92/92 [==============================] - 0s 396us/sample - loss: 0.4296 - accuracy: 0.7935 - val_loss: 0.4202 - val_accuracy: 0.7500\n",
            "Epoch 87/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.5059 - accuracy: 0.8152 - val_loss: 0.4181 - val_accuracy: 0.7500\n",
            "Epoch 88/200\n",
            "92/92 [==============================] - 0s 391us/sample - loss: 0.3081 - accuracy: 0.8696 - val_loss: 0.4145 - val_accuracy: 0.7500\n",
            "Epoch 89/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.4603 - accuracy: 0.8152 - val_loss: 0.4119 - val_accuracy: 0.7500\n",
            "Epoch 90/200\n",
            "92/92 [==============================] - 0s 389us/sample - loss: 0.4478 - accuracy: 0.8152 - val_loss: 0.4092 - val_accuracy: 0.7500\n",
            "Epoch 91/200\n",
            "92/92 [==============================] - 0s 337us/sample - loss: 0.5656 - accuracy: 0.7609 - val_loss: 0.4076 - val_accuracy: 0.7500\n",
            "Epoch 92/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.4054 - accuracy: 0.7935 - val_loss: 0.4037 - val_accuracy: 0.7500\n",
            "Epoch 93/200\n",
            "92/92 [==============================] - 0s 328us/sample - loss: 0.4730 - accuracy: 0.8261 - val_loss: 0.3993 - val_accuracy: 0.7917\n",
            "Epoch 94/200\n",
            "92/92 [==============================] - 0s 358us/sample - loss: 0.5398 - accuracy: 0.7717 - val_loss: 0.3917 - val_accuracy: 0.7917\n",
            "Epoch 95/200\n",
            "92/92 [==============================] - 0s 361us/sample - loss: 0.3773 - accuracy: 0.8478 - val_loss: 0.3850 - val_accuracy: 0.8333\n",
            "Epoch 96/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.3650 - accuracy: 0.8043 - val_loss: 0.3799 - val_accuracy: 0.8333\n",
            "Epoch 97/200\n",
            "92/92 [==============================] - 0s 402us/sample - loss: 0.4365 - accuracy: 0.8043 - val_loss: 0.3756 - val_accuracy: 0.8333\n",
            "Epoch 98/200\n",
            "92/92 [==============================] - 0s 434us/sample - loss: 0.4450 - accuracy: 0.8370 - val_loss: 0.3718 - val_accuracy: 0.8333\n",
            "Epoch 99/200\n",
            "92/92 [==============================] - 0s 409us/sample - loss: 0.5062 - accuracy: 0.7935 - val_loss: 0.3705 - val_accuracy: 0.8333\n",
            "Epoch 100/200\n",
            "92/92 [==============================] - 0s 373us/sample - loss: 0.4638 - accuracy: 0.7717 - val_loss: 0.3693 - val_accuracy: 0.8333\n",
            "Epoch 101/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.4422 - accuracy: 0.7609 - val_loss: 0.3676 - val_accuracy: 0.8333\n",
            "Epoch 102/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.4314 - accuracy: 0.7935 - val_loss: 0.3642 - val_accuracy: 0.8333\n",
            "Epoch 103/200\n",
            "92/92 [==============================] - 0s 352us/sample - loss: 0.5735 - accuracy: 0.7283 - val_loss: 0.3638 - val_accuracy: 0.8333\n",
            "Epoch 104/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.3775 - accuracy: 0.8261 - val_loss: 0.3619 - val_accuracy: 0.8333\n",
            "Epoch 105/200\n",
            "92/92 [==============================] - 0s 386us/sample - loss: 0.3744 - accuracy: 0.8478 - val_loss: 0.3587 - val_accuracy: 0.8333\n",
            "Epoch 106/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.4046 - accuracy: 0.8152 - val_loss: 0.3557 - val_accuracy: 0.8333\n",
            "Epoch 107/200\n",
            "92/92 [==============================] - 0s 394us/sample - loss: 0.3808 - accuracy: 0.8478 - val_loss: 0.3520 - val_accuracy: 0.8333\n",
            "Epoch 108/200\n",
            "92/92 [==============================] - 0s 350us/sample - loss: 0.3333 - accuracy: 0.8913 - val_loss: 0.3469 - val_accuracy: 0.8333\n",
            "Epoch 109/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.3760 - accuracy: 0.8370 - val_loss: 0.3427 - val_accuracy: 0.8333\n",
            "Epoch 110/200\n",
            "92/92 [==============================] - 0s 431us/sample - loss: 0.3332 - accuracy: 0.8370 - val_loss: 0.3385 - val_accuracy: 0.8333\n",
            "Epoch 111/200\n",
            "92/92 [==============================] - 0s 393us/sample - loss: 0.3327 - accuracy: 0.8370 - val_loss: 0.3370 - val_accuracy: 0.8333\n",
            "Epoch 112/200\n",
            "92/92 [==============================] - 0s 377us/sample - loss: 0.3796 - accuracy: 0.8152 - val_loss: 0.3359 - val_accuracy: 0.8750\n",
            "Epoch 113/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.4934 - accuracy: 0.7935 - val_loss: 0.3344 - val_accuracy: 0.8750\n",
            "Epoch 114/200\n",
            "92/92 [==============================] - 0s 355us/sample - loss: 0.3467 - accuracy: 0.8696 - val_loss: 0.3316 - val_accuracy: 0.8750\n",
            "Epoch 115/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.3619 - accuracy: 0.8043 - val_loss: 0.3288 - val_accuracy: 0.8750\n",
            "Epoch 116/200\n",
            "92/92 [==============================] - 0s 369us/sample - loss: 0.3671 - accuracy: 0.8261 - val_loss: 0.3255 - val_accuracy: 0.8750\n",
            "Epoch 117/200\n",
            "92/92 [==============================] - 0s 370us/sample - loss: 0.3787 - accuracy: 0.8261 - val_loss: 0.3218 - val_accuracy: 0.8750\n",
            "Epoch 118/200\n",
            "92/92 [==============================] - 0s 382us/sample - loss: 0.4524 - accuracy: 0.7826 - val_loss: 0.3187 - val_accuracy: 0.8750\n",
            "Epoch 119/200\n",
            "92/92 [==============================] - 0s 389us/sample - loss: 0.3433 - accuracy: 0.8478 - val_loss: 0.3154 - val_accuracy: 0.8333\n",
            "Epoch 120/200\n",
            "92/92 [==============================] - 0s 360us/sample - loss: 0.3644 - accuracy: 0.8478 - val_loss: 0.3134 - val_accuracy: 0.8333\n",
            "Epoch 121/200\n",
            "92/92 [==============================] - 0s 398us/sample - loss: 0.4712 - accuracy: 0.7826 - val_loss: 0.3106 - val_accuracy: 0.8333\n",
            "Epoch 122/200\n",
            "92/92 [==============================] - 0s 358us/sample - loss: 0.4132 - accuracy: 0.8370 - val_loss: 0.3083 - val_accuracy: 0.8333\n",
            "Epoch 123/200\n",
            "92/92 [==============================] - 0s 393us/sample - loss: 0.4182 - accuracy: 0.8587 - val_loss: 0.3052 - val_accuracy: 0.8333\n",
            "Epoch 124/200\n",
            "92/92 [==============================] - 0s 416us/sample - loss: 0.3033 - accuracy: 0.8152 - val_loss: 0.3030 - val_accuracy: 0.8333\n",
            "Epoch 125/200\n",
            "92/92 [==============================] - 0s 450us/sample - loss: 0.3444 - accuracy: 0.8370 - val_loss: 0.3016 - val_accuracy: 0.8750\n",
            "Epoch 126/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.4452 - accuracy: 0.7935 - val_loss: 0.2998 - val_accuracy: 0.8750\n",
            "Epoch 127/200\n",
            "92/92 [==============================] - 0s 382us/sample - loss: 0.3880 - accuracy: 0.8261 - val_loss: 0.2977 - val_accuracy: 0.8750\n",
            "Epoch 128/200\n",
            "92/92 [==============================] - 0s 360us/sample - loss: 0.3789 - accuracy: 0.8478 - val_loss: 0.2957 - val_accuracy: 0.8750\n",
            "Epoch 129/200\n",
            "92/92 [==============================] - 0s 411us/sample - loss: 0.4055 - accuracy: 0.8370 - val_loss: 0.2940 - val_accuracy: 0.8750\n",
            "Epoch 130/200\n",
            "92/92 [==============================] - 0s 418us/sample - loss: 0.3679 - accuracy: 0.8587 - val_loss: 0.2927 - val_accuracy: 0.8750\n",
            "Epoch 131/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.3494 - accuracy: 0.8152 - val_loss: 0.2910 - val_accuracy: 0.8750\n",
            "Epoch 132/200\n",
            "92/92 [==============================] - 0s 370us/sample - loss: 0.4115 - accuracy: 0.8043 - val_loss: 0.2887 - val_accuracy: 0.8750\n",
            "Epoch 133/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.4179 - accuracy: 0.8152 - val_loss: 0.2858 - val_accuracy: 0.9167\n",
            "Epoch 134/200\n",
            "92/92 [==============================] - 0s 405us/sample - loss: 0.3901 - accuracy: 0.8261 - val_loss: 0.2832 - val_accuracy: 0.9167\n",
            "Epoch 135/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.4460 - accuracy: 0.8370 - val_loss: 0.2819 - val_accuracy: 0.9167\n",
            "Epoch 136/200\n",
            "92/92 [==============================] - 0s 371us/sample - loss: 0.5839 - accuracy: 0.7609 - val_loss: 0.2819 - val_accuracy: 0.9167\n",
            "Epoch 137/200\n",
            "92/92 [==============================] - 0s 354us/sample - loss: 0.3081 - accuracy: 0.8478 - val_loss: 0.2807 - val_accuracy: 0.9167\n",
            "Epoch 138/200\n",
            "92/92 [==============================] - 0s 366us/sample - loss: 0.4628 - accuracy: 0.8370 - val_loss: 0.2797 - val_accuracy: 0.9167\n",
            "Epoch 139/200\n",
            "92/92 [==============================] - 0s 403us/sample - loss: 0.3115 - accuracy: 0.8587 - val_loss: 0.2795 - val_accuracy: 0.9167\n",
            "Epoch 140/200\n",
            "92/92 [==============================] - 0s 362us/sample - loss: 0.3491 - accuracy: 0.8152 - val_loss: 0.2786 - val_accuracy: 0.9167\n",
            "Epoch 141/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.4257 - accuracy: 0.8478 - val_loss: 0.2784 - val_accuracy: 0.8750\n",
            "Epoch 142/200\n",
            "92/92 [==============================] - 0s 389us/sample - loss: 0.2843 - accuracy: 0.8804 - val_loss: 0.2791 - val_accuracy: 0.8750\n",
            "Epoch 143/200\n",
            "92/92 [==============================] - 0s 350us/sample - loss: 0.5166 - accuracy: 0.8043 - val_loss: 0.2798 - val_accuracy: 0.8750\n",
            "Epoch 144/200\n",
            "92/92 [==============================] - 0s 394us/sample - loss: 0.3261 - accuracy: 0.8804 - val_loss: 0.2806 - val_accuracy: 0.8750\n",
            "Epoch 145/200\n",
            "92/92 [==============================] - 0s 460us/sample - loss: 0.4575 - accuracy: 0.7826 - val_loss: 0.2818 - val_accuracy: 0.8750\n",
            "Epoch 146/200\n",
            "92/92 [==============================] - 0s 382us/sample - loss: 0.3398 - accuracy: 0.8913 - val_loss: 0.2828 - val_accuracy: 0.8750\n",
            "Epoch 147/200\n",
            "92/92 [==============================] - 0s 363us/sample - loss: 0.4576 - accuracy: 0.8043 - val_loss: 0.2836 - val_accuracy: 0.8750\n",
            "Epoch 148/200\n",
            "92/92 [==============================] - 0s 340us/sample - loss: 0.3205 - accuracy: 0.8913 - val_loss: 0.2834 - val_accuracy: 0.8750\n",
            "Epoch 149/200\n",
            "92/92 [==============================] - 0s 394us/sample - loss: 0.3164 - accuracy: 0.8913 - val_loss: 0.2824 - val_accuracy: 0.8750\n",
            "Epoch 150/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.3412 - accuracy: 0.8478 - val_loss: 0.2807 - val_accuracy: 0.8750\n",
            "Epoch 151/200\n",
            "92/92 [==============================] - 0s 373us/sample - loss: 0.3721 - accuracy: 0.8478 - val_loss: 0.2792 - val_accuracy: 0.8750\n",
            "Epoch 152/200\n",
            "92/92 [==============================] - 0s 496us/sample - loss: 0.4740 - accuracy: 0.8370 - val_loss: 0.2774 - val_accuracy: 0.8750\n",
            "Epoch 153/200\n",
            "92/92 [==============================] - 0s 399us/sample - loss: 0.3123 - accuracy: 0.8478 - val_loss: 0.2760 - val_accuracy: 0.8750\n",
            "Epoch 154/200\n",
            "92/92 [==============================] - 0s 481us/sample - loss: 0.3088 - accuracy: 0.8913 - val_loss: 0.2755 - val_accuracy: 0.8750\n",
            "Epoch 155/200\n",
            "92/92 [==============================] - 0s 383us/sample - loss: 0.3445 - accuracy: 0.8478 - val_loss: 0.2758 - val_accuracy: 0.8750\n",
            "Epoch 156/200\n",
            "92/92 [==============================] - 0s 406us/sample - loss: 0.3800 - accuracy: 0.8261 - val_loss: 0.2763 - val_accuracy: 0.8750\n",
            "Epoch 157/200\n",
            "92/92 [==============================] - 0s 362us/sample - loss: 0.3113 - accuracy: 0.8478 - val_loss: 0.2753 - val_accuracy: 0.8750\n",
            "Epoch 158/200\n",
            "92/92 [==============================] - 0s 421us/sample - loss: 0.3516 - accuracy: 0.8043 - val_loss: 0.2751 - val_accuracy: 0.8750\n",
            "Epoch 159/200\n",
            "92/92 [==============================] - 0s 358us/sample - loss: 0.4041 - accuracy: 0.8043 - val_loss: 0.2754 - val_accuracy: 0.8750\n",
            "Epoch 160/200\n",
            "92/92 [==============================] - 0s 394us/sample - loss: 0.3425 - accuracy: 0.8696 - val_loss: 0.2755 - val_accuracy: 0.8750\n",
            "Epoch 161/200\n",
            "92/92 [==============================] - 0s 389us/sample - loss: 0.3505 - accuracy: 0.8478 - val_loss: 0.2761 - val_accuracy: 0.8750\n",
            "Epoch 162/200\n",
            "92/92 [==============================] - 0s 346us/sample - loss: 0.4057 - accuracy: 0.8152 - val_loss: 0.2777 - val_accuracy: 0.9167\n",
            "Epoch 163/200\n",
            "92/92 [==============================] - 0s 431us/sample - loss: 0.4288 - accuracy: 0.8043 - val_loss: 0.2784 - val_accuracy: 0.9167\n",
            "Epoch 164/200\n",
            "92/92 [==============================] - 0s 417us/sample - loss: 0.3452 - accuracy: 0.8370 - val_loss: 0.2792 - val_accuracy: 0.9167\n",
            "Epoch 165/200\n",
            "92/92 [==============================] - 0s 371us/sample - loss: 0.3495 - accuracy: 0.8804 - val_loss: 0.2810 - val_accuracy: 0.9167\n",
            "Epoch 166/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.2431 - accuracy: 0.9239 - val_loss: 0.2827 - val_accuracy: 0.9167\n",
            "Epoch 167/200\n",
            "92/92 [==============================] - 0s 352us/sample - loss: 0.3617 - accuracy: 0.8370 - val_loss: 0.2838 - val_accuracy: 0.9167\n",
            "Epoch 168/200\n",
            "92/92 [==============================] - 0s 395us/sample - loss: 0.2992 - accuracy: 0.9022 - val_loss: 0.2842 - val_accuracy: 0.8750\n",
            "Epoch 169/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.3633 - accuracy: 0.8370 - val_loss: 0.2848 - val_accuracy: 0.8750\n",
            "Epoch 170/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.3163 - accuracy: 0.8587 - val_loss: 0.2838 - val_accuracy: 0.8750\n",
            "Epoch 171/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.4374 - accuracy: 0.8261 - val_loss: 0.2795 - val_accuracy: 0.8750\n",
            "Epoch 172/200\n",
            "92/92 [==============================] - 0s 434us/sample - loss: 0.3678 - accuracy: 0.8152 - val_loss: 0.2769 - val_accuracy: 0.8750\n",
            "Epoch 173/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.3514 - accuracy: 0.8696 - val_loss: 0.2754 - val_accuracy: 0.8750\n",
            "Epoch 174/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.2276 - accuracy: 0.9022 - val_loss: 0.2740 - val_accuracy: 0.8750\n",
            "Epoch 175/200\n",
            "92/92 [==============================] - 0s 433us/sample - loss: 0.3158 - accuracy: 0.8261 - val_loss: 0.2735 - val_accuracy: 0.8750\n",
            "Epoch 176/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.2887 - accuracy: 0.8696 - val_loss: 0.2724 - val_accuracy: 0.8750\n",
            "Epoch 177/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.3261 - accuracy: 0.8587 - val_loss: 0.2710 - val_accuracy: 0.8750\n",
            "Epoch 178/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.3196 - accuracy: 0.8587 - val_loss: 0.2701 - val_accuracy: 0.8750\n",
            "Epoch 179/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.2862 - accuracy: 0.8804 - val_loss: 0.2693 - val_accuracy: 0.8750\n",
            "Epoch 180/200\n",
            "92/92 [==============================] - 0s 475us/sample - loss: 0.4070 - accuracy: 0.8043 - val_loss: 0.2696 - val_accuracy: 0.8750\n",
            "Epoch 181/200\n",
            "92/92 [==============================] - 0s 397us/sample - loss: 0.3087 - accuracy: 0.8587 - val_loss: 0.2708 - val_accuracy: 0.8750\n",
            "Epoch 182/200\n",
            "92/92 [==============================] - 0s 380us/sample - loss: 0.3843 - accuracy: 0.8478 - val_loss: 0.2720 - val_accuracy: 0.8750\n",
            "Epoch 183/200\n",
            "92/92 [==============================] - 0s 377us/sample - loss: 0.3716 - accuracy: 0.8043 - val_loss: 0.2729 - val_accuracy: 0.8750\n",
            "Epoch 184/200\n",
            "92/92 [==============================] - 0s 354us/sample - loss: 0.2890 - accuracy: 0.8804 - val_loss: 0.2742 - val_accuracy: 0.8750\n",
            "Epoch 185/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.3846 - accuracy: 0.8587 - val_loss: 0.2736 - val_accuracy: 0.8750\n",
            "Epoch 186/200\n",
            "92/92 [==============================] - 0s 354us/sample - loss: 0.3930 - accuracy: 0.8478 - val_loss: 0.2737 - val_accuracy: 0.8750\n",
            "Epoch 187/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.4089 - accuracy: 0.8478 - val_loss: 0.2731 - val_accuracy: 0.8750\n",
            "Epoch 188/200\n",
            "92/92 [==============================] - 0s 477us/sample - loss: 0.3396 - accuracy: 0.8478 - val_loss: 0.2707 - val_accuracy: 0.8750\n",
            "Epoch 189/200\n",
            "92/92 [==============================] - 0s 398us/sample - loss: 0.3402 - accuracy: 0.8261 - val_loss: 0.2668 - val_accuracy: 0.8750\n",
            "Epoch 190/200\n",
            "92/92 [==============================] - 0s 406us/sample - loss: 0.2992 - accuracy: 0.8587 - val_loss: 0.2656 - val_accuracy: 0.8750\n",
            "Epoch 191/200\n",
            "92/92 [==============================] - 0s 429us/sample - loss: 0.3427 - accuracy: 0.8478 - val_loss: 0.2657 - val_accuracy: 0.8750\n",
            "Epoch 192/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.3067 - accuracy: 0.8696 - val_loss: 0.2648 - val_accuracy: 0.8750\n",
            "Epoch 193/200\n",
            "92/92 [==============================] - 0s 466us/sample - loss: 0.2900 - accuracy: 0.8478 - val_loss: 0.2636 - val_accuracy: 0.8750\n",
            "Epoch 194/200\n",
            "92/92 [==============================] - 0s 402us/sample - loss: 0.3271 - accuracy: 0.8587 - val_loss: 0.2646 - val_accuracy: 0.8750\n",
            "Epoch 195/200\n",
            "92/92 [==============================] - 0s 441us/sample - loss: 0.3663 - accuracy: 0.8587 - val_loss: 0.2655 - val_accuracy: 0.8750\n",
            "Epoch 196/200\n",
            "92/92 [==============================] - 0s 508us/sample - loss: 0.2472 - accuracy: 0.9130 - val_loss: 0.2664 - val_accuracy: 0.8750\n",
            "Epoch 197/200\n",
            "92/92 [==============================] - 0s 402us/sample - loss: 0.2809 - accuracy: 0.8804 - val_loss: 0.2679 - val_accuracy: 0.8750\n",
            "Epoch 198/200\n",
            "92/92 [==============================] - 0s 425us/sample - loss: 0.2010 - accuracy: 0.8913 - val_loss: 0.2710 - val_accuracy: 0.8750\n",
            "Epoch 199/200\n",
            "92/92 [==============================] - 0s 437us/sample - loss: 0.2998 - accuracy: 0.8261 - val_loss: 0.2748 - val_accuracy: 0.8750\n",
            "Epoch 200/200\n",
            "92/92 [==============================] - 0s 432us/sample - loss: 0.3365 - accuracy: 0.8587 - val_loss: 0.2785 - val_accuracy: 0.8750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp6Me_jgUwhD"
      },
      "source": [
        "def plot_learningCurve(history, epochs):\n",
        "\n",
        "  #Plot Training e validation accuracy values\n",
        "  epoch_range = range(1, epochs+1)\n",
        "  plt.plot(epoch_range, history.history['accuracy'])\n",
        "  plt.plot(epoch_range, history.history['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  #Plot training e validation accuracy loss values\n",
        "  plt.plot(epoch_range, history.history['loss'])\n",
        "  plt.plot(epoch_range, history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTLcrdVj6Dw3"
      },
      "source": [
        "def plot_learningCurve2(history, epochs):\n",
        "\n",
        "  #Plot Training e validation accuracy values\n",
        "  epoch_range = range(1, epochs+1)\n",
        "  plt.plot(epoch_range, history.history['accuracy'])\n",
        "  plt.plot(epoch_range, history.history['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3pRpqi2U0wq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "4630f4e1-a3fc-48dc-9e9c-84a776d6c962"
      },
      "source": [
        "plot_learningCurve(history,epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amlDt-PvU6W5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "df699cf8-35da-4bce-e505-d3c4658d2a8a"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('test loss, test acc:', results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r24/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 242us/sample - loss: 0.2785 - accuracy: 0.8750\n",
            "test loss, test acc: [0.2784515917301178, 0.875]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9vY0m-aah9-"
      },
      "source": [
        "def evaluate_model(x_train, y_train, x_test, y_test):\n",
        "\tverbose, epochs, batch_size = 1, 200, 128\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(9,1)))\n",
        "\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "\tmodel.add(Dropout(0.5))\n",
        " \n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(2, activation='softmax'))\n",
        "\tmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test), verbose=1)\n",
        "\t# evaluate model\n",
        "\t_, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CZVPpUjfZNv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6814485-04bb-496c-848c-2cb05154fb29"
      },
      "source": [
        "# repeat experiment\n",
        "repeats = 5\n",
        "scores = list()\n",
        "for r in range(repeats):\n",
        "\tscore = evaluate_model(x_train, y_train, x_test, y_test)\n",
        "\tscore = score * 100.0\n",
        "\tprint('>#%d: %.3f' % (r+1, score))\n",
        "\tscores.append(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d0832268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d0832268>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d0832268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d0832268>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 1s 6ms/sample - loss: 0.7011 - accuracy: 0.3804 - val_loss: 0.6426 - val_accuracy: 0.7500\n",
            "Epoch 2/200\n",
            "92/92 [==============================] - 0s 295us/sample - loss: 0.6777 - accuracy: 0.5109 - val_loss: 0.6015 - val_accuracy: 0.6667\n",
            "Epoch 3/200\n",
            "92/92 [==============================] - 0s 263us/sample - loss: 0.6540 - accuracy: 0.6739 - val_loss: 0.5687 - val_accuracy: 0.7083\n",
            "Epoch 4/200\n",
            "92/92 [==============================] - 0s 292us/sample - loss: 0.6487 - accuracy: 0.6087 - val_loss: 0.5412 - val_accuracy: 0.7917\n",
            "Epoch 5/200\n",
            "92/92 [==============================] - 0s 271us/sample - loss: 0.6283 - accuracy: 0.6739 - val_loss: 0.5144 - val_accuracy: 0.8750\n",
            "Epoch 6/200\n",
            "92/92 [==============================] - 0s 270us/sample - loss: 0.6169 - accuracy: 0.7500 - val_loss: 0.4867 - val_accuracy: 0.9583\n",
            "Epoch 7/200\n",
            "92/92 [==============================] - 0s 262us/sample - loss: 0.6015 - accuracy: 0.7065 - val_loss: 0.4632 - val_accuracy: 1.0000\n",
            "Epoch 8/200\n",
            "92/92 [==============================] - 0s 268us/sample - loss: 0.6028 - accuracy: 0.7500 - val_loss: 0.4377 - val_accuracy: 0.9583\n",
            "Epoch 9/200\n",
            "92/92 [==============================] - 0s 255us/sample - loss: 0.5889 - accuracy: 0.6957 - val_loss: 0.4128 - val_accuracy: 0.9583\n",
            "Epoch 10/200\n",
            "92/92 [==============================] - 0s 254us/sample - loss: 0.5691 - accuracy: 0.6848 - val_loss: 0.3898 - val_accuracy: 0.9583\n",
            "Epoch 11/200\n",
            "92/92 [==============================] - 0s 244us/sample - loss: 0.5760 - accuracy: 0.6630 - val_loss: 0.3633 - val_accuracy: 0.9583\n",
            "Epoch 12/200\n",
            "92/92 [==============================] - 0s 261us/sample - loss: 0.5314 - accuracy: 0.7609 - val_loss: 0.3426 - val_accuracy: 0.9583\n",
            "Epoch 13/200\n",
            "92/92 [==============================] - 0s 269us/sample - loss: 0.5273 - accuracy: 0.7500 - val_loss: 0.3219 - val_accuracy: 0.9583\n",
            "Epoch 14/200\n",
            "92/92 [==============================] - 0s 276us/sample - loss: 0.5318 - accuracy: 0.7609 - val_loss: 0.3064 - val_accuracy: 0.9583\n",
            "Epoch 15/200\n",
            "92/92 [==============================] - 0s 296us/sample - loss: 0.5522 - accuracy: 0.7065 - val_loss: 0.2939 - val_accuracy: 0.9583\n",
            "Epoch 16/200\n",
            "92/92 [==============================] - 0s 318us/sample - loss: 0.5310 - accuracy: 0.7500 - val_loss: 0.2802 - val_accuracy: 0.9583\n",
            "Epoch 17/200\n",
            "92/92 [==============================] - 0s 346us/sample - loss: 0.4845 - accuracy: 0.7935 - val_loss: 0.2719 - val_accuracy: 0.9583\n",
            "Epoch 18/200\n",
            "92/92 [==============================] - 0s 263us/sample - loss: 0.4791 - accuracy: 0.7935 - val_loss: 0.2686 - val_accuracy: 0.9583\n",
            "Epoch 19/200\n",
            "92/92 [==============================] - 0s 272us/sample - loss: 0.4425 - accuracy: 0.7826 - val_loss: 0.2639 - val_accuracy: 0.9583\n",
            "Epoch 20/200\n",
            "92/92 [==============================] - 0s 269us/sample - loss: 0.4872 - accuracy: 0.7935 - val_loss: 0.2584 - val_accuracy: 0.9583\n",
            "Epoch 21/200\n",
            "92/92 [==============================] - 0s 280us/sample - loss: 0.4435 - accuracy: 0.7826 - val_loss: 0.2538 - val_accuracy: 0.9583\n",
            "Epoch 22/200\n",
            "92/92 [==============================] - 0s 288us/sample - loss: 0.4774 - accuracy: 0.7500 - val_loss: 0.2512 - val_accuracy: 0.9583\n",
            "Epoch 23/200\n",
            "92/92 [==============================] - 0s 275us/sample - loss: 0.4726 - accuracy: 0.8152 - val_loss: 0.2458 - val_accuracy: 0.9583\n",
            "Epoch 24/200\n",
            "92/92 [==============================] - 0s 244us/sample - loss: 0.3881 - accuracy: 0.8370 - val_loss: 0.2410 - val_accuracy: 0.9583\n",
            "Epoch 25/200\n",
            "92/92 [==============================] - 0s 264us/sample - loss: 0.4282 - accuracy: 0.8043 - val_loss: 0.2348 - val_accuracy: 0.9583\n",
            "Epoch 26/200\n",
            "92/92 [==============================] - 0s 255us/sample - loss: 0.4284 - accuracy: 0.8043 - val_loss: 0.2292 - val_accuracy: 0.9583\n",
            "Epoch 27/200\n",
            "92/92 [==============================] - 0s 292us/sample - loss: 0.4469 - accuracy: 0.7826 - val_loss: 0.2282 - val_accuracy: 0.9583\n",
            "Epoch 28/200\n",
            "92/92 [==============================] - 0s 291us/sample - loss: 0.4143 - accuracy: 0.8043 - val_loss: 0.2266 - val_accuracy: 0.9583\n",
            "Epoch 29/200\n",
            "92/92 [==============================] - 0s 311us/sample - loss: 0.4051 - accuracy: 0.8261 - val_loss: 0.2293 - val_accuracy: 0.9583\n",
            "Epoch 30/200\n",
            "92/92 [==============================] - 0s 302us/sample - loss: 0.4146 - accuracy: 0.7826 - val_loss: 0.2297 - val_accuracy: 0.9583\n",
            "Epoch 31/200\n",
            "92/92 [==============================] - 0s 282us/sample - loss: 0.3588 - accuracy: 0.8696 - val_loss: 0.2246 - val_accuracy: 0.9583\n",
            "Epoch 32/200\n",
            "92/92 [==============================] - 0s 274us/sample - loss: 0.3591 - accuracy: 0.8587 - val_loss: 0.2223 - val_accuracy: 0.9583\n",
            "Epoch 33/200\n",
            "92/92 [==============================] - 0s 260us/sample - loss: 0.3765 - accuracy: 0.8370 - val_loss: 0.2208 - val_accuracy: 0.9583\n",
            "Epoch 34/200\n",
            "92/92 [==============================] - 0s 262us/sample - loss: 0.3981 - accuracy: 0.8043 - val_loss: 0.2252 - val_accuracy: 0.9167\n",
            "Epoch 35/200\n",
            "92/92 [==============================] - 0s 265us/sample - loss: 0.3731 - accuracy: 0.8370 - val_loss: 0.2442 - val_accuracy: 0.8750\n",
            "Epoch 36/200\n",
            "92/92 [==============================] - 0s 258us/sample - loss: 0.3659 - accuracy: 0.8370 - val_loss: 0.2526 - val_accuracy: 0.8750\n",
            "Epoch 37/200\n",
            "92/92 [==============================] - 0s 273us/sample - loss: 0.3999 - accuracy: 0.8370 - val_loss: 0.2383 - val_accuracy: 0.9167\n",
            "Epoch 38/200\n",
            "92/92 [==============================] - 0s 267us/sample - loss: 0.3442 - accuracy: 0.8261 - val_loss: 0.2297 - val_accuracy: 0.9583\n",
            "Epoch 39/200\n",
            "92/92 [==============================] - 0s 277us/sample - loss: 0.3358 - accuracy: 0.8804 - val_loss: 0.2299 - val_accuracy: 0.9583\n",
            "Epoch 40/200\n",
            "92/92 [==============================] - 0s 262us/sample - loss: 0.3499 - accuracy: 0.8587 - val_loss: 0.2310 - val_accuracy: 0.9583\n",
            "Epoch 41/200\n",
            "92/92 [==============================] - 0s 266us/sample - loss: 0.3492 - accuracy: 0.8370 - val_loss: 0.2383 - val_accuracy: 0.9167\n",
            "Epoch 42/200\n",
            "92/92 [==============================] - 0s 247us/sample - loss: 0.3303 - accuracy: 0.8913 - val_loss: 0.2457 - val_accuracy: 0.8750\n",
            "Epoch 43/200\n",
            "92/92 [==============================] - 0s 286us/sample - loss: 0.3184 - accuracy: 0.8804 - val_loss: 0.2437 - val_accuracy: 0.9167\n",
            "Epoch 44/200\n",
            "92/92 [==============================] - 0s 278us/sample - loss: 0.3148 - accuracy: 0.8696 - val_loss: 0.2373 - val_accuracy: 0.9167\n",
            "Epoch 45/200\n",
            "92/92 [==============================] - 0s 316us/sample - loss: 0.3205 - accuracy: 0.8696 - val_loss: 0.2360 - val_accuracy: 0.9167\n",
            "Epoch 46/200\n",
            "92/92 [==============================] - 0s 337us/sample - loss: 0.2861 - accuracy: 0.9022 - val_loss: 0.2386 - val_accuracy: 0.9167\n",
            "Epoch 47/200\n",
            "92/92 [==============================] - 0s 308us/sample - loss: 0.3263 - accuracy: 0.8804 - val_loss: 0.2370 - val_accuracy: 0.9167\n",
            "Epoch 48/200\n",
            "92/92 [==============================] - 0s 307us/sample - loss: 0.3375 - accuracy: 0.8587 - val_loss: 0.2313 - val_accuracy: 0.9583\n",
            "Epoch 49/200\n",
            "92/92 [==============================] - 0s 293us/sample - loss: 0.3042 - accuracy: 0.8696 - val_loss: 0.2403 - val_accuracy: 0.8750\n",
            "Epoch 50/200\n",
            "92/92 [==============================] - 0s 266us/sample - loss: 0.2573 - accuracy: 0.8913 - val_loss: 0.2625 - val_accuracy: 0.8750\n",
            "Epoch 51/200\n",
            "92/92 [==============================] - 0s 261us/sample - loss: 0.2781 - accuracy: 0.8913 - val_loss: 0.2601 - val_accuracy: 0.8750\n",
            "Epoch 52/200\n",
            "92/92 [==============================] - 0s 282us/sample - loss: 0.2899 - accuracy: 0.9348 - val_loss: 0.2528 - val_accuracy: 0.8750\n",
            "Epoch 53/200\n",
            "92/92 [==============================] - 0s 279us/sample - loss: 0.3001 - accuracy: 0.8696 - val_loss: 0.2522 - val_accuracy: 0.9167\n",
            "Epoch 54/200\n",
            "92/92 [==============================] - 0s 286us/sample - loss: 0.2599 - accuracy: 0.9022 - val_loss: 0.2499 - val_accuracy: 0.9583\n",
            "Epoch 55/200\n",
            "92/92 [==============================] - 0s 304us/sample - loss: 0.2621 - accuracy: 0.8804 - val_loss: 0.2577 - val_accuracy: 0.8750\n",
            "Epoch 56/200\n",
            "92/92 [==============================] - 0s 281us/sample - loss: 0.2600 - accuracy: 0.9239 - val_loss: 0.2718 - val_accuracy: 0.8750\n",
            "Epoch 57/200\n",
            "92/92 [==============================] - 0s 330us/sample - loss: 0.2738 - accuracy: 0.8804 - val_loss: 0.2627 - val_accuracy: 0.8750\n",
            "Epoch 58/200\n",
            "92/92 [==============================] - 0s 284us/sample - loss: 0.2426 - accuracy: 0.9022 - val_loss: 0.2580 - val_accuracy: 0.8750\n",
            "Epoch 59/200\n",
            "92/92 [==============================] - 0s 304us/sample - loss: 0.2114 - accuracy: 0.9674 - val_loss: 0.2466 - val_accuracy: 0.9167\n",
            "Epoch 60/200\n",
            "92/92 [==============================] - 0s 330us/sample - loss: 0.2458 - accuracy: 0.9239 - val_loss: 0.2395 - val_accuracy: 0.9583\n",
            "Epoch 61/200\n",
            "92/92 [==============================] - 0s 278us/sample - loss: 0.2898 - accuracy: 0.9022 - val_loss: 0.2621 - val_accuracy: 0.8750\n",
            "Epoch 62/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.2443 - accuracy: 0.9565 - val_loss: 0.2595 - val_accuracy: 0.8750\n",
            "Epoch 63/200\n",
            "92/92 [==============================] - 0s 270us/sample - loss: 0.2336 - accuracy: 0.8913 - val_loss: 0.2430 - val_accuracy: 0.9583\n",
            "Epoch 64/200\n",
            "92/92 [==============================] - 0s 346us/sample - loss: 0.2316 - accuracy: 0.9130 - val_loss: 0.2432 - val_accuracy: 0.9583\n",
            "Epoch 65/200\n",
            "92/92 [==============================] - 0s 290us/sample - loss: 0.2542 - accuracy: 0.8804 - val_loss: 0.2648 - val_accuracy: 0.8750\n",
            "Epoch 66/200\n",
            "92/92 [==============================] - 0s 298us/sample - loss: 0.2323 - accuracy: 0.9239 - val_loss: 0.2985 - val_accuracy: 0.8333\n",
            "Epoch 67/200\n",
            "92/92 [==============================] - 0s 299us/sample - loss: 0.2987 - accuracy: 0.8913 - val_loss: 0.2879 - val_accuracy: 0.8750\n",
            "Epoch 68/200\n",
            "92/92 [==============================] - 0s 301us/sample - loss: 0.2587 - accuracy: 0.9022 - val_loss: 0.2667 - val_accuracy: 0.8750\n",
            "Epoch 69/200\n",
            "92/92 [==============================] - 0s 277us/sample - loss: 0.2115 - accuracy: 0.9457 - val_loss: 0.2692 - val_accuracy: 0.9167\n",
            "Epoch 70/200\n",
            "92/92 [==============================] - 0s 304us/sample - loss: 0.2122 - accuracy: 0.9348 - val_loss: 0.2746 - val_accuracy: 0.9167\n",
            "Epoch 71/200\n",
            "92/92 [==============================] - 0s 297us/sample - loss: 0.2718 - accuracy: 0.9022 - val_loss: 0.2887 - val_accuracy: 0.9167\n",
            "Epoch 72/200\n",
            "92/92 [==============================] - 0s 302us/sample - loss: 0.2233 - accuracy: 0.9130 - val_loss: 0.3105 - val_accuracy: 0.8333\n",
            "Epoch 73/200\n",
            "92/92 [==============================] - 0s 295us/sample - loss: 0.3094 - accuracy: 0.8804 - val_loss: 0.3112 - val_accuracy: 0.8333\n",
            "Epoch 74/200\n",
            "92/92 [==============================] - 0s 282us/sample - loss: 0.2207 - accuracy: 0.9565 - val_loss: 0.2779 - val_accuracy: 0.9167\n",
            "Epoch 75/200\n",
            "92/92 [==============================] - 0s 285us/sample - loss: 0.1902 - accuracy: 0.9565 - val_loss: 0.2643 - val_accuracy: 0.9167\n",
            "Epoch 76/200\n",
            "92/92 [==============================] - 0s 312us/sample - loss: 0.2252 - accuracy: 0.9022 - val_loss: 0.2817 - val_accuracy: 0.9167\n",
            "Epoch 77/200\n",
            "92/92 [==============================] - 0s 338us/sample - loss: 0.2020 - accuracy: 0.9022 - val_loss: 0.2911 - val_accuracy: 0.9167\n",
            "Epoch 78/200\n",
            "92/92 [==============================] - 0s 341us/sample - loss: 0.2190 - accuracy: 0.9348 - val_loss: 0.2834 - val_accuracy: 0.9167\n",
            "Epoch 79/200\n",
            "92/92 [==============================] - 0s 318us/sample - loss: 0.2002 - accuracy: 0.9348 - val_loss: 0.2731 - val_accuracy: 0.9167\n",
            "Epoch 80/200\n",
            "92/92 [==============================] - 0s 288us/sample - loss: 0.2344 - accuracy: 0.8913 - val_loss: 0.2670 - val_accuracy: 0.9167\n",
            "Epoch 81/200\n",
            "92/92 [==============================] - 0s 282us/sample - loss: 0.1951 - accuracy: 0.9239 - val_loss: 0.2744 - val_accuracy: 0.8750\n",
            "Epoch 82/200\n",
            "92/92 [==============================] - 0s 288us/sample - loss: 0.2174 - accuracy: 0.9022 - val_loss: 0.2745 - val_accuracy: 0.9167\n",
            "Epoch 83/200\n",
            "92/92 [==============================] - 0s 275us/sample - loss: 0.2252 - accuracy: 0.9022 - val_loss: 0.2767 - val_accuracy: 0.9167\n",
            "Epoch 84/200\n",
            "92/92 [==============================] - 0s 290us/sample - loss: 0.1977 - accuracy: 0.9348 - val_loss: 0.2652 - val_accuracy: 0.9583\n",
            "Epoch 85/200\n",
            "92/92 [==============================] - 0s 343us/sample - loss: 0.1973 - accuracy: 0.9239 - val_loss: 0.2648 - val_accuracy: 0.9583\n",
            "Epoch 86/200\n",
            "92/92 [==============================] - 0s 327us/sample - loss: 0.2040 - accuracy: 0.9457 - val_loss: 0.2737 - val_accuracy: 0.9167\n",
            "Epoch 87/200\n",
            "92/92 [==============================] - 0s 320us/sample - loss: 0.2087 - accuracy: 0.9239 - val_loss: 0.2737 - val_accuracy: 0.9583\n",
            "Epoch 88/200\n",
            "92/92 [==============================] - 0s 379us/sample - loss: 0.1771 - accuracy: 0.9565 - val_loss: 0.2792 - val_accuracy: 0.8750\n",
            "Epoch 89/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.1846 - accuracy: 0.9457 - val_loss: 0.2791 - val_accuracy: 0.8750\n",
            "Epoch 90/200\n",
            "92/92 [==============================] - 0s 280us/sample - loss: 0.1613 - accuracy: 0.9783 - val_loss: 0.2761 - val_accuracy: 0.9583\n",
            "Epoch 91/200\n",
            "92/92 [==============================] - 0s 283us/sample - loss: 0.1560 - accuracy: 0.9457 - val_loss: 0.2785 - val_accuracy: 0.8750\n",
            "Epoch 92/200\n",
            "92/92 [==============================] - 0s 300us/sample - loss: 0.1533 - accuracy: 0.9674 - val_loss: 0.2909 - val_accuracy: 0.8750\n",
            "Epoch 93/200\n",
            "92/92 [==============================] - 0s 298us/sample - loss: 0.1893 - accuracy: 0.9239 - val_loss: 0.2855 - val_accuracy: 0.8750\n",
            "Epoch 94/200\n",
            "92/92 [==============================] - 0s 296us/sample - loss: 0.1468 - accuracy: 0.9674 - val_loss: 0.2729 - val_accuracy: 0.9167\n",
            "Epoch 95/200\n",
            "92/92 [==============================] - 0s 347us/sample - loss: 0.1935 - accuracy: 0.9130 - val_loss: 0.2703 - val_accuracy: 0.9167\n",
            "Epoch 96/200\n",
            "92/92 [==============================] - 0s 276us/sample - loss: 0.1333 - accuracy: 0.9674 - val_loss: 0.2711 - val_accuracy: 0.9167\n",
            "Epoch 97/200\n",
            "92/92 [==============================] - 0s 306us/sample - loss: 0.1505 - accuracy: 0.9565 - val_loss: 0.2783 - val_accuracy: 0.9167\n",
            "Epoch 98/200\n",
            "92/92 [==============================] - 0s 296us/sample - loss: 0.1747 - accuracy: 0.9565 - val_loss: 0.2971 - val_accuracy: 0.8333\n",
            "Epoch 99/200\n",
            "92/92 [==============================] - 0s 283us/sample - loss: 0.1871 - accuracy: 0.9457 - val_loss: 0.2964 - val_accuracy: 0.8750\n",
            "Epoch 100/200\n",
            "92/92 [==============================] - 0s 322us/sample - loss: 0.2569 - accuracy: 0.8804 - val_loss: 0.2840 - val_accuracy: 0.8750\n",
            "Epoch 101/200\n",
            "92/92 [==============================] - 0s 295us/sample - loss: 0.1625 - accuracy: 0.9239 - val_loss: 0.3017 - val_accuracy: 0.8750\n",
            "Epoch 102/200\n",
            "92/92 [==============================] - 0s 306us/sample - loss: 0.1496 - accuracy: 0.9457 - val_loss: 0.3443 - val_accuracy: 0.8750\n",
            "Epoch 103/200\n",
            "92/92 [==============================] - 0s 301us/sample - loss: 0.1534 - accuracy: 0.9348 - val_loss: 0.3584 - val_accuracy: 0.8750\n",
            "Epoch 104/200\n",
            "92/92 [==============================] - 0s 326us/sample - loss: 0.1570 - accuracy: 0.9348 - val_loss: 0.3224 - val_accuracy: 0.8750\n",
            "Epoch 105/200\n",
            "92/92 [==============================] - 0s 310us/sample - loss: 0.1391 - accuracy: 0.9457 - val_loss: 0.2998 - val_accuracy: 0.9167\n",
            "Epoch 106/200\n",
            "92/92 [==============================] - 0s 325us/sample - loss: 0.1271 - accuracy: 0.9674 - val_loss: 0.3065 - val_accuracy: 0.8750\n",
            "Epoch 107/200\n",
            "92/92 [==============================] - 0s 305us/sample - loss: 0.1226 - accuracy: 0.9674 - val_loss: 0.3203 - val_accuracy: 0.8750\n",
            "Epoch 108/200\n",
            "92/92 [==============================] - 0s 300us/sample - loss: 0.1705 - accuracy: 0.9457 - val_loss: 0.3272 - val_accuracy: 0.8750\n",
            "Epoch 109/200\n",
            "92/92 [==============================] - 0s 322us/sample - loss: 0.1669 - accuracy: 0.9348 - val_loss: 0.3230 - val_accuracy: 0.8750\n",
            "Epoch 110/200\n",
            "92/92 [==============================] - 0s 306us/sample - loss: 0.1275 - accuracy: 0.9674 - val_loss: 0.3249 - val_accuracy: 0.8750\n",
            "Epoch 111/200\n",
            "92/92 [==============================] - 0s 312us/sample - loss: 0.1491 - accuracy: 0.9565 - val_loss: 0.3720 - val_accuracy: 0.8333\n",
            "Epoch 112/200\n",
            "92/92 [==============================] - 0s 281us/sample - loss: 0.1614 - accuracy: 0.9674 - val_loss: 0.4252 - val_accuracy: 0.8333\n",
            "Epoch 113/200\n",
            "92/92 [==============================] - 0s 294us/sample - loss: 0.1466 - accuracy: 0.9457 - val_loss: 0.3557 - val_accuracy: 0.8750\n",
            "Epoch 114/200\n",
            "92/92 [==============================] - 0s 313us/sample - loss: 0.1210 - accuracy: 0.9565 - val_loss: 0.2964 - val_accuracy: 0.9167\n",
            "Epoch 115/200\n",
            "92/92 [==============================] - 0s 334us/sample - loss: 0.1078 - accuracy: 0.9674 - val_loss: 0.2887 - val_accuracy: 0.9167\n",
            "Epoch 116/200\n",
            "92/92 [==============================] - 0s 320us/sample - loss: 0.1342 - accuracy: 0.9457 - val_loss: 0.3201 - val_accuracy: 0.9167\n",
            "Epoch 117/200\n",
            "92/92 [==============================] - 0s 296us/sample - loss: 0.1169 - accuracy: 0.9565 - val_loss: 0.4039 - val_accuracy: 0.8750\n",
            "Epoch 118/200\n",
            "92/92 [==============================] - 0s 324us/sample - loss: 0.1203 - accuracy: 0.9783 - val_loss: 0.4356 - val_accuracy: 0.8333\n",
            "Epoch 119/200\n",
            "92/92 [==============================] - 0s 291us/sample - loss: 0.1315 - accuracy: 0.9565 - val_loss: 0.3707 - val_accuracy: 0.9167\n",
            "Epoch 120/200\n",
            "92/92 [==============================] - 0s 327us/sample - loss: 0.1302 - accuracy: 0.9674 - val_loss: 0.3102 - val_accuracy: 0.9167\n",
            "Epoch 121/200\n",
            "92/92 [==============================] - 0s 301us/sample - loss: 0.1147 - accuracy: 0.9457 - val_loss: 0.2897 - val_accuracy: 0.9167\n",
            "Epoch 122/200\n",
            "92/92 [==============================] - 0s 319us/sample - loss: 0.1383 - accuracy: 0.9348 - val_loss: 0.3071 - val_accuracy: 0.8750\n",
            "Epoch 123/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.1181 - accuracy: 0.9457 - val_loss: 0.3512 - val_accuracy: 0.8750\n",
            "Epoch 124/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.1060 - accuracy: 0.9674 - val_loss: 0.4138 - val_accuracy: 0.8333\n",
            "Epoch 125/200\n",
            "92/92 [==============================] - 0s 307us/sample - loss: 0.1208 - accuracy: 0.9565 - val_loss: 0.4116 - val_accuracy: 0.8333\n",
            "Epoch 126/200\n",
            "92/92 [==============================] - 0s 297us/sample - loss: 0.1398 - accuracy: 0.9239 - val_loss: 0.3980 - val_accuracy: 0.8750\n",
            "Epoch 127/200\n",
            "92/92 [==============================] - 0s 295us/sample - loss: 0.1156 - accuracy: 0.9674 - val_loss: 0.4007 - val_accuracy: 0.8750\n",
            "Epoch 128/200\n",
            "92/92 [==============================] - 0s 288us/sample - loss: 0.1028 - accuracy: 0.9565 - val_loss: 0.3976 - val_accuracy: 0.9167\n",
            "Epoch 129/200\n",
            "92/92 [==============================] - 0s 283us/sample - loss: 0.1480 - accuracy: 0.9457 - val_loss: 0.3697 - val_accuracy: 0.9167\n",
            "Epoch 130/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.1053 - accuracy: 0.9783 - val_loss: 0.3784 - val_accuracy: 0.8750\n",
            "Epoch 131/200\n",
            "92/92 [==============================] - 0s 304us/sample - loss: 0.0856 - accuracy: 0.9783 - val_loss: 0.3872 - val_accuracy: 0.8750\n",
            "Epoch 132/200\n",
            "92/92 [==============================] - 0s 324us/sample - loss: 0.1320 - accuracy: 0.9348 - val_loss: 0.3983 - val_accuracy: 0.8333\n",
            "Epoch 133/200\n",
            "92/92 [==============================] - 0s 316us/sample - loss: 0.1323 - accuracy: 0.9565 - val_loss: 0.3736 - val_accuracy: 0.8333\n",
            "Epoch 134/200\n",
            "92/92 [==============================] - 0s 313us/sample - loss: 0.1440 - accuracy: 0.9457 - val_loss: 0.3344 - val_accuracy: 0.8750\n",
            "Epoch 135/200\n",
            "92/92 [==============================] - 0s 347us/sample - loss: 0.0917 - accuracy: 0.9783 - val_loss: 0.3403 - val_accuracy: 0.9167\n",
            "Epoch 136/200\n",
            "92/92 [==============================] - 0s 331us/sample - loss: 0.0890 - accuracy: 0.9565 - val_loss: 0.3746 - val_accuracy: 0.9167\n",
            "Epoch 137/200\n",
            "92/92 [==============================] - 0s 322us/sample - loss: 0.0772 - accuracy: 0.9783 - val_loss: 0.4318 - val_accuracy: 0.8333\n",
            "Epoch 138/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.0965 - accuracy: 0.9674 - val_loss: 0.4530 - val_accuracy: 0.8333\n",
            "Epoch 139/200\n",
            "92/92 [==============================] - 0s 338us/sample - loss: 0.1005 - accuracy: 0.9783 - val_loss: 0.4408 - val_accuracy: 0.8750\n",
            "Epoch 140/200\n",
            "92/92 [==============================] - 0s 323us/sample - loss: 0.0633 - accuracy: 0.9783 - val_loss: 0.4371 - val_accuracy: 0.8750\n",
            "Epoch 141/200\n",
            "92/92 [==============================] - 0s 303us/sample - loss: 0.1668 - accuracy: 0.9674 - val_loss: 0.4250 - val_accuracy: 0.8750\n",
            "Epoch 142/200\n",
            "92/92 [==============================] - 0s 305us/sample - loss: 0.0748 - accuracy: 0.9783 - val_loss: 0.4064 - val_accuracy: 0.8750\n",
            "Epoch 143/200\n",
            "92/92 [==============================] - 0s 335us/sample - loss: 0.1273 - accuracy: 0.9565 - val_loss: 0.4142 - val_accuracy: 0.8750\n",
            "Epoch 144/200\n",
            "92/92 [==============================] - 0s 299us/sample - loss: 0.1117 - accuracy: 0.9457 - val_loss: 0.4328 - val_accuracy: 0.8333\n",
            "Epoch 145/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.1068 - accuracy: 0.9674 - val_loss: 0.4284 - val_accuracy: 0.8333\n",
            "Epoch 146/200\n",
            "92/92 [==============================] - 0s 375us/sample - loss: 0.0780 - accuracy: 0.9674 - val_loss: 0.4099 - val_accuracy: 0.8750\n",
            "Epoch 147/200\n",
            "92/92 [==============================] - 0s 348us/sample - loss: 0.0759 - accuracy: 0.9783 - val_loss: 0.4089 - val_accuracy: 0.8750\n",
            "Epoch 148/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.0723 - accuracy: 0.9891 - val_loss: 0.4183 - val_accuracy: 0.8750\n",
            "Epoch 149/200\n",
            "92/92 [==============================] - 0s 329us/sample - loss: 0.0919 - accuracy: 0.9783 - val_loss: 0.4692 - val_accuracy: 0.8333\n",
            "Epoch 150/200\n",
            "92/92 [==============================] - 0s 364us/sample - loss: 0.0705 - accuracy: 0.9891 - val_loss: 0.5161 - val_accuracy: 0.8333\n",
            "Epoch 151/200\n",
            "92/92 [==============================] - 0s 302us/sample - loss: 0.1078 - accuracy: 0.9565 - val_loss: 0.4680 - val_accuracy: 0.8750\n",
            "Epoch 152/200\n",
            "92/92 [==============================] - 0s 355us/sample - loss: 0.1120 - accuracy: 0.9348 - val_loss: 0.3993 - val_accuracy: 0.8750\n",
            "Epoch 153/200\n",
            "92/92 [==============================] - 0s 316us/sample - loss: 0.1162 - accuracy: 0.9565 - val_loss: 0.4156 - val_accuracy: 0.8750\n",
            "Epoch 154/200\n",
            "92/92 [==============================] - 0s 398us/sample - loss: 0.0656 - accuracy: 0.9783 - val_loss: 0.4437 - val_accuracy: 0.9167\n",
            "Epoch 155/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.0735 - accuracy: 0.9674 - val_loss: 0.4827 - val_accuracy: 0.8750\n",
            "Epoch 156/200\n",
            "92/92 [==============================] - 0s 418us/sample - loss: 0.0653 - accuracy: 0.9674 - val_loss: 0.4954 - val_accuracy: 0.8750\n",
            "Epoch 157/200\n",
            "92/92 [==============================] - 0s 349us/sample - loss: 0.1095 - accuracy: 0.9674 - val_loss: 0.4772 - val_accuracy: 0.8750\n",
            "Epoch 158/200\n",
            "92/92 [==============================] - 0s 407us/sample - loss: 0.0879 - accuracy: 0.9891 - val_loss: 0.4706 - val_accuracy: 0.8333\n",
            "Epoch 159/200\n",
            "92/92 [==============================] - 0s 308us/sample - loss: 0.0712 - accuracy: 0.9891 - val_loss: 0.4422 - val_accuracy: 0.8333\n",
            "Epoch 160/200\n",
            "92/92 [==============================] - 0s 320us/sample - loss: 0.0543 - accuracy: 0.9891 - val_loss: 0.4301 - val_accuracy: 0.8333\n",
            "Epoch 161/200\n",
            "92/92 [==============================] - 0s 366us/sample - loss: 0.0843 - accuracy: 0.9674 - val_loss: 0.4364 - val_accuracy: 0.8333\n",
            "Epoch 162/200\n",
            "92/92 [==============================] - 0s 355us/sample - loss: 0.1041 - accuracy: 0.9457 - val_loss: 0.4299 - val_accuracy: 0.8333\n",
            "Epoch 163/200\n",
            "92/92 [==============================] - 0s 324us/sample - loss: 0.0613 - accuracy: 0.9783 - val_loss: 0.4075 - val_accuracy: 0.8333\n",
            "Epoch 164/200\n",
            "92/92 [==============================] - 0s 315us/sample - loss: 0.0988 - accuracy: 0.9783 - val_loss: 0.3914 - val_accuracy: 0.8750\n",
            "Epoch 165/200\n",
            "92/92 [==============================] - 0s 326us/sample - loss: 0.0985 - accuracy: 0.9674 - val_loss: 0.4004 - val_accuracy: 0.8750\n",
            "Epoch 166/200\n",
            "92/92 [==============================] - 0s 330us/sample - loss: 0.1015 - accuracy: 0.9674 - val_loss: 0.4182 - val_accuracy: 0.8750\n",
            "Epoch 167/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.1062 - accuracy: 0.9674 - val_loss: 0.4121 - val_accuracy: 0.8750\n",
            "Epoch 168/200\n",
            "92/92 [==============================] - 0s 301us/sample - loss: 0.0594 - accuracy: 0.9783 - val_loss: 0.3815 - val_accuracy: 0.8750\n",
            "Epoch 169/200\n",
            "92/92 [==============================] - 0s 298us/sample - loss: 0.0897 - accuracy: 0.9674 - val_loss: 0.3725 - val_accuracy: 0.8750\n",
            "Epoch 170/200\n",
            "92/92 [==============================] - 0s 338us/sample - loss: 0.0744 - accuracy: 0.9783 - val_loss: 0.3900 - val_accuracy: 0.8750\n",
            "Epoch 171/200\n",
            "92/92 [==============================] - 0s 294us/sample - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.4395 - val_accuracy: 0.8333\n",
            "Epoch 172/200\n",
            "92/92 [==============================] - 0s 327us/sample - loss: 0.0910 - accuracy: 0.9674 - val_loss: 0.5030 - val_accuracy: 0.7917\n",
            "Epoch 173/200\n",
            "92/92 [==============================] - 0s 314us/sample - loss: 0.0925 - accuracy: 0.9783 - val_loss: 0.5228 - val_accuracy: 0.7917\n",
            "Epoch 174/200\n",
            "92/92 [==============================] - 0s 324us/sample - loss: 0.0602 - accuracy: 0.9783 - val_loss: 0.5106 - val_accuracy: 0.8750\n",
            "Epoch 175/200\n",
            "92/92 [==============================] - 0s 317us/sample - loss: 0.0712 - accuracy: 0.9891 - val_loss: 0.4680 - val_accuracy: 0.8750\n",
            "Epoch 176/200\n",
            "92/92 [==============================] - 0s 324us/sample - loss: 0.0393 - accuracy: 1.0000 - val_loss: 0.4515 - val_accuracy: 0.8750\n",
            "Epoch 177/200\n",
            "92/92 [==============================] - 0s 302us/sample - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.4434 - val_accuracy: 0.8750\n",
            "Epoch 178/200\n",
            "92/92 [==============================] - 0s 305us/sample - loss: 0.0833 - accuracy: 0.9674 - val_loss: 0.4677 - val_accuracy: 0.8750\n",
            "Epoch 179/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.0616 - accuracy: 0.9783 - val_loss: 0.4808 - val_accuracy: 0.8750\n",
            "Epoch 180/200\n",
            "92/92 [==============================] - 0s 304us/sample - loss: 0.0781 - accuracy: 0.9457 - val_loss: 0.4814 - val_accuracy: 0.8750\n",
            "Epoch 181/200\n",
            "92/92 [==============================] - 0s 295us/sample - loss: 0.0777 - accuracy: 0.9674 - val_loss: 0.4546 - val_accuracy: 0.8750\n",
            "Epoch 182/200\n",
            "92/92 [==============================] - 0s 318us/sample - loss: 0.0430 - accuracy: 1.0000 - val_loss: 0.5128 - val_accuracy: 0.8750\n",
            "Epoch 183/200\n",
            "92/92 [==============================] - 0s 397us/sample - loss: 0.0647 - accuracy: 0.9783 - val_loss: 0.5396 - val_accuracy: 0.8333\n",
            "Epoch 184/200\n",
            "92/92 [==============================] - 0s 310us/sample - loss: 0.0746 - accuracy: 0.9674 - val_loss: 0.6034 - val_accuracy: 0.8333\n",
            "Epoch 185/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.6549 - val_accuracy: 0.8333\n",
            "Epoch 186/200\n",
            "92/92 [==============================] - 0s 330us/sample - loss: 0.0576 - accuracy: 0.9783 - val_loss: 0.6798 - val_accuracy: 0.8333\n",
            "Epoch 187/200\n",
            "92/92 [==============================] - 0s 351us/sample - loss: 0.0596 - accuracy: 0.9891 - val_loss: 0.6370 - val_accuracy: 0.8333\n",
            "Epoch 188/200\n",
            "92/92 [==============================] - 0s 463us/sample - loss: 0.0396 - accuracy: 0.9891 - val_loss: 0.5586 - val_accuracy: 0.8333\n",
            "Epoch 189/200\n",
            "92/92 [==============================] - 0s 314us/sample - loss: 0.0364 - accuracy: 0.9891 - val_loss: 0.5091 - val_accuracy: 0.8750\n",
            "Epoch 190/200\n",
            "92/92 [==============================] - 0s 329us/sample - loss: 0.0807 - accuracy: 0.9674 - val_loss: 0.5223 - val_accuracy: 0.8333\n",
            "Epoch 191/200\n",
            "92/92 [==============================] - 0s 292us/sample - loss: 0.0401 - accuracy: 0.9891 - val_loss: 0.5589 - val_accuracy: 0.8333\n",
            "Epoch 192/200\n",
            "92/92 [==============================] - 0s 334us/sample - loss: 0.0980 - accuracy: 0.9783 - val_loss: 0.6154 - val_accuracy: 0.8333\n",
            "Epoch 193/200\n",
            "92/92 [==============================] - 0s 330us/sample - loss: 0.0897 - accuracy: 0.9565 - val_loss: 0.6025 - val_accuracy: 0.8333\n",
            "Epoch 194/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.1121 - accuracy: 0.9565 - val_loss: 0.5190 - val_accuracy: 0.8750\n",
            "Epoch 195/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.0420 - accuracy: 0.9891 - val_loss: 0.4733 - val_accuracy: 0.8750\n",
            "Epoch 196/200\n",
            "92/92 [==============================] - 0s 323us/sample - loss: 0.0911 - accuracy: 0.9783 - val_loss: 0.4972 - val_accuracy: 0.8750\n",
            "Epoch 197/200\n",
            "92/92 [==============================] - 0s 296us/sample - loss: 0.0508 - accuracy: 0.9891 - val_loss: 0.5520 - val_accuracy: 0.8750\n",
            "Epoch 198/200\n",
            "92/92 [==============================] - 0s 326us/sample - loss: 0.0449 - accuracy: 1.0000 - val_loss: 0.5903 - val_accuracy: 0.8750\n",
            "Epoch 199/200\n",
            "92/92 [==============================] - 0s 305us/sample - loss: 0.0747 - accuracy: 0.9674 - val_loss: 0.5970 - val_accuracy: 0.8750\n",
            "Epoch 200/200\n",
            "92/92 [==============================] - 0s 328us/sample - loss: 0.0917 - accuracy: 0.9783 - val_loss: 0.6008 - val_accuracy: 0.8750\n",
            ">#1: 87.500\n",
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e95c9840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e95c9840>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e95c9840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e95c9840>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 1s 6ms/sample - loss: 0.7074 - accuracy: 0.4783 - val_loss: 0.6319 - val_accuracy: 0.8750\n",
            "Epoch 2/200\n",
            "92/92 [==============================] - 0s 383us/sample - loss: 0.6629 - accuracy: 0.5870 - val_loss: 0.5873 - val_accuracy: 0.8333\n",
            "Epoch 3/200\n",
            "92/92 [==============================] - 0s 330us/sample - loss: 0.6523 - accuracy: 0.5978 - val_loss: 0.5578 - val_accuracy: 0.7500\n",
            "Epoch 4/200\n",
            "92/92 [==============================] - 0s 300us/sample - loss: 0.6369 - accuracy: 0.6630 - val_loss: 0.5291 - val_accuracy: 0.8333\n",
            "Epoch 5/200\n",
            "92/92 [==============================] - 0s 323us/sample - loss: 0.6339 - accuracy: 0.5978 - val_loss: 0.5011 - val_accuracy: 0.9583\n",
            "Epoch 6/200\n",
            "92/92 [==============================] - 0s 382us/sample - loss: 0.6275 - accuracy: 0.6087 - val_loss: 0.4737 - val_accuracy: 0.9167\n",
            "Epoch 7/200\n",
            "92/92 [==============================] - 0s 340us/sample - loss: 0.6089 - accuracy: 0.7283 - val_loss: 0.4477 - val_accuracy: 0.9583\n",
            "Epoch 8/200\n",
            "92/92 [==============================] - 0s 315us/sample - loss: 0.5971 - accuracy: 0.6957 - val_loss: 0.4239 - val_accuracy: 0.9583\n",
            "Epoch 9/200\n",
            "92/92 [==============================] - 0s 364us/sample - loss: 0.5700 - accuracy: 0.7826 - val_loss: 0.3968 - val_accuracy: 0.9583\n",
            "Epoch 10/200\n",
            "92/92 [==============================] - 0s 341us/sample - loss: 0.5770 - accuracy: 0.7283 - val_loss: 0.3723 - val_accuracy: 0.9583\n",
            "Epoch 11/200\n",
            "92/92 [==============================] - 0s 354us/sample - loss: 0.5463 - accuracy: 0.7391 - val_loss: 0.3534 - val_accuracy: 0.9583\n",
            "Epoch 12/200\n",
            "92/92 [==============================] - 0s 409us/sample - loss: 0.5292 - accuracy: 0.7174 - val_loss: 0.3355 - val_accuracy: 0.9583\n",
            "Epoch 13/200\n",
            "92/92 [==============================] - 0s 350us/sample - loss: 0.5084 - accuracy: 0.7717 - val_loss: 0.3202 - val_accuracy: 0.9583\n",
            "Epoch 14/200\n",
            "92/92 [==============================] - 0s 316us/sample - loss: 0.5194 - accuracy: 0.7391 - val_loss: 0.3059 - val_accuracy: 0.9583\n",
            "Epoch 15/200\n",
            "92/92 [==============================] - 0s 334us/sample - loss: 0.4843 - accuracy: 0.8043 - val_loss: 0.2946 - val_accuracy: 0.9583\n",
            "Epoch 16/200\n",
            "92/92 [==============================] - 0s 323us/sample - loss: 0.5032 - accuracy: 0.7609 - val_loss: 0.2813 - val_accuracy: 0.9583\n",
            "Epoch 17/200\n",
            "92/92 [==============================] - 0s 308us/sample - loss: 0.5287 - accuracy: 0.7283 - val_loss: 0.2751 - val_accuracy: 0.9583\n",
            "Epoch 18/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.4780 - accuracy: 0.8043 - val_loss: 0.2713 - val_accuracy: 0.9583\n",
            "Epoch 19/200\n",
            "92/92 [==============================] - 0s 316us/sample - loss: 0.4622 - accuracy: 0.8478 - val_loss: 0.2726 - val_accuracy: 0.9583\n",
            "Epoch 20/200\n",
            "92/92 [==============================] - 0s 321us/sample - loss: 0.4519 - accuracy: 0.8478 - val_loss: 0.2777 - val_accuracy: 0.9583\n",
            "Epoch 21/200\n",
            "92/92 [==============================] - 0s 366us/sample - loss: 0.4523 - accuracy: 0.8152 - val_loss: 0.2750 - val_accuracy: 0.9583\n",
            "Epoch 22/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.4584 - accuracy: 0.8152 - val_loss: 0.2782 - val_accuracy: 0.9583\n",
            "Epoch 23/200\n",
            "92/92 [==============================] - 0s 366us/sample - loss: 0.4407 - accuracy: 0.7935 - val_loss: 0.2816 - val_accuracy: 0.9583\n",
            "Epoch 24/200\n",
            "92/92 [==============================] - 0s 338us/sample - loss: 0.4597 - accuracy: 0.7283 - val_loss: 0.2834 - val_accuracy: 0.9583\n",
            "Epoch 25/200\n",
            "92/92 [==============================] - 0s 344us/sample - loss: 0.4633 - accuracy: 0.8043 - val_loss: 0.2831 - val_accuracy: 0.9583\n",
            "Epoch 26/200\n",
            "92/92 [==============================] - 0s 349us/sample - loss: 0.4387 - accuracy: 0.8152 - val_loss: 0.2798 - val_accuracy: 0.9167\n",
            "Epoch 27/200\n",
            "92/92 [==============================] - 0s 325us/sample - loss: 0.4397 - accuracy: 0.8043 - val_loss: 0.2716 - val_accuracy: 0.9583\n",
            "Epoch 28/200\n",
            "92/92 [==============================] - 0s 341us/sample - loss: 0.3827 - accuracy: 0.8804 - val_loss: 0.2660 - val_accuracy: 0.9583\n",
            "Epoch 29/200\n",
            "92/92 [==============================] - 0s 336us/sample - loss: 0.4017 - accuracy: 0.8478 - val_loss: 0.2656 - val_accuracy: 0.9583\n",
            "Epoch 30/200\n",
            "92/92 [==============================] - 0s 368us/sample - loss: 0.4137 - accuracy: 0.7935 - val_loss: 0.2641 - val_accuracy: 0.9583\n",
            "Epoch 31/200\n",
            "92/92 [==============================] - 0s 361us/sample - loss: 0.3926 - accuracy: 0.8587 - val_loss: 0.2581 - val_accuracy: 0.9583\n",
            "Epoch 32/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.3736 - accuracy: 0.8370 - val_loss: 0.2562 - val_accuracy: 0.9583\n",
            "Epoch 33/200\n",
            "92/92 [==============================] - 0s 395us/sample - loss: 0.3785 - accuracy: 0.8478 - val_loss: 0.2511 - val_accuracy: 0.9583\n",
            "Epoch 34/200\n",
            "92/92 [==============================] - 0s 350us/sample - loss: 0.3217 - accuracy: 0.8804 - val_loss: 0.2466 - val_accuracy: 0.9583\n",
            "Epoch 35/200\n",
            "92/92 [==============================] - 0s 405us/sample - loss: 0.3676 - accuracy: 0.8587 - val_loss: 0.2522 - val_accuracy: 0.9167\n",
            "Epoch 36/200\n",
            "92/92 [==============================] - 0s 438us/sample - loss: 0.3828 - accuracy: 0.8261 - val_loss: 0.2528 - val_accuracy: 0.9167\n",
            "Epoch 37/200\n",
            "92/92 [==============================] - 0s 316us/sample - loss: 0.3968 - accuracy: 0.8587 - val_loss: 0.2510 - val_accuracy: 0.9167\n",
            "Epoch 38/200\n",
            "92/92 [==============================] - 0s 362us/sample - loss: 0.3287 - accuracy: 0.9022 - val_loss: 0.2508 - val_accuracy: 0.9167\n",
            "Epoch 39/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.3502 - accuracy: 0.8804 - val_loss: 0.2517 - val_accuracy: 0.9167\n",
            "Epoch 40/200\n",
            "92/92 [==============================] - 0s 292us/sample - loss: 0.3325 - accuracy: 0.9022 - val_loss: 0.2524 - val_accuracy: 0.9167\n",
            "Epoch 41/200\n",
            "92/92 [==============================] - 0s 433us/sample - loss: 0.3062 - accuracy: 0.8587 - val_loss: 0.2550 - val_accuracy: 0.9167\n",
            "Epoch 42/200\n",
            "92/92 [==============================] - 0s 363us/sample - loss: 0.3459 - accuracy: 0.8804 - val_loss: 0.2513 - val_accuracy: 0.9167\n",
            "Epoch 43/200\n",
            "92/92 [==============================] - 0s 350us/sample - loss: 0.3267 - accuracy: 0.8804 - val_loss: 0.2547 - val_accuracy: 0.9167\n",
            "Epoch 44/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.2935 - accuracy: 0.9130 - val_loss: 0.2475 - val_accuracy: 0.9167\n",
            "Epoch 45/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.3339 - accuracy: 0.8913 - val_loss: 0.2412 - val_accuracy: 0.9167\n",
            "Epoch 46/200\n",
            "92/92 [==============================] - 0s 324us/sample - loss: 0.3144 - accuracy: 0.8696 - val_loss: 0.2372 - val_accuracy: 0.9167\n",
            "Epoch 47/200\n",
            "92/92 [==============================] - 0s 331us/sample - loss: 0.2916 - accuracy: 0.9130 - val_loss: 0.2351 - val_accuracy: 0.9167\n",
            "Epoch 48/200\n",
            "92/92 [==============================] - 0s 348us/sample - loss: 0.2912 - accuracy: 0.8804 - val_loss: 0.2373 - val_accuracy: 0.9167\n",
            "Epoch 49/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.2654 - accuracy: 0.9457 - val_loss: 0.2424 - val_accuracy: 0.9167\n",
            "Epoch 50/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.2947 - accuracy: 0.9130 - val_loss: 0.2578 - val_accuracy: 0.9167\n",
            "Epoch 51/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.2832 - accuracy: 0.9130 - val_loss: 0.2554 - val_accuracy: 0.9583\n",
            "Epoch 52/200\n",
            "92/92 [==============================] - 0s 349us/sample - loss: 0.2718 - accuracy: 0.9022 - val_loss: 0.2475 - val_accuracy: 0.9583\n",
            "Epoch 53/200\n",
            "92/92 [==============================] - 0s 379us/sample - loss: 0.2826 - accuracy: 0.8913 - val_loss: 0.2531 - val_accuracy: 0.9583\n",
            "Epoch 54/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.2642 - accuracy: 0.8913 - val_loss: 0.2604 - val_accuracy: 0.9583\n",
            "Epoch 55/200\n",
            "92/92 [==============================] - 0s 362us/sample - loss: 0.2543 - accuracy: 0.9239 - val_loss: 0.2722 - val_accuracy: 0.9167\n",
            "Epoch 56/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.2829 - accuracy: 0.8804 - val_loss: 0.2835 - val_accuracy: 0.9167\n",
            "Epoch 57/200\n",
            "92/92 [==============================] - 0s 396us/sample - loss: 0.3082 - accuracy: 0.8804 - val_loss: 0.2938 - val_accuracy: 0.8750\n",
            "Epoch 58/200\n",
            "92/92 [==============================] - 0s 310us/sample - loss: 0.3310 - accuracy: 0.8478 - val_loss: 0.2587 - val_accuracy: 0.9583\n",
            "Epoch 59/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.2716 - accuracy: 0.8804 - val_loss: 0.2477 - val_accuracy: 0.9583\n",
            "Epoch 60/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.2304 - accuracy: 0.9348 - val_loss: 0.2489 - val_accuracy: 0.9583\n",
            "Epoch 61/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.2419 - accuracy: 0.8913 - val_loss: 0.2564 - val_accuracy: 0.9583\n",
            "Epoch 62/200\n",
            "92/92 [==============================] - 0s 341us/sample - loss: 0.2528 - accuracy: 0.9130 - val_loss: 0.2586 - val_accuracy: 0.9583\n",
            "Epoch 63/200\n",
            "92/92 [==============================] - 0s 406us/sample - loss: 0.2909 - accuracy: 0.8696 - val_loss: 0.2581 - val_accuracy: 0.9583\n",
            "Epoch 64/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.2736 - accuracy: 0.9130 - val_loss: 0.2587 - val_accuracy: 0.9583\n",
            "Epoch 65/200\n",
            "92/92 [==============================] - 0s 419us/sample - loss: 0.2304 - accuracy: 0.9022 - val_loss: 0.2693 - val_accuracy: 0.9167\n",
            "Epoch 66/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.2476 - accuracy: 0.9022 - val_loss: 0.2894 - val_accuracy: 0.8750\n",
            "Epoch 67/200\n",
            "92/92 [==============================] - 0s 380us/sample - loss: 0.2058 - accuracy: 0.9239 - val_loss: 0.2999 - val_accuracy: 0.8750\n",
            "Epoch 68/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.2718 - accuracy: 0.8913 - val_loss: 0.2796 - val_accuracy: 0.9167\n",
            "Epoch 69/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.2173 - accuracy: 0.9239 - val_loss: 0.2697 - val_accuracy: 0.9167\n",
            "Epoch 70/200\n",
            "92/92 [==============================] - 0s 366us/sample - loss: 0.2381 - accuracy: 0.9239 - val_loss: 0.2681 - val_accuracy: 0.9583\n",
            "Epoch 71/200\n",
            "92/92 [==============================] - 0s 368us/sample - loss: 0.2094 - accuracy: 0.9239 - val_loss: 0.2738 - val_accuracy: 0.9583\n",
            "Epoch 72/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.2281 - accuracy: 0.9022 - val_loss: 0.2859 - val_accuracy: 0.9583\n",
            "Epoch 73/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.2192 - accuracy: 0.9457 - val_loss: 0.3061 - val_accuracy: 0.8750\n",
            "Epoch 74/200\n",
            "92/92 [==============================] - 0s 379us/sample - loss: 0.2093 - accuracy: 0.9239 - val_loss: 0.3004 - val_accuracy: 0.9167\n",
            "Epoch 75/200\n",
            "92/92 [==============================] - 0s 339us/sample - loss: 0.2218 - accuracy: 0.9239 - val_loss: 0.2968 - val_accuracy: 0.9583\n",
            "Epoch 76/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.2246 - accuracy: 0.9457 - val_loss: 0.3056 - val_accuracy: 0.9167\n",
            "Epoch 77/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.1575 - accuracy: 0.9565 - val_loss: 0.3061 - val_accuracy: 0.9167\n",
            "Epoch 78/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.2006 - accuracy: 0.9239 - val_loss: 0.2983 - val_accuracy: 0.9167\n",
            "Epoch 79/200\n",
            "92/92 [==============================] - 0s 338us/sample - loss: 0.2560 - accuracy: 0.9022 - val_loss: 0.2909 - val_accuracy: 0.9167\n",
            "Epoch 80/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.2235 - accuracy: 0.9239 - val_loss: 0.2850 - val_accuracy: 0.9167\n",
            "Epoch 81/200\n",
            "92/92 [==============================] - 0s 349us/sample - loss: 0.2276 - accuracy: 0.9022 - val_loss: 0.2888 - val_accuracy: 0.9167\n",
            "Epoch 82/200\n",
            "92/92 [==============================] - 0s 392us/sample - loss: 0.1712 - accuracy: 0.9239 - val_loss: 0.2895 - val_accuracy: 0.9583\n",
            "Epoch 83/200\n",
            "92/92 [==============================] - 0s 330us/sample - loss: 0.2206 - accuracy: 0.9130 - val_loss: 0.2979 - val_accuracy: 0.9583\n",
            "Epoch 84/200\n",
            "92/92 [==============================] - 0s 398us/sample - loss: 0.1969 - accuracy: 0.9457 - val_loss: 0.3058 - val_accuracy: 0.9167\n",
            "Epoch 85/200\n",
            "92/92 [==============================] - 0s 347us/sample - loss: 0.1740 - accuracy: 0.9348 - val_loss: 0.3111 - val_accuracy: 0.8750\n",
            "Epoch 86/200\n",
            "92/92 [==============================] - 0s 323us/sample - loss: 0.1729 - accuracy: 0.9565 - val_loss: 0.3049 - val_accuracy: 0.9167\n",
            "Epoch 87/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.2268 - accuracy: 0.9022 - val_loss: 0.2983 - val_accuracy: 0.9167\n",
            "Epoch 88/200\n",
            "92/92 [==============================] - 0s 408us/sample - loss: 0.2594 - accuracy: 0.9130 - val_loss: 0.2998 - val_accuracy: 0.9167\n",
            "Epoch 89/200\n",
            "92/92 [==============================] - 0s 346us/sample - loss: 0.1589 - accuracy: 0.9348 - val_loss: 0.3219 - val_accuracy: 0.8750\n",
            "Epoch 90/200\n",
            "92/92 [==============================] - 0s 380us/sample - loss: 0.2362 - accuracy: 0.9022 - val_loss: 0.2991 - val_accuracy: 0.9167\n",
            "Epoch 91/200\n",
            "92/92 [==============================] - 0s 371us/sample - loss: 0.1981 - accuracy: 0.9239 - val_loss: 0.2808 - val_accuracy: 0.9583\n",
            "Epoch 92/200\n",
            "92/92 [==============================] - 0s 386us/sample - loss: 0.1640 - accuracy: 0.9130 - val_loss: 0.2778 - val_accuracy: 0.9583\n",
            "Epoch 93/200\n",
            "92/92 [==============================] - 0s 392us/sample - loss: 0.1818 - accuracy: 0.9348 - val_loss: 0.2818 - val_accuracy: 0.9583\n",
            "Epoch 94/200\n",
            "92/92 [==============================] - 0s 338us/sample - loss: 0.1346 - accuracy: 0.9674 - val_loss: 0.2875 - val_accuracy: 0.9583\n",
            "Epoch 95/200\n",
            "92/92 [==============================] - 0s 454us/sample - loss: 0.1388 - accuracy: 0.9674 - val_loss: 0.2942 - val_accuracy: 0.9583\n",
            "Epoch 96/200\n",
            "92/92 [==============================] - 0s 370us/sample - loss: 0.1516 - accuracy: 0.9348 - val_loss: 0.2958 - val_accuracy: 0.9583\n",
            "Epoch 97/200\n",
            "92/92 [==============================] - 0s 364us/sample - loss: 0.2059 - accuracy: 0.8913 - val_loss: 0.3102 - val_accuracy: 0.8750\n",
            "Epoch 98/200\n",
            "92/92 [==============================] - 0s 369us/sample - loss: 0.1586 - accuracy: 0.9674 - val_loss: 0.3170 - val_accuracy: 0.8750\n",
            "Epoch 99/200\n",
            "92/92 [==============================] - 0s 368us/sample - loss: 0.1745 - accuracy: 0.9457 - val_loss: 0.2974 - val_accuracy: 0.9583\n",
            "Epoch 100/200\n",
            "92/92 [==============================] - 0s 369us/sample - loss: 0.1568 - accuracy: 0.9674 - val_loss: 0.2805 - val_accuracy: 0.9583\n",
            "Epoch 101/200\n",
            "92/92 [==============================] - 0s 371us/sample - loss: 0.1245 - accuracy: 0.9565 - val_loss: 0.2850 - val_accuracy: 0.9167\n",
            "Epoch 102/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.1505 - accuracy: 0.9457 - val_loss: 0.2910 - val_accuracy: 0.9167\n",
            "Epoch 103/200\n",
            "92/92 [==============================] - 0s 333us/sample - loss: 0.1386 - accuracy: 0.9348 - val_loss: 0.3049 - val_accuracy: 0.9167\n",
            "Epoch 104/200\n",
            "92/92 [==============================] - 0s 354us/sample - loss: 0.1600 - accuracy: 0.9457 - val_loss: 0.3181 - val_accuracy: 0.8750\n",
            "Epoch 105/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.1268 - accuracy: 0.9783 - val_loss: 0.3163 - val_accuracy: 0.8750\n",
            "Epoch 106/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.1655 - accuracy: 0.9348 - val_loss: 0.3088 - val_accuracy: 0.8750\n",
            "Epoch 107/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.1545 - accuracy: 0.9457 - val_loss: 0.2998 - val_accuracy: 0.9167\n",
            "Epoch 108/200\n",
            "92/92 [==============================] - 0s 373us/sample - loss: 0.1435 - accuracy: 0.9457 - val_loss: 0.2932 - val_accuracy: 0.9583\n",
            "Epoch 109/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.1112 - accuracy: 0.9565 - val_loss: 0.2981 - val_accuracy: 0.9167\n",
            "Epoch 110/200\n",
            "92/92 [==============================] - 0s 348us/sample - loss: 0.1167 - accuracy: 0.9891 - val_loss: 0.2952 - val_accuracy: 0.9167\n",
            "Epoch 111/200\n",
            "92/92 [==============================] - 0s 375us/sample - loss: 0.1042 - accuracy: 0.9674 - val_loss: 0.2947 - val_accuracy: 0.9167\n",
            "Epoch 112/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.1353 - accuracy: 0.9239 - val_loss: 0.2931 - val_accuracy: 0.9167\n",
            "Epoch 113/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.1726 - accuracy: 0.9239 - val_loss: 0.2905 - val_accuracy: 0.9583\n",
            "Epoch 114/200\n",
            "92/92 [==============================] - 0s 400us/sample - loss: 0.1121 - accuracy: 0.9783 - val_loss: 0.2942 - val_accuracy: 0.9583\n",
            "Epoch 115/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.1295 - accuracy: 0.9457 - val_loss: 0.3118 - val_accuracy: 0.9583\n",
            "Epoch 116/200\n",
            "92/92 [==============================] - 0s 368us/sample - loss: 0.1014 - accuracy: 0.9674 - val_loss: 0.3422 - val_accuracy: 0.8333\n",
            "Epoch 117/200\n",
            "92/92 [==============================] - 0s 344us/sample - loss: 0.1390 - accuracy: 0.9239 - val_loss: 0.3839 - val_accuracy: 0.8333\n",
            "Epoch 118/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.1665 - accuracy: 0.9457 - val_loss: 0.3803 - val_accuracy: 0.7917\n",
            "Epoch 119/200\n",
            "92/92 [==============================] - 0s 425us/sample - loss: 0.1221 - accuracy: 0.9565 - val_loss: 0.3459 - val_accuracy: 0.8750\n",
            "Epoch 120/200\n",
            "92/92 [==============================] - 0s 426us/sample - loss: 0.0968 - accuracy: 0.9674 - val_loss: 0.3294 - val_accuracy: 0.9167\n",
            "Epoch 121/200\n",
            "92/92 [==============================] - 0s 368us/sample - loss: 0.1454 - accuracy: 0.9565 - val_loss: 0.3487 - val_accuracy: 0.8750\n",
            "Epoch 122/200\n",
            "92/92 [==============================] - 0s 395us/sample - loss: 0.1067 - accuracy: 0.9457 - val_loss: 0.3871 - val_accuracy: 0.8750\n",
            "Epoch 123/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.1164 - accuracy: 0.9348 - val_loss: 0.3965 - val_accuracy: 0.8750\n",
            "Epoch 124/200\n",
            "92/92 [==============================] - 0s 298us/sample - loss: 0.1229 - accuracy: 0.9783 - val_loss: 0.3754 - val_accuracy: 0.8750\n",
            "Epoch 125/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.0972 - accuracy: 0.9565 - val_loss: 0.3605 - val_accuracy: 0.8750\n",
            "Epoch 126/200\n",
            "92/92 [==============================] - 0s 328us/sample - loss: 0.0890 - accuracy: 0.9783 - val_loss: 0.3559 - val_accuracy: 0.8750\n",
            "Epoch 127/200\n",
            "92/92 [==============================] - 0s 315us/sample - loss: 0.1255 - accuracy: 0.9457 - val_loss: 0.3669 - val_accuracy: 0.8750\n",
            "Epoch 128/200\n",
            "92/92 [==============================] - 0s 366us/sample - loss: 0.1056 - accuracy: 0.9783 - val_loss: 0.3687 - val_accuracy: 0.8333\n",
            "Epoch 129/200\n",
            "92/92 [==============================] - 0s 329us/sample - loss: 0.1445 - accuracy: 0.9457 - val_loss: 0.3565 - val_accuracy: 0.7917\n",
            "Epoch 130/200\n",
            "92/92 [==============================] - 0s 390us/sample - loss: 0.0876 - accuracy: 0.9783 - val_loss: 0.3462 - val_accuracy: 0.9167\n",
            "Epoch 131/200\n",
            "92/92 [==============================] - 0s 395us/sample - loss: 0.1111 - accuracy: 0.9674 - val_loss: 0.3335 - val_accuracy: 0.9583\n",
            "Epoch 132/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.0906 - accuracy: 0.9565 - val_loss: 0.3483 - val_accuracy: 0.9583\n",
            "Epoch 133/200\n",
            "92/92 [==============================] - 0s 397us/sample - loss: 0.1232 - accuracy: 0.9457 - val_loss: 0.3636 - val_accuracy: 0.9167\n",
            "Epoch 134/200\n",
            "92/92 [==============================] - 0s 337us/sample - loss: 0.1695 - accuracy: 0.9457 - val_loss: 0.4148 - val_accuracy: 0.8333\n",
            "Epoch 135/200\n",
            "92/92 [==============================] - 0s 396us/sample - loss: 0.1270 - accuracy: 0.9674 - val_loss: 0.4431 - val_accuracy: 0.8333\n",
            "Epoch 136/200\n",
            "92/92 [==============================] - 0s 344us/sample - loss: 0.0831 - accuracy: 0.9783 - val_loss: 0.4359 - val_accuracy: 0.8333\n",
            "Epoch 137/200\n",
            "92/92 [==============================] - 0s 460us/sample - loss: 0.0769 - accuracy: 0.9891 - val_loss: 0.4330 - val_accuracy: 0.7917\n",
            "Epoch 138/200\n",
            "92/92 [==============================] - 0s 423us/sample - loss: 0.0892 - accuracy: 0.9565 - val_loss: 0.4279 - val_accuracy: 0.8333\n",
            "Epoch 139/200\n",
            "92/92 [==============================] - 0s 403us/sample - loss: 0.1751 - accuracy: 0.9565 - val_loss: 0.4354 - val_accuracy: 0.7917\n",
            "Epoch 140/200\n",
            "92/92 [==============================] - 0s 426us/sample - loss: 0.1288 - accuracy: 0.9565 - val_loss: 0.4181 - val_accuracy: 0.7917\n",
            "Epoch 141/200\n",
            "92/92 [==============================] - 0s 478us/sample - loss: 0.1355 - accuracy: 0.9565 - val_loss: 0.4125 - val_accuracy: 0.8750\n",
            "Epoch 142/200\n",
            "92/92 [==============================] - 0s 368us/sample - loss: 0.0893 - accuracy: 0.9783 - val_loss: 0.3960 - val_accuracy: 0.8750\n",
            "Epoch 143/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.0974 - accuracy: 0.9457 - val_loss: 0.3678 - val_accuracy: 0.8750\n",
            "Epoch 144/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.1182 - accuracy: 0.9674 - val_loss: 0.3458 - val_accuracy: 0.9583\n",
            "Epoch 145/200\n",
            "92/92 [==============================] - 0s 334us/sample - loss: 0.0898 - accuracy: 0.9891 - val_loss: 0.3417 - val_accuracy: 0.9583\n",
            "Epoch 146/200\n",
            "92/92 [==============================] - 0s 351us/sample - loss: 0.1184 - accuracy: 0.9565 - val_loss: 0.3562 - val_accuracy: 0.8750\n",
            "Epoch 147/200\n",
            "92/92 [==============================] - 0s 438us/sample - loss: 0.1099 - accuracy: 0.9565 - val_loss: 0.4040 - val_accuracy: 0.8750\n",
            "Epoch 148/200\n",
            "92/92 [==============================] - 0s 361us/sample - loss: 0.0913 - accuracy: 0.9891 - val_loss: 0.4402 - val_accuracy: 0.7917\n",
            "Epoch 149/200\n",
            "92/92 [==============================] - 0s 373us/sample - loss: 0.0703 - accuracy: 0.9891 - val_loss: 0.4280 - val_accuracy: 0.8333\n",
            "Epoch 150/200\n",
            "92/92 [==============================] - 0s 367us/sample - loss: 0.1041 - accuracy: 0.9565 - val_loss: 0.4092 - val_accuracy: 0.8750\n",
            "Epoch 151/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.0980 - accuracy: 0.9674 - val_loss: 0.3842 - val_accuracy: 0.9167\n",
            "Epoch 152/200\n",
            "92/92 [==============================] - 0s 427us/sample - loss: 0.0728 - accuracy: 0.9891 - val_loss: 0.3765 - val_accuracy: 0.9583\n",
            "Epoch 153/200\n",
            "92/92 [==============================] - 0s 412us/sample - loss: 0.0907 - accuracy: 0.9565 - val_loss: 0.3763 - val_accuracy: 0.9583\n",
            "Epoch 154/200\n",
            "92/92 [==============================] - 0s 389us/sample - loss: 0.0916 - accuracy: 0.9783 - val_loss: 0.3735 - val_accuracy: 0.9167\n",
            "Epoch 155/200\n",
            "92/92 [==============================] - 0s 437us/sample - loss: 0.0644 - accuracy: 0.9783 - val_loss: 0.3827 - val_accuracy: 0.8750\n",
            "Epoch 156/200\n",
            "92/92 [==============================] - 0s 406us/sample - loss: 0.0927 - accuracy: 0.9674 - val_loss: 0.4042 - val_accuracy: 0.8750\n",
            "Epoch 157/200\n",
            "92/92 [==============================] - 0s 371us/sample - loss: 0.0901 - accuracy: 0.9783 - val_loss: 0.3937 - val_accuracy: 0.8750\n",
            "Epoch 158/200\n",
            "92/92 [==============================] - 0s 409us/sample - loss: 0.1008 - accuracy: 0.9783 - val_loss: 0.3860 - val_accuracy: 0.8750\n",
            "Epoch 159/200\n",
            "92/92 [==============================] - 0s 426us/sample - loss: 0.0656 - accuracy: 1.0000 - val_loss: 0.3892 - val_accuracy: 0.8750\n",
            "Epoch 160/200\n",
            "92/92 [==============================] - 0s 403us/sample - loss: 0.0773 - accuracy: 0.9783 - val_loss: 0.3941 - val_accuracy: 0.8750\n",
            "Epoch 161/200\n",
            "92/92 [==============================] - 0s 379us/sample - loss: 0.0548 - accuracy: 0.9783 - val_loss: 0.3951 - val_accuracy: 0.9167\n",
            "Epoch 162/200\n",
            "92/92 [==============================] - 0s 440us/sample - loss: 0.1024 - accuracy: 0.9565 - val_loss: 0.3929 - val_accuracy: 0.8750\n",
            "Epoch 163/200\n",
            "92/92 [==============================] - 0s 432us/sample - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.4058 - val_accuracy: 0.8333\n",
            "Epoch 164/200\n",
            "92/92 [==============================] - 0s 393us/sample - loss: 0.0834 - accuracy: 0.9674 - val_loss: 0.3954 - val_accuracy: 0.8750\n",
            "Epoch 165/200\n",
            "92/92 [==============================] - 0s 406us/sample - loss: 0.0710 - accuracy: 0.9783 - val_loss: 0.4018 - val_accuracy: 0.9167\n",
            "Epoch 166/200\n",
            "92/92 [==============================] - 0s 426us/sample - loss: 0.0556 - accuracy: 1.0000 - val_loss: 0.4110 - val_accuracy: 0.9167\n",
            "Epoch 167/200\n",
            "92/92 [==============================] - 0s 425us/sample - loss: 0.0960 - accuracy: 0.9783 - val_loss: 0.4064 - val_accuracy: 0.9167\n",
            "Epoch 168/200\n",
            "92/92 [==============================] - 0s 468us/sample - loss: 0.0544 - accuracy: 0.9891 - val_loss: 0.3958 - val_accuracy: 0.9167\n",
            "Epoch 169/200\n",
            "92/92 [==============================] - 0s 412us/sample - loss: 0.0583 - accuracy: 0.9783 - val_loss: 0.4013 - val_accuracy: 0.9167\n",
            "Epoch 170/200\n",
            "92/92 [==============================] - 0s 428us/sample - loss: 0.0636 - accuracy: 0.9891 - val_loss: 0.4187 - val_accuracy: 0.8750\n",
            "Epoch 171/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.0562 - accuracy: 0.9783 - val_loss: 0.4168 - val_accuracy: 0.8750\n",
            "Epoch 172/200\n",
            "92/92 [==============================] - 0s 395us/sample - loss: 0.0757 - accuracy: 0.9783 - val_loss: 0.4016 - val_accuracy: 0.8750\n",
            "Epoch 173/200\n",
            "92/92 [==============================] - 0s 391us/sample - loss: 0.0514 - accuracy: 0.9783 - val_loss: 0.3884 - val_accuracy: 0.8750\n",
            "Epoch 174/200\n",
            "92/92 [==============================] - 0s 383us/sample - loss: 0.0906 - accuracy: 0.9674 - val_loss: 0.4006 - val_accuracy: 0.8750\n",
            "Epoch 175/200\n",
            "92/92 [==============================] - 0s 413us/sample - loss: 0.0518 - accuracy: 0.9783 - val_loss: 0.4470 - val_accuracy: 0.8333\n",
            "Epoch 176/200\n",
            "92/92 [==============================] - 0s 443us/sample - loss: 0.0658 - accuracy: 1.0000 - val_loss: 0.5007 - val_accuracy: 0.7917\n",
            "Epoch 177/200\n",
            "92/92 [==============================] - 0s 410us/sample - loss: 0.0757 - accuracy: 0.9783 - val_loss: 0.4890 - val_accuracy: 0.8333\n",
            "Epoch 178/200\n",
            "92/92 [==============================] - 0s 426us/sample - loss: 0.0794 - accuracy: 0.9674 - val_loss: 0.4576 - val_accuracy: 0.8750\n",
            "Epoch 179/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.0482 - accuracy: 1.0000 - val_loss: 0.4438 - val_accuracy: 0.8750\n",
            "Epoch 180/200\n",
            "92/92 [==============================] - 0s 331us/sample - loss: 0.0622 - accuracy: 0.9891 - val_loss: 0.4439 - val_accuracy: 0.8750\n",
            "Epoch 181/200\n",
            "92/92 [==============================] - 0s 387us/sample - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.4540 - val_accuracy: 0.8750\n",
            "Epoch 182/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.0910 - accuracy: 0.9674 - val_loss: 0.5116 - val_accuracy: 0.8750\n",
            "Epoch 183/200\n",
            "92/92 [==============================] - 0s 383us/sample - loss: 0.0633 - accuracy: 0.9891 - val_loss: 0.5714 - val_accuracy: 0.8333\n",
            "Epoch 184/200\n",
            "92/92 [==============================] - 0s 386us/sample - loss: 0.0996 - accuracy: 0.9457 - val_loss: 0.5733 - val_accuracy: 0.7917\n",
            "Epoch 185/200\n",
            "92/92 [==============================] - 0s 404us/sample - loss: 0.0716 - accuracy: 0.9674 - val_loss: 0.5497 - val_accuracy: 0.8333\n",
            "Epoch 186/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.1111 - accuracy: 0.9674 - val_loss: 0.4992 - val_accuracy: 0.9167\n",
            "Epoch 187/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.0792 - accuracy: 0.9565 - val_loss: 0.4741 - val_accuracy: 0.9167\n",
            "Epoch 188/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.0809 - accuracy: 0.9674 - val_loss: 0.4751 - val_accuracy: 0.9167\n",
            "Epoch 189/200\n",
            "92/92 [==============================] - 0s 316us/sample - loss: 0.0563 - accuracy: 0.9783 - val_loss: 0.4797 - val_accuracy: 0.9167\n",
            "Epoch 190/200\n",
            "92/92 [==============================] - 0s 367us/sample - loss: 0.0872 - accuracy: 0.9783 - val_loss: 0.4796 - val_accuracy: 0.8750\n",
            "Epoch 191/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.0567 - accuracy: 0.9891 - val_loss: 0.4857 - val_accuracy: 0.8750\n",
            "Epoch 192/200\n",
            "92/92 [==============================] - 0s 368us/sample - loss: 0.0541 - accuracy: 0.9891 - val_loss: 0.4844 - val_accuracy: 0.8333\n",
            "Epoch 193/200\n",
            "92/92 [==============================] - 0s 363us/sample - loss: 0.0410 - accuracy: 0.9891 - val_loss: 0.4934 - val_accuracy: 0.7917\n",
            "Epoch 194/200\n",
            "92/92 [==============================] - 0s 380us/sample - loss: 0.0456 - accuracy: 0.9891 - val_loss: 0.4908 - val_accuracy: 0.7917\n",
            "Epoch 195/200\n",
            "92/92 [==============================] - 0s 391us/sample - loss: 0.0406 - accuracy: 1.0000 - val_loss: 0.4828 - val_accuracy: 0.7917\n",
            "Epoch 196/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.0299 - accuracy: 0.9891 - val_loss: 0.4722 - val_accuracy: 0.8333\n",
            "Epoch 197/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.4626 - val_accuracy: 0.8750\n",
            "Epoch 198/200\n",
            "92/92 [==============================] - 0s 457us/sample - loss: 0.0499 - accuracy: 0.9891 - val_loss: 0.4722 - val_accuracy: 0.8750\n",
            "Epoch 199/200\n",
            "92/92 [==============================] - 0s 340us/sample - loss: 0.0555 - accuracy: 1.0000 - val_loss: 0.4987 - val_accuracy: 0.8750\n",
            "Epoch 200/200\n",
            "92/92 [==============================] - 0s 400us/sample - loss: 0.0421 - accuracy: 0.9891 - val_loss: 0.5032 - val_accuracy: 0.8750\n",
            ">#2: 87.500\n",
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbaac158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbaac158>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbaac158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbaac158>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 1s 6ms/sample - loss: 0.6928 - accuracy: 0.5435 - val_loss: 0.6478 - val_accuracy: 0.7500\n",
            "Epoch 2/200\n",
            "92/92 [==============================] - 0s 393us/sample - loss: 0.6729 - accuracy: 0.5652 - val_loss: 0.6041 - val_accuracy: 0.6250\n",
            "Epoch 3/200\n",
            "92/92 [==============================] - 0s 313us/sample - loss: 0.6502 - accuracy: 0.6087 - val_loss: 0.5723 - val_accuracy: 0.7083\n",
            "Epoch 4/200\n",
            "92/92 [==============================] - 0s 363us/sample - loss: 0.6314 - accuracy: 0.6848 - val_loss: 0.5362 - val_accuracy: 0.7500\n",
            "Epoch 5/200\n",
            "92/92 [==============================] - 0s 404us/sample - loss: 0.6155 - accuracy: 0.7065 - val_loss: 0.5004 - val_accuracy: 0.8750\n",
            "Epoch 6/200\n",
            "92/92 [==============================] - 0s 408us/sample - loss: 0.6099 - accuracy: 0.7283 - val_loss: 0.4665 - val_accuracy: 0.9167\n",
            "Epoch 7/200\n",
            "92/92 [==============================] - 0s 394us/sample - loss: 0.6026 - accuracy: 0.6848 - val_loss: 0.4333 - val_accuracy: 0.9167\n",
            "Epoch 8/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.5787 - accuracy: 0.7391 - val_loss: 0.4019 - val_accuracy: 0.9167\n",
            "Epoch 9/200\n",
            "92/92 [==============================] - 0s 355us/sample - loss: 0.5531 - accuracy: 0.7717 - val_loss: 0.3711 - val_accuracy: 0.9167\n",
            "Epoch 10/200\n",
            "92/92 [==============================] - 0s 332us/sample - loss: 0.5623 - accuracy: 0.7065 - val_loss: 0.3435 - val_accuracy: 0.9167\n",
            "Epoch 11/200\n",
            "92/92 [==============================] - 0s 403us/sample - loss: 0.5654 - accuracy: 0.6957 - val_loss: 0.3228 - val_accuracy: 0.9583\n",
            "Epoch 12/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.5326 - accuracy: 0.7283 - val_loss: 0.3061 - val_accuracy: 0.9583\n",
            "Epoch 13/200\n",
            "92/92 [==============================] - 0s 390us/sample - loss: 0.5202 - accuracy: 0.7717 - val_loss: 0.2920 - val_accuracy: 0.9583\n",
            "Epoch 14/200\n",
            "92/92 [==============================] - 0s 574us/sample - loss: 0.4998 - accuracy: 0.7935 - val_loss: 0.2850 - val_accuracy: 0.9167\n",
            "Epoch 15/200\n",
            "92/92 [==============================] - 0s 420us/sample - loss: 0.5057 - accuracy: 0.7717 - val_loss: 0.2741 - val_accuracy: 0.9167\n",
            "Epoch 16/200\n",
            "92/92 [==============================] - 0s 392us/sample - loss: 0.5068 - accuracy: 0.7283 - val_loss: 0.2671 - val_accuracy: 0.9583\n",
            "Epoch 17/200\n",
            "92/92 [==============================] - 0s 379us/sample - loss: 0.4945 - accuracy: 0.7935 - val_loss: 0.2623 - val_accuracy: 0.9583\n",
            "Epoch 18/200\n",
            "92/92 [==============================] - 0s 383us/sample - loss: 0.4566 - accuracy: 0.7935 - val_loss: 0.2569 - val_accuracy: 0.9583\n",
            "Epoch 19/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.4616 - accuracy: 0.8152 - val_loss: 0.2561 - val_accuracy: 0.9583\n",
            "Epoch 20/200\n",
            "92/92 [==============================] - 0s 411us/sample - loss: 0.4398 - accuracy: 0.8043 - val_loss: 0.2591 - val_accuracy: 0.9167\n",
            "Epoch 21/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.4777 - accuracy: 0.7609 - val_loss: 0.2628 - val_accuracy: 0.9167\n",
            "Epoch 22/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.4360 - accuracy: 0.7826 - val_loss: 0.2592 - val_accuracy: 0.8750\n",
            "Epoch 23/200\n",
            "92/92 [==============================] - 0s 332us/sample - loss: 0.4438 - accuracy: 0.7935 - val_loss: 0.2443 - val_accuracy: 0.9583\n",
            "Epoch 24/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.4876 - accuracy: 0.8043 - val_loss: 0.2382 - val_accuracy: 0.9583\n",
            "Epoch 25/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.4217 - accuracy: 0.8478 - val_loss: 0.2386 - val_accuracy: 0.9583\n",
            "Epoch 26/200\n",
            "92/92 [==============================] - 0s 387us/sample - loss: 0.3956 - accuracy: 0.8152 - val_loss: 0.2432 - val_accuracy: 0.9167\n",
            "Epoch 27/200\n",
            "92/92 [==============================] - 0s 333us/sample - loss: 0.4406 - accuracy: 0.7826 - val_loss: 0.2512 - val_accuracy: 0.9167\n",
            "Epoch 28/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.4012 - accuracy: 0.8152 - val_loss: 0.2486 - val_accuracy: 0.9167\n",
            "Epoch 29/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.3748 - accuracy: 0.8370 - val_loss: 0.2454 - val_accuracy: 0.9167\n",
            "Epoch 30/200\n",
            "92/92 [==============================] - 0s 407us/sample - loss: 0.4718 - accuracy: 0.7283 - val_loss: 0.2523 - val_accuracy: 0.9167\n",
            "Epoch 31/200\n",
            "92/92 [==============================] - 0s 320us/sample - loss: 0.4352 - accuracy: 0.8043 - val_loss: 0.2640 - val_accuracy: 0.9167\n",
            "Epoch 32/200\n",
            "92/92 [==============================] - 0s 318us/sample - loss: 0.3885 - accuracy: 0.8261 - val_loss: 0.2563 - val_accuracy: 0.9167\n",
            "Epoch 33/200\n",
            "92/92 [==============================] - 0s 319us/sample - loss: 0.3947 - accuracy: 0.8152 - val_loss: 0.2514 - val_accuracy: 0.9167\n",
            "Epoch 34/200\n",
            "92/92 [==============================] - 0s 434us/sample - loss: 0.3429 - accuracy: 0.8913 - val_loss: 0.2441 - val_accuracy: 0.9167\n",
            "Epoch 35/200\n",
            "92/92 [==============================] - 0s 452us/sample - loss: 0.3448 - accuracy: 0.8804 - val_loss: 0.2450 - val_accuracy: 0.9167\n",
            "Epoch 36/200\n",
            "92/92 [==============================] - 0s 403us/sample - loss: 0.3561 - accuracy: 0.8804 - val_loss: 0.2466 - val_accuracy: 0.9167\n",
            "Epoch 37/200\n",
            "92/92 [==============================] - 0s 444us/sample - loss: 0.3378 - accuracy: 0.8478 - val_loss: 0.2493 - val_accuracy: 0.9167\n",
            "Epoch 38/200\n",
            "92/92 [==============================] - 0s 446us/sample - loss: 0.3601 - accuracy: 0.8587 - val_loss: 0.2496 - val_accuracy: 0.9167\n",
            "Epoch 39/200\n",
            "92/92 [==============================] - 0s 474us/sample - loss: 0.3385 - accuracy: 0.8913 - val_loss: 0.2379 - val_accuracy: 0.9167\n",
            "Epoch 40/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.3421 - accuracy: 0.8804 - val_loss: 0.2413 - val_accuracy: 0.9167\n",
            "Epoch 41/200\n",
            "92/92 [==============================] - 0s 431us/sample - loss: 0.3245 - accuracy: 0.8804 - val_loss: 0.2540 - val_accuracy: 0.9167\n",
            "Epoch 42/200\n",
            "92/92 [==============================] - 0s 447us/sample - loss: 0.3528 - accuracy: 0.8804 - val_loss: 0.2572 - val_accuracy: 0.9167\n",
            "Epoch 43/200\n",
            "92/92 [==============================] - 0s 498us/sample - loss: 0.3338 - accuracy: 0.8261 - val_loss: 0.2487 - val_accuracy: 0.9167\n",
            "Epoch 44/200\n",
            "92/92 [==============================] - 0s 450us/sample - loss: 0.3179 - accuracy: 0.8587 - val_loss: 0.2411 - val_accuracy: 0.9167\n",
            "Epoch 45/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.3505 - accuracy: 0.8696 - val_loss: 0.2428 - val_accuracy: 0.9167\n",
            "Epoch 46/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.3135 - accuracy: 0.8696 - val_loss: 0.2410 - val_accuracy: 0.9167\n",
            "Epoch 47/200\n",
            "92/92 [==============================] - 0s 407us/sample - loss: 0.2848 - accuracy: 0.9022 - val_loss: 0.2377 - val_accuracy: 0.9167\n",
            "Epoch 48/200\n",
            "92/92 [==============================] - 0s 398us/sample - loss: 0.3301 - accuracy: 0.8696 - val_loss: 0.2370 - val_accuracy: 0.9167\n",
            "Epoch 49/200\n",
            "92/92 [==============================] - 0s 411us/sample - loss: 0.3237 - accuracy: 0.9130 - val_loss: 0.2379 - val_accuracy: 0.9167\n",
            "Epoch 50/200\n",
            "92/92 [==============================] - 0s 504us/sample - loss: 0.2732 - accuracy: 0.9022 - val_loss: 0.2353 - val_accuracy: 0.9167\n",
            "Epoch 51/200\n",
            "92/92 [==============================] - 0s 348us/sample - loss: 0.2802 - accuracy: 0.8913 - val_loss: 0.2419 - val_accuracy: 0.9167\n",
            "Epoch 52/200\n",
            "92/92 [==============================] - 0s 450us/sample - loss: 0.3044 - accuracy: 0.8913 - val_loss: 0.2515 - val_accuracy: 0.9167\n",
            "Epoch 53/200\n",
            "92/92 [==============================] - 0s 375us/sample - loss: 0.2547 - accuracy: 0.9348 - val_loss: 0.2472 - val_accuracy: 0.9167\n",
            "Epoch 54/200\n",
            "92/92 [==============================] - 0s 383us/sample - loss: 0.3293 - accuracy: 0.8587 - val_loss: 0.2315 - val_accuracy: 0.9167\n",
            "Epoch 55/200\n",
            "92/92 [==============================] - 0s 413us/sample - loss: 0.2845 - accuracy: 0.8804 - val_loss: 0.2266 - val_accuracy: 0.9583\n",
            "Epoch 56/200\n",
            "92/92 [==============================] - 0s 413us/sample - loss: 0.3450 - accuracy: 0.8587 - val_loss: 0.2249 - val_accuracy: 0.9583\n",
            "Epoch 57/200\n",
            "92/92 [==============================] - 0s 585us/sample - loss: 0.2780 - accuracy: 0.8587 - val_loss: 0.2337 - val_accuracy: 0.9167\n",
            "Epoch 58/200\n",
            "92/92 [==============================] - 0s 462us/sample - loss: 0.2685 - accuracy: 0.9022 - val_loss: 0.2425 - val_accuracy: 0.8750\n",
            "Epoch 59/200\n",
            "92/92 [==============================] - 0s 425us/sample - loss: 0.2582 - accuracy: 0.9130 - val_loss: 0.2456 - val_accuracy: 0.8750\n",
            "Epoch 60/200\n",
            "92/92 [==============================] - 0s 355us/sample - loss: 0.2372 - accuracy: 0.9130 - val_loss: 0.2456 - val_accuracy: 0.9167\n",
            "Epoch 61/200\n",
            "92/92 [==============================] - 0s 391us/sample - loss: 0.2718 - accuracy: 0.9022 - val_loss: 0.2392 - val_accuracy: 0.9167\n",
            "Epoch 62/200\n",
            "92/92 [==============================] - 0s 444us/sample - loss: 0.2333 - accuracy: 0.9239 - val_loss: 0.2351 - val_accuracy: 0.9167\n",
            "Epoch 63/200\n",
            "92/92 [==============================] - 0s 393us/sample - loss: 0.2333 - accuracy: 0.9130 - val_loss: 0.2336 - val_accuracy: 0.9167\n",
            "Epoch 64/200\n",
            "92/92 [==============================] - 0s 389us/sample - loss: 0.2634 - accuracy: 0.8913 - val_loss: 0.2295 - val_accuracy: 0.9167\n",
            "Epoch 65/200\n",
            "92/92 [==============================] - 0s 399us/sample - loss: 0.2918 - accuracy: 0.8804 - val_loss: 0.2427 - val_accuracy: 0.9167\n",
            "Epoch 66/200\n",
            "92/92 [==============================] - 0s 422us/sample - loss: 0.2683 - accuracy: 0.8696 - val_loss: 0.2535 - val_accuracy: 0.9167\n",
            "Epoch 67/200\n",
            "92/92 [==============================] - 0s 514us/sample - loss: 0.2729 - accuracy: 0.8804 - val_loss: 0.2486 - val_accuracy: 0.9167\n",
            "Epoch 68/200\n",
            "92/92 [==============================] - 0s 418us/sample - loss: 0.2986 - accuracy: 0.8913 - val_loss: 0.2550 - val_accuracy: 0.8750\n",
            "Epoch 69/200\n",
            "92/92 [==============================] - 0s 355us/sample - loss: 0.2588 - accuracy: 0.9457 - val_loss: 0.2548 - val_accuracy: 0.8750\n",
            "Epoch 70/200\n",
            "92/92 [==============================] - 0s 457us/sample - loss: 0.2309 - accuracy: 0.9130 - val_loss: 0.2565 - val_accuracy: 0.8750\n",
            "Epoch 71/200\n",
            "92/92 [==============================] - 0s 400us/sample - loss: 0.2146 - accuracy: 0.9457 - val_loss: 0.2485 - val_accuracy: 0.8750\n",
            "Epoch 72/200\n",
            "92/92 [==============================] - 0s 498us/sample - loss: 0.2103 - accuracy: 0.9348 - val_loss: 0.2439 - val_accuracy: 0.9167\n",
            "Epoch 73/200\n",
            "92/92 [==============================] - 0s 455us/sample - loss: 0.2318 - accuracy: 0.9239 - val_loss: 0.2438 - val_accuracy: 0.9167\n",
            "Epoch 74/200\n",
            "92/92 [==============================] - 0s 371us/sample - loss: 0.2085 - accuracy: 0.9348 - val_loss: 0.2526 - val_accuracy: 0.9167\n",
            "Epoch 75/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.2000 - accuracy: 0.9565 - val_loss: 0.2505 - val_accuracy: 0.8750\n",
            "Epoch 76/200\n",
            "92/92 [==============================] - 0s 449us/sample - loss: 0.2175 - accuracy: 0.9022 - val_loss: 0.2586 - val_accuracy: 0.8750\n",
            "Epoch 77/200\n",
            "92/92 [==============================] - 0s 392us/sample - loss: 0.1892 - accuracy: 0.9457 - val_loss: 0.2658 - val_accuracy: 0.8750\n",
            "Epoch 78/200\n",
            "92/92 [==============================] - 0s 452us/sample - loss: 0.1856 - accuracy: 0.9239 - val_loss: 0.2705 - val_accuracy: 0.8750\n",
            "Epoch 79/200\n",
            "92/92 [==============================] - 0s 398us/sample - loss: 0.1966 - accuracy: 0.9239 - val_loss: 0.2658 - val_accuracy: 0.8750\n",
            "Epoch 80/200\n",
            "92/92 [==============================] - 0s 431us/sample - loss: 0.2169 - accuracy: 0.9022 - val_loss: 0.2716 - val_accuracy: 0.9167\n",
            "Epoch 81/200\n",
            "92/92 [==============================] - 0s 361us/sample - loss: 0.2302 - accuracy: 0.8913 - val_loss: 0.2835 - val_accuracy: 0.8750\n",
            "Epoch 82/200\n",
            "92/92 [==============================] - 0s 496us/sample - loss: 0.2481 - accuracy: 0.9022 - val_loss: 0.2950 - val_accuracy: 0.8750\n",
            "Epoch 83/200\n",
            "92/92 [==============================] - 0s 403us/sample - loss: 0.1387 - accuracy: 0.9783 - val_loss: 0.2919 - val_accuracy: 0.8750\n",
            "Epoch 84/200\n",
            "92/92 [==============================] - 0s 431us/sample - loss: 0.2515 - accuracy: 0.9348 - val_loss: 0.2846 - val_accuracy: 0.9167\n",
            "Epoch 85/200\n",
            "92/92 [==============================] - 0s 416us/sample - loss: 0.1776 - accuracy: 0.9457 - val_loss: 0.2729 - val_accuracy: 0.9167\n",
            "Epoch 86/200\n",
            "92/92 [==============================] - 0s 480us/sample - loss: 0.1544 - accuracy: 0.9565 - val_loss: 0.2728 - val_accuracy: 0.9167\n",
            "Epoch 87/200\n",
            "92/92 [==============================] - 0s 405us/sample - loss: 0.1642 - accuracy: 0.9348 - val_loss: 0.2816 - val_accuracy: 0.9167\n",
            "Epoch 88/200\n",
            "92/92 [==============================] - 0s 391us/sample - loss: 0.1324 - accuracy: 0.9674 - val_loss: 0.2854 - val_accuracy: 0.9167\n",
            "Epoch 89/200\n",
            "92/92 [==============================] - 0s 450us/sample - loss: 0.1592 - accuracy: 0.9783 - val_loss: 0.2776 - val_accuracy: 0.9167\n",
            "Epoch 90/200\n",
            "92/92 [==============================] - 0s 440us/sample - loss: 0.1765 - accuracy: 0.9239 - val_loss: 0.2710 - val_accuracy: 0.9167\n",
            "Epoch 91/200\n",
            "92/92 [==============================] - 0s 412us/sample - loss: 0.1419 - accuracy: 0.9457 - val_loss: 0.2705 - val_accuracy: 0.9167\n",
            "Epoch 92/200\n",
            "92/92 [==============================] - 0s 499us/sample - loss: 0.1269 - accuracy: 0.9565 - val_loss: 0.2691 - val_accuracy: 0.9167\n",
            "Epoch 93/200\n",
            "92/92 [==============================] - 0s 454us/sample - loss: 0.1574 - accuracy: 0.9565 - val_loss: 0.2748 - val_accuracy: 0.9167\n",
            "Epoch 94/200\n",
            "92/92 [==============================] - 0s 444us/sample - loss: 0.1492 - accuracy: 0.9457 - val_loss: 0.2828 - val_accuracy: 0.9167\n",
            "Epoch 95/200\n",
            "92/92 [==============================] - 0s 441us/sample - loss: 0.1843 - accuracy: 0.9022 - val_loss: 0.2955 - val_accuracy: 0.8750\n",
            "Epoch 96/200\n",
            "92/92 [==============================] - 0s 400us/sample - loss: 0.1560 - accuracy: 0.9674 - val_loss: 0.2990 - val_accuracy: 0.8750\n",
            "Epoch 97/200\n",
            "92/92 [==============================] - 0s 411us/sample - loss: 0.1642 - accuracy: 0.9130 - val_loss: 0.2864 - val_accuracy: 0.9167\n",
            "Epoch 98/200\n",
            "92/92 [==============================] - 0s 422us/sample - loss: 0.1375 - accuracy: 0.9457 - val_loss: 0.2709 - val_accuracy: 0.9167\n",
            "Epoch 99/200\n",
            "92/92 [==============================] - 0s 422us/sample - loss: 0.1813 - accuracy: 0.9130 - val_loss: 0.2668 - val_accuracy: 0.9167\n",
            "Epoch 100/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.1677 - accuracy: 0.9565 - val_loss: 0.2658 - val_accuracy: 0.8750\n",
            "Epoch 101/200\n",
            "92/92 [==============================] - 0s 414us/sample - loss: 0.1535 - accuracy: 0.9565 - val_loss: 0.2683 - val_accuracy: 0.8750\n",
            "Epoch 102/200\n",
            "92/92 [==============================] - 0s 397us/sample - loss: 0.1288 - accuracy: 0.9565 - val_loss: 0.2625 - val_accuracy: 0.8750\n",
            "Epoch 103/200\n",
            "92/92 [==============================] - 0s 402us/sample - loss: 0.2087 - accuracy: 0.9022 - val_loss: 0.2641 - val_accuracy: 0.8750\n",
            "Epoch 104/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.1354 - accuracy: 0.9783 - val_loss: 0.2806 - val_accuracy: 0.8750\n",
            "Epoch 105/200\n",
            "92/92 [==============================] - 0s 404us/sample - loss: 0.1535 - accuracy: 0.9239 - val_loss: 0.3043 - val_accuracy: 0.8750\n",
            "Epoch 106/200\n",
            "92/92 [==============================] - 0s 368us/sample - loss: 0.1438 - accuracy: 0.9457 - val_loss: 0.3053 - val_accuracy: 0.8750\n",
            "Epoch 107/200\n",
            "92/92 [==============================] - 0s 421us/sample - loss: 0.1903 - accuracy: 0.9130 - val_loss: 0.2962 - val_accuracy: 0.8750\n",
            "Epoch 108/200\n",
            "92/92 [==============================] - 0s 465us/sample - loss: 0.1015 - accuracy: 0.9783 - val_loss: 0.2813 - val_accuracy: 0.9167\n",
            "Epoch 109/200\n",
            "92/92 [==============================] - 0s 496us/sample - loss: 0.1445 - accuracy: 0.9457 - val_loss: 0.2764 - val_accuracy: 0.9583\n",
            "Epoch 110/200\n",
            "92/92 [==============================] - 0s 375us/sample - loss: 0.1467 - accuracy: 0.9457 - val_loss: 0.2842 - val_accuracy: 0.9583\n",
            "Epoch 111/200\n",
            "92/92 [==============================] - 0s 423us/sample - loss: 0.0928 - accuracy: 1.0000 - val_loss: 0.2989 - val_accuracy: 0.8750\n",
            "Epoch 112/200\n",
            "92/92 [==============================] - 0s 446us/sample - loss: 0.1310 - accuracy: 0.9565 - val_loss: 0.3001 - val_accuracy: 0.8750\n",
            "Epoch 113/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.1300 - accuracy: 0.9457 - val_loss: 0.2990 - val_accuracy: 0.8750\n",
            "Epoch 114/200\n",
            "92/92 [==============================] - 0s 433us/sample - loss: 0.1354 - accuracy: 0.9674 - val_loss: 0.2874 - val_accuracy: 0.8750\n",
            "Epoch 115/200\n",
            "92/92 [==============================] - 0s 454us/sample - loss: 0.1383 - accuracy: 0.9348 - val_loss: 0.2777 - val_accuracy: 0.9167\n",
            "Epoch 116/200\n",
            "92/92 [==============================] - 0s 369us/sample - loss: 0.1163 - accuracy: 0.9674 - val_loss: 0.2718 - val_accuracy: 0.9167\n",
            "Epoch 117/200\n",
            "92/92 [==============================] - 0s 501us/sample - loss: 0.0890 - accuracy: 0.9674 - val_loss: 0.2740 - val_accuracy: 0.9167\n",
            "Epoch 118/200\n",
            "92/92 [==============================] - 0s 470us/sample - loss: 0.1107 - accuracy: 0.9565 - val_loss: 0.2923 - val_accuracy: 0.8750\n",
            "Epoch 119/200\n",
            "92/92 [==============================] - 0s 387us/sample - loss: 0.0868 - accuracy: 0.9783 - val_loss: 0.3207 - val_accuracy: 0.8750\n",
            "Epoch 120/200\n",
            "92/92 [==============================] - 0s 490us/sample - loss: 0.1339 - accuracy: 0.9348 - val_loss: 0.3398 - val_accuracy: 0.8750\n",
            "Epoch 121/200\n",
            "92/92 [==============================] - 0s 420us/sample - loss: 0.1397 - accuracy: 0.9565 - val_loss: 0.3380 - val_accuracy: 0.8750\n",
            "Epoch 122/200\n",
            "92/92 [==============================] - 0s 393us/sample - loss: 0.1226 - accuracy: 0.9348 - val_loss: 0.3132 - val_accuracy: 0.9167\n",
            "Epoch 123/200\n",
            "92/92 [==============================] - 0s 394us/sample - loss: 0.1215 - accuracy: 0.9565 - val_loss: 0.3008 - val_accuracy: 0.9583\n",
            "Epoch 124/200\n",
            "92/92 [==============================] - 0s 386us/sample - loss: 0.1244 - accuracy: 0.9348 - val_loss: 0.3045 - val_accuracy: 0.9167\n",
            "Epoch 125/200\n",
            "92/92 [==============================] - 0s 461us/sample - loss: 0.0982 - accuracy: 0.9783 - val_loss: 0.3185 - val_accuracy: 0.8750\n",
            "Epoch 126/200\n",
            "92/92 [==============================] - 0s 446us/sample - loss: 0.1100 - accuracy: 0.9674 - val_loss: 0.3225 - val_accuracy: 0.8750\n",
            "Epoch 127/200\n",
            "92/92 [==============================] - 0s 449us/sample - loss: 0.0929 - accuracy: 0.9891 - val_loss: 0.3340 - val_accuracy: 0.8750\n",
            "Epoch 128/200\n",
            "92/92 [==============================] - 0s 395us/sample - loss: 0.0921 - accuracy: 0.9783 - val_loss: 0.3352 - val_accuracy: 0.8750\n",
            "Epoch 129/200\n",
            "92/92 [==============================] - 0s 439us/sample - loss: 0.1149 - accuracy: 0.9674 - val_loss: 0.3179 - val_accuracy: 0.8750\n",
            "Epoch 130/200\n",
            "92/92 [==============================] - 0s 393us/sample - loss: 0.1184 - accuracy: 0.9348 - val_loss: 0.3120 - val_accuracy: 0.9167\n",
            "Epoch 131/200\n",
            "92/92 [==============================] - 0s 441us/sample - loss: 0.0958 - accuracy: 0.9565 - val_loss: 0.3073 - val_accuracy: 0.9167\n",
            "Epoch 132/200\n",
            "92/92 [==============================] - 0s 453us/sample - loss: 0.1191 - accuracy: 0.9457 - val_loss: 0.3109 - val_accuracy: 0.9167\n",
            "Epoch 133/200\n",
            "92/92 [==============================] - 0s 443us/sample - loss: 0.0921 - accuracy: 0.9783 - val_loss: 0.3092 - val_accuracy: 0.9167\n",
            "Epoch 134/200\n",
            "92/92 [==============================] - 0s 495us/sample - loss: 0.1063 - accuracy: 0.9674 - val_loss: 0.3117 - val_accuracy: 0.9583\n",
            "Epoch 135/200\n",
            "92/92 [==============================] - 0s 469us/sample - loss: 0.1005 - accuracy: 0.9565 - val_loss: 0.3061 - val_accuracy: 0.9583\n",
            "Epoch 136/200\n",
            "92/92 [==============================] - 0s 377us/sample - loss: 0.0741 - accuracy: 0.9783 - val_loss: 0.2997 - val_accuracy: 0.9167\n",
            "Epoch 137/200\n",
            "92/92 [==============================] - 0s 441us/sample - loss: 0.0810 - accuracy: 0.9891 - val_loss: 0.3139 - val_accuracy: 0.9167\n",
            "Epoch 138/200\n",
            "92/92 [==============================] - 0s 421us/sample - loss: 0.1018 - accuracy: 0.9565 - val_loss: 0.3289 - val_accuracy: 0.8750\n",
            "Epoch 139/200\n",
            "92/92 [==============================] - 0s 634us/sample - loss: 0.1139 - accuracy: 0.9565 - val_loss: 0.3568 - val_accuracy: 0.8750\n",
            "Epoch 140/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.1032 - accuracy: 0.9457 - val_loss: 0.3731 - val_accuracy: 0.8750\n",
            "Epoch 141/200\n",
            "92/92 [==============================] - 0s 341us/sample - loss: 0.0843 - accuracy: 0.9783 - val_loss: 0.3774 - val_accuracy: 0.8750\n",
            "Epoch 142/200\n",
            "92/92 [==============================] - 0s 535us/sample - loss: 0.0878 - accuracy: 0.9565 - val_loss: 0.3892 - val_accuracy: 0.8750\n",
            "Epoch 143/200\n",
            "92/92 [==============================] - 0s 418us/sample - loss: 0.1268 - accuracy: 0.9239 - val_loss: 0.3900 - val_accuracy: 0.9167\n",
            "Epoch 144/200\n",
            "92/92 [==============================] - 0s 474us/sample - loss: 0.1358 - accuracy: 0.9348 - val_loss: 0.3855 - val_accuracy: 0.9167\n",
            "Epoch 145/200\n",
            "92/92 [==============================] - 0s 382us/sample - loss: 0.0801 - accuracy: 0.9783 - val_loss: 0.3862 - val_accuracy: 0.9167\n",
            "Epoch 146/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.0730 - accuracy: 0.9674 - val_loss: 0.3938 - val_accuracy: 0.8750\n",
            "Epoch 147/200\n",
            "92/92 [==============================] - 0s 458us/sample - loss: 0.0739 - accuracy: 0.9891 - val_loss: 0.3664 - val_accuracy: 0.9167\n",
            "Epoch 148/200\n",
            "92/92 [==============================] - 0s 415us/sample - loss: 0.1407 - accuracy: 0.9348 - val_loss: 0.3567 - val_accuracy: 0.9167\n",
            "Epoch 149/200\n",
            "92/92 [==============================] - 0s 430us/sample - loss: 0.0875 - accuracy: 0.9891 - val_loss: 0.3562 - val_accuracy: 0.9167\n",
            "Epoch 150/200\n",
            "92/92 [==============================] - 0s 422us/sample - loss: 0.0985 - accuracy: 0.9565 - val_loss: 0.3662 - val_accuracy: 0.9583\n",
            "Epoch 151/200\n",
            "92/92 [==============================] - 0s 451us/sample - loss: 0.0889 - accuracy: 0.9674 - val_loss: 0.3646 - val_accuracy: 0.9167\n",
            "Epoch 152/200\n",
            "92/92 [==============================] - 0s 421us/sample - loss: 0.0805 - accuracy: 0.9674 - val_loss: 0.3604 - val_accuracy: 0.8750\n",
            "Epoch 153/200\n",
            "92/92 [==============================] - 0s 504us/sample - loss: 0.0888 - accuracy: 0.9674 - val_loss: 0.3511 - val_accuracy: 0.8750\n",
            "Epoch 154/200\n",
            "92/92 [==============================] - 0s 424us/sample - loss: 0.0484 - accuracy: 0.9891 - val_loss: 0.3366 - val_accuracy: 0.8750\n",
            "Epoch 155/200\n",
            "92/92 [==============================] - 0s 428us/sample - loss: 0.0662 - accuracy: 0.9891 - val_loss: 0.3307 - val_accuracy: 0.9167\n",
            "Epoch 156/200\n",
            "92/92 [==============================] - 0s 416us/sample - loss: 0.0765 - accuracy: 0.9891 - val_loss: 0.3311 - val_accuracy: 0.8750\n",
            "Epoch 157/200\n",
            "92/92 [==============================] - 0s 331us/sample - loss: 0.0572 - accuracy: 0.9891 - val_loss: 0.3421 - val_accuracy: 0.8750\n",
            "Epoch 158/200\n",
            "92/92 [==============================] - 0s 413us/sample - loss: 0.0666 - accuracy: 0.9783 - val_loss: 0.3545 - val_accuracy: 0.8750\n",
            "Epoch 159/200\n",
            "92/92 [==============================] - 0s 439us/sample - loss: 0.0577 - accuracy: 0.9891 - val_loss: 0.3568 - val_accuracy: 0.8750\n",
            "Epoch 160/200\n",
            "92/92 [==============================] - 0s 443us/sample - loss: 0.0720 - accuracy: 0.9674 - val_loss: 0.3631 - val_accuracy: 0.8750\n",
            "Epoch 161/200\n",
            "92/92 [==============================] - 0s 422us/sample - loss: 0.0945 - accuracy: 0.9674 - val_loss: 0.3538 - val_accuracy: 0.8750\n",
            "Epoch 162/200\n",
            "92/92 [==============================] - 0s 423us/sample - loss: 0.0615 - accuracy: 0.9891 - val_loss: 0.3462 - val_accuracy: 0.9167\n",
            "Epoch 163/200\n",
            "92/92 [==============================] - 0s 410us/sample - loss: 0.0948 - accuracy: 0.9565 - val_loss: 0.3528 - val_accuracy: 0.9167\n",
            "Epoch 164/200\n",
            "92/92 [==============================] - 0s 370us/sample - loss: 0.1153 - accuracy: 0.9457 - val_loss: 0.3815 - val_accuracy: 0.8750\n",
            "Epoch 165/200\n",
            "92/92 [==============================] - 0s 496us/sample - loss: 0.0707 - accuracy: 0.9674 - val_loss: 0.4240 - val_accuracy: 0.8750\n",
            "Epoch 166/200\n",
            "92/92 [==============================] - 0s 436us/sample - loss: 0.0911 - accuracy: 0.9674 - val_loss: 0.4492 - val_accuracy: 0.9167\n",
            "Epoch 167/200\n",
            "92/92 [==============================] - 0s 537us/sample - loss: 0.0728 - accuracy: 0.9674 - val_loss: 0.4527 - val_accuracy: 0.9167\n",
            "Epoch 168/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.0699 - accuracy: 0.9674 - val_loss: 0.4460 - val_accuracy: 0.8750\n",
            "Epoch 169/200\n",
            "92/92 [==============================] - 0s 408us/sample - loss: 0.0735 - accuracy: 0.9783 - val_loss: 0.4256 - val_accuracy: 0.8750\n",
            "Epoch 170/200\n",
            "92/92 [==============================] - 0s 434us/sample - loss: 0.0776 - accuracy: 0.9674 - val_loss: 0.3975 - val_accuracy: 0.9167\n",
            "Epoch 171/200\n",
            "92/92 [==============================] - 0s 430us/sample - loss: 0.0933 - accuracy: 0.9783 - val_loss: 0.3904 - val_accuracy: 0.9167\n",
            "Epoch 172/200\n",
            "92/92 [==============================] - 0s 343us/sample - loss: 0.0520 - accuracy: 0.9783 - val_loss: 0.4052 - val_accuracy: 0.9167\n",
            "Epoch 173/200\n",
            "92/92 [==============================] - 0s 412us/sample - loss: 0.0737 - accuracy: 0.9783 - val_loss: 0.4224 - val_accuracy: 0.9167\n",
            "Epoch 174/200\n",
            "92/92 [==============================] - 0s 398us/sample - loss: 0.0432 - accuracy: 0.9891 - val_loss: 0.4267 - val_accuracy: 0.9167\n",
            "Epoch 175/200\n",
            "92/92 [==============================] - 0s 452us/sample - loss: 0.0604 - accuracy: 0.9891 - val_loss: 0.4339 - val_accuracy: 0.8750\n",
            "Epoch 176/200\n",
            "92/92 [==============================] - 0s 450us/sample - loss: 0.0907 - accuracy: 0.9783 - val_loss: 0.4326 - val_accuracy: 0.8750\n",
            "Epoch 177/200\n",
            "92/92 [==============================] - 0s 444us/sample - loss: 0.0562 - accuracy: 0.9891 - val_loss: 0.4258 - val_accuracy: 0.8750\n",
            "Epoch 178/200\n",
            "92/92 [==============================] - 0s 470us/sample - loss: 0.0724 - accuracy: 0.9783 - val_loss: 0.4494 - val_accuracy: 0.8750\n",
            "Epoch 179/200\n",
            "92/92 [==============================] - 0s 386us/sample - loss: 0.0678 - accuracy: 0.9783 - val_loss: 0.4937 - val_accuracy: 0.8750\n",
            "Epoch 180/200\n",
            "92/92 [==============================] - 0s 432us/sample - loss: 0.0540 - accuracy: 0.9891 - val_loss: 0.5400 - val_accuracy: 0.8750\n",
            "Epoch 181/200\n",
            "92/92 [==============================] - 0s 419us/sample - loss: 0.0852 - accuracy: 0.9674 - val_loss: 0.5332 - val_accuracy: 0.8750\n",
            "Epoch 182/200\n",
            "92/92 [==============================] - 0s 339us/sample - loss: 0.0932 - accuracy: 0.9457 - val_loss: 0.4586 - val_accuracy: 0.8750\n",
            "Epoch 183/200\n",
            "92/92 [==============================] - 0s 477us/sample - loss: 0.0527 - accuracy: 0.9891 - val_loss: 0.4285 - val_accuracy: 0.8750\n",
            "Epoch 184/200\n",
            "92/92 [==============================] - 0s 541us/sample - loss: 0.0470 - accuracy: 0.9783 - val_loss: 0.4111 - val_accuracy: 0.8750\n",
            "Epoch 185/200\n",
            "92/92 [==============================] - 0s 417us/sample - loss: 0.0486 - accuracy: 0.9891 - val_loss: 0.4061 - val_accuracy: 0.8750\n",
            "Epoch 186/200\n",
            "92/92 [==============================] - 0s 441us/sample - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.4082 - val_accuracy: 0.8750\n",
            "Epoch 187/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.4136 - val_accuracy: 0.8750\n",
            "Epoch 188/200\n",
            "92/92 [==============================] - 0s 447us/sample - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.4132 - val_accuracy: 0.8750\n",
            "Epoch 189/200\n",
            "92/92 [==============================] - 0s 443us/sample - loss: 0.0935 - accuracy: 0.9674 - val_loss: 0.4209 - val_accuracy: 0.8750\n",
            "Epoch 190/200\n",
            "92/92 [==============================] - 0s 387us/sample - loss: 0.0423 - accuracy: 0.9891 - val_loss: 0.4288 - val_accuracy: 0.8750\n",
            "Epoch 191/200\n",
            "92/92 [==============================] - 0s 421us/sample - loss: 0.0838 - accuracy: 0.9674 - val_loss: 0.4212 - val_accuracy: 0.9167\n",
            "Epoch 192/200\n",
            "92/92 [==============================] - 0s 593us/sample - loss: 0.0822 - accuracy: 0.9674 - val_loss: 0.4308 - val_accuracy: 0.8750\n",
            "Epoch 193/200\n",
            "92/92 [==============================] - 0s 447us/sample - loss: 0.0402 - accuracy: 0.9891 - val_loss: 0.4498 - val_accuracy: 0.8750\n",
            "Epoch 194/200\n",
            "92/92 [==============================] - 0s 429us/sample - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.4678 - val_accuracy: 0.8750\n",
            "Epoch 195/200\n",
            "92/92 [==============================] - 0s 402us/sample - loss: 0.0541 - accuracy: 0.9891 - val_loss: 0.4718 - val_accuracy: 0.8750\n",
            "Epoch 196/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.0561 - accuracy: 0.9891 - val_loss: 0.4588 - val_accuracy: 0.8750\n",
            "Epoch 197/200\n",
            "92/92 [==============================] - 0s 380us/sample - loss: 0.0472 - accuracy: 1.0000 - val_loss: 0.4733 - val_accuracy: 0.8750\n",
            "Epoch 198/200\n",
            "92/92 [==============================] - 0s 420us/sample - loss: 0.0751 - accuracy: 0.9565 - val_loss: 0.4912 - val_accuracy: 0.9167\n",
            "Epoch 199/200\n",
            "92/92 [==============================] - 0s 469us/sample - loss: 0.0512 - accuracy: 0.9891 - val_loss: 0.4754 - val_accuracy: 0.8750\n",
            "Epoch 200/200\n",
            "92/92 [==============================] - 0s 457us/sample - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.4762 - val_accuracy: 0.8750\n",
            ">#3: 87.500\n",
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e291a8c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e291a8c8>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e291a8c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e291a8c8>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 1s 6ms/sample - loss: 0.7020 - accuracy: 0.4674 - val_loss: 0.6520 - val_accuracy: 0.8333\n",
            "Epoch 2/200\n",
            "92/92 [==============================] - 0s 460us/sample - loss: 0.6667 - accuracy: 0.6630 - val_loss: 0.6174 - val_accuracy: 0.8750\n",
            "Epoch 3/200\n",
            "92/92 [==============================] - 0s 438us/sample - loss: 0.6595 - accuracy: 0.6630 - val_loss: 0.5853 - val_accuracy: 0.9167\n",
            "Epoch 4/200\n",
            "92/92 [==============================] - 0s 409us/sample - loss: 0.6374 - accuracy: 0.6848 - val_loss: 0.5538 - val_accuracy: 0.8750\n",
            "Epoch 5/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.6314 - accuracy: 0.6957 - val_loss: 0.5241 - val_accuracy: 0.9583\n",
            "Epoch 6/200\n",
            "92/92 [==============================] - 0s 439us/sample - loss: 0.6062 - accuracy: 0.6848 - val_loss: 0.4952 - val_accuracy: 0.9167\n",
            "Epoch 7/200\n",
            "92/92 [==============================] - 0s 451us/sample - loss: 0.6071 - accuracy: 0.6848 - val_loss: 0.4630 - val_accuracy: 0.9167\n",
            "Epoch 8/200\n",
            "92/92 [==============================] - 0s 418us/sample - loss: 0.5807 - accuracy: 0.7500 - val_loss: 0.4291 - val_accuracy: 0.9167\n",
            "Epoch 9/200\n",
            "92/92 [==============================] - 0s 431us/sample - loss: 0.5699 - accuracy: 0.7283 - val_loss: 0.3984 - val_accuracy: 0.9583\n",
            "Epoch 10/200\n",
            "92/92 [==============================] - 0s 434us/sample - loss: 0.5381 - accuracy: 0.7717 - val_loss: 0.3679 - val_accuracy: 0.9583\n",
            "Epoch 11/200\n",
            "92/92 [==============================] - 0s 346us/sample - loss: 0.5301 - accuracy: 0.8043 - val_loss: 0.3415 - val_accuracy: 0.9583\n",
            "Epoch 12/200\n",
            "92/92 [==============================] - 0s 425us/sample - loss: 0.5337 - accuracy: 0.7391 - val_loss: 0.3174 - val_accuracy: 0.9583\n",
            "Epoch 13/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.5194 - accuracy: 0.7283 - val_loss: 0.2925 - val_accuracy: 0.9583\n",
            "Epoch 14/200\n",
            "92/92 [==============================] - 0s 418us/sample - loss: 0.5229 - accuracy: 0.7391 - val_loss: 0.2796 - val_accuracy: 0.9583\n",
            "Epoch 15/200\n",
            "92/92 [==============================] - 0s 545us/sample - loss: 0.5176 - accuracy: 0.7391 - val_loss: 0.2709 - val_accuracy: 0.9583\n",
            "Epoch 16/200\n",
            "92/92 [==============================] - 0s 558us/sample - loss: 0.4974 - accuracy: 0.7500 - val_loss: 0.2710 - val_accuracy: 0.9583\n",
            "Epoch 17/200\n",
            "92/92 [==============================] - 0s 442us/sample - loss: 0.4878 - accuracy: 0.7500 - val_loss: 0.2688 - val_accuracy: 0.9583\n",
            "Epoch 18/200\n",
            "92/92 [==============================] - 0s 425us/sample - loss: 0.4601 - accuracy: 0.8152 - val_loss: 0.2707 - val_accuracy: 0.9583\n",
            "Epoch 19/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.4709 - accuracy: 0.7717 - val_loss: 0.2694 - val_accuracy: 0.9583\n",
            "Epoch 20/200\n",
            "92/92 [==============================] - 0s 375us/sample - loss: 0.4409 - accuracy: 0.7935 - val_loss: 0.2621 - val_accuracy: 0.9583\n",
            "Epoch 21/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.4763 - accuracy: 0.7609 - val_loss: 0.2571 - val_accuracy: 0.9583\n",
            "Epoch 22/200\n",
            "92/92 [==============================] - 0s 449us/sample - loss: 0.4373 - accuracy: 0.7826 - val_loss: 0.2563 - val_accuracy: 0.9583\n",
            "Epoch 23/200\n",
            "92/92 [==============================] - 0s 432us/sample - loss: 0.4598 - accuracy: 0.7935 - val_loss: 0.2579 - val_accuracy: 0.9583\n",
            "Epoch 24/200\n",
            "92/92 [==============================] - 0s 416us/sample - loss: 0.4311 - accuracy: 0.8370 - val_loss: 0.2638 - val_accuracy: 0.9583\n",
            "Epoch 25/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.4157 - accuracy: 0.8152 - val_loss: 0.2610 - val_accuracy: 0.9167\n",
            "Epoch 26/200\n",
            "92/92 [==============================] - 0s 400us/sample - loss: 0.4230 - accuracy: 0.8370 - val_loss: 0.2575 - val_accuracy: 0.9583\n",
            "Epoch 27/200\n",
            "92/92 [==============================] - 0s 454us/sample - loss: 0.3907 - accuracy: 0.8587 - val_loss: 0.2621 - val_accuracy: 0.9167\n",
            "Epoch 28/200\n",
            "92/92 [==============================] - 0s 492us/sample - loss: 0.3856 - accuracy: 0.8478 - val_loss: 0.2644 - val_accuracy: 0.9167\n",
            "Epoch 29/200\n",
            "92/92 [==============================] - 0s 404us/sample - loss: 0.3935 - accuracy: 0.8587 - val_loss: 0.2660 - val_accuracy: 0.9167\n",
            "Epoch 30/200\n",
            "92/92 [==============================] - 0s 438us/sample - loss: 0.3597 - accuracy: 0.8804 - val_loss: 0.2648 - val_accuracy: 0.9167\n",
            "Epoch 31/200\n",
            "92/92 [==============================] - 0s 445us/sample - loss: 0.3556 - accuracy: 0.8478 - val_loss: 0.2696 - val_accuracy: 0.9167\n",
            "Epoch 32/200\n",
            "92/92 [==============================] - 0s 468us/sample - loss: 0.3713 - accuracy: 0.8587 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 33/200\n",
            "92/92 [==============================] - 0s 439us/sample - loss: 0.3568 - accuracy: 0.8370 - val_loss: 0.2695 - val_accuracy: 0.9167\n",
            "Epoch 34/200\n",
            "92/92 [==============================] - 0s 498us/sample - loss: 0.3770 - accuracy: 0.8696 - val_loss: 0.2679 - val_accuracy: 0.9167\n",
            "Epoch 35/200\n",
            "92/92 [==============================] - 0s 427us/sample - loss: 0.3786 - accuracy: 0.8478 - val_loss: 0.2761 - val_accuracy: 0.9167\n",
            "Epoch 36/200\n",
            "92/92 [==============================] - 0s 435us/sample - loss: 0.3261 - accuracy: 0.8696 - val_loss: 0.2911 - val_accuracy: 0.8750\n",
            "Epoch 37/200\n",
            "92/92 [==============================] - 0s 440us/sample - loss: 0.3655 - accuracy: 0.8370 - val_loss: 0.2790 - val_accuracy: 0.9167\n",
            "Epoch 38/200\n",
            "92/92 [==============================] - 0s 427us/sample - loss: 0.3429 - accuracy: 0.8696 - val_loss: 0.2581 - val_accuracy: 0.9167\n",
            "Epoch 39/200\n",
            "92/92 [==============================] - 0s 404us/sample - loss: 0.3410 - accuracy: 0.8696 - val_loss: 0.2505 - val_accuracy: 0.9583\n",
            "Epoch 40/200\n",
            "92/92 [==============================] - 0s 467us/sample - loss: 0.2998 - accuracy: 0.8696 - val_loss: 0.2503 - val_accuracy: 0.9167\n",
            "Epoch 41/200\n",
            "92/92 [==============================] - 0s 473us/sample - loss: 0.3339 - accuracy: 0.8043 - val_loss: 0.2618 - val_accuracy: 0.8750\n",
            "Epoch 42/200\n",
            "92/92 [==============================] - 0s 418us/sample - loss: 0.3569 - accuracy: 0.8696 - val_loss: 0.2673 - val_accuracy: 0.8750\n",
            "Epoch 43/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.3040 - accuracy: 0.8913 - val_loss: 0.2468 - val_accuracy: 0.9167\n",
            "Epoch 44/200\n",
            "92/92 [==============================] - 0s 437us/sample - loss: 0.2915 - accuracy: 0.8804 - val_loss: 0.2324 - val_accuracy: 0.9583\n",
            "Epoch 45/200\n",
            "92/92 [==============================] - 0s 452us/sample - loss: 0.2790 - accuracy: 0.9022 - val_loss: 0.2333 - val_accuracy: 0.9583\n",
            "Epoch 46/200\n",
            "92/92 [==============================] - 0s 503us/sample - loss: 0.2862 - accuracy: 0.8913 - val_loss: 0.2396 - val_accuracy: 0.9583\n",
            "Epoch 47/200\n",
            "92/92 [==============================] - 0s 426us/sample - loss: 0.3095 - accuracy: 0.9130 - val_loss: 0.2499 - val_accuracy: 0.9167\n",
            "Epoch 48/200\n",
            "92/92 [==============================] - 0s 493us/sample - loss: 0.3113 - accuracy: 0.8696 - val_loss: 0.2666 - val_accuracy: 0.9167\n",
            "Epoch 49/200\n",
            "92/92 [==============================] - 0s 432us/sample - loss: 0.3221 - accuracy: 0.8587 - val_loss: 0.2660 - val_accuracy: 0.9167\n",
            "Epoch 50/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.2540 - accuracy: 0.9348 - val_loss: 0.2624 - val_accuracy: 0.9167\n",
            "Epoch 51/200\n",
            "92/92 [==============================] - 0s 445us/sample - loss: 0.3085 - accuracy: 0.8696 - val_loss: 0.2618 - val_accuracy: 0.9167\n",
            "Epoch 52/200\n",
            "92/92 [==============================] - 0s 482us/sample - loss: 0.2959 - accuracy: 0.9022 - val_loss: 0.2734 - val_accuracy: 0.9167\n",
            "Epoch 53/200\n",
            "92/92 [==============================] - 0s 435us/sample - loss: 0.3252 - accuracy: 0.8696 - val_loss: 0.2775 - val_accuracy: 0.9167\n",
            "Epoch 54/200\n",
            "92/92 [==============================] - 0s 363us/sample - loss: 0.2380 - accuracy: 0.8913 - val_loss: 0.2903 - val_accuracy: 0.8750\n",
            "Epoch 55/200\n",
            "92/92 [==============================] - 0s 456us/sample - loss: 0.2529 - accuracy: 0.9022 - val_loss: 0.2995 - val_accuracy: 0.8750\n",
            "Epoch 56/200\n",
            "92/92 [==============================] - 0s 338us/sample - loss: 0.2714 - accuracy: 0.9130 - val_loss: 0.2891 - val_accuracy: 0.8750\n",
            "Epoch 57/200\n",
            "92/92 [==============================] - 0s 495us/sample - loss: 0.2805 - accuracy: 0.8913 - val_loss: 0.2785 - val_accuracy: 0.9167\n",
            "Epoch 58/200\n",
            "92/92 [==============================] - 0s 512us/sample - loss: 0.2654 - accuracy: 0.9130 - val_loss: 0.2743 - val_accuracy: 0.9167\n",
            "Epoch 59/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.2457 - accuracy: 0.8913 - val_loss: 0.2641 - val_accuracy: 0.9167\n",
            "Epoch 60/200\n",
            "92/92 [==============================] - 0s 494us/sample - loss: 0.2417 - accuracy: 0.9022 - val_loss: 0.2654 - val_accuracy: 0.9167\n",
            "Epoch 61/200\n",
            "92/92 [==============================] - 0s 369us/sample - loss: 0.2706 - accuracy: 0.9239 - val_loss: 0.2719 - val_accuracy: 0.8750\n",
            "Epoch 62/200\n",
            "92/92 [==============================] - 0s 353us/sample - loss: 0.2130 - accuracy: 0.9565 - val_loss: 0.2747 - val_accuracy: 0.8750\n",
            "Epoch 63/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.2283 - accuracy: 0.9348 - val_loss: 0.2735 - val_accuracy: 0.8750\n",
            "Epoch 64/200\n",
            "92/92 [==============================] - 0s 424us/sample - loss: 0.1885 - accuracy: 0.9565 - val_loss: 0.2705 - val_accuracy: 0.9167\n",
            "Epoch 65/200\n",
            "92/92 [==============================] - 0s 354us/sample - loss: 0.2180 - accuracy: 0.8804 - val_loss: 0.2579 - val_accuracy: 0.9167\n",
            "Epoch 66/200\n",
            "92/92 [==============================] - 0s 425us/sample - loss: 0.2558 - accuracy: 0.9022 - val_loss: 0.2596 - val_accuracy: 0.9167\n",
            "Epoch 67/200\n",
            "92/92 [==============================] - 0s 371us/sample - loss: 0.1821 - accuracy: 0.9565 - val_loss: 0.2639 - val_accuracy: 0.9167\n",
            "Epoch 68/200\n",
            "92/92 [==============================] - 0s 416us/sample - loss: 0.2182 - accuracy: 0.9565 - val_loss: 0.2812 - val_accuracy: 0.9167\n",
            "Epoch 69/200\n",
            "92/92 [==============================] - 0s 423us/sample - loss: 0.2275 - accuracy: 0.8804 - val_loss: 0.2948 - val_accuracy: 0.9167\n",
            "Epoch 70/200\n",
            "92/92 [==============================] - 0s 437us/sample - loss: 0.2769 - accuracy: 0.9022 - val_loss: 0.2951 - val_accuracy: 0.9167\n",
            "Epoch 71/200\n",
            "92/92 [==============================] - 0s 375us/sample - loss: 0.2042 - accuracy: 0.9239 - val_loss: 0.2986 - val_accuracy: 0.9167\n",
            "Epoch 72/200\n",
            "92/92 [==============================] - 0s 502us/sample - loss: 0.2382 - accuracy: 0.9239 - val_loss: 0.3054 - val_accuracy: 0.8750\n",
            "Epoch 73/200\n",
            "92/92 [==============================] - 0s 428us/sample - loss: 0.2138 - accuracy: 0.9239 - val_loss: 0.3201 - val_accuracy: 0.8750\n",
            "Epoch 74/200\n",
            "92/92 [==============================] - 0s 461us/sample - loss: 0.1994 - accuracy: 0.9239 - val_loss: 0.3274 - val_accuracy: 0.8750\n",
            "Epoch 75/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.2026 - accuracy: 0.9239 - val_loss: 0.3051 - val_accuracy: 0.8750\n",
            "Epoch 76/200\n",
            "92/92 [==============================] - 0s 332us/sample - loss: 0.2305 - accuracy: 0.9130 - val_loss: 0.2915 - val_accuracy: 0.8750\n",
            "Epoch 77/200\n",
            "92/92 [==============================] - 0s 437us/sample - loss: 0.1540 - accuracy: 0.9457 - val_loss: 0.2879 - val_accuracy: 0.8750\n",
            "Epoch 78/200\n",
            "92/92 [==============================] - 0s 382us/sample - loss: 0.1866 - accuracy: 0.9348 - val_loss: 0.2969 - val_accuracy: 0.8750\n",
            "Epoch 79/200\n",
            "92/92 [==============================] - 0s 389us/sample - loss: 0.2144 - accuracy: 0.8913 - val_loss: 0.3052 - val_accuracy: 0.8750\n",
            "Epoch 80/200\n",
            "92/92 [==============================] - 0s 364us/sample - loss: 0.1980 - accuracy: 0.9130 - val_loss: 0.3313 - val_accuracy: 0.8750\n",
            "Epoch 81/200\n",
            "92/92 [==============================] - 0s 397us/sample - loss: 0.1901 - accuracy: 0.9457 - val_loss: 0.3253 - val_accuracy: 0.8750\n",
            "Epoch 82/200\n",
            "92/92 [==============================] - 0s 414us/sample - loss: 0.1937 - accuracy: 0.9130 - val_loss: 0.3274 - val_accuracy: 0.8750\n",
            "Epoch 83/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.1820 - accuracy: 0.9565 - val_loss: 0.3403 - val_accuracy: 0.8750\n",
            "Epoch 84/200\n",
            "92/92 [==============================] - 0s 454us/sample - loss: 0.1906 - accuracy: 0.9457 - val_loss: 0.3437 - val_accuracy: 0.8750\n",
            "Epoch 85/200\n",
            "92/92 [==============================] - 0s 432us/sample - loss: 0.1616 - accuracy: 0.9457 - val_loss: 0.3391 - val_accuracy: 0.8750\n",
            "Epoch 86/200\n",
            "92/92 [==============================] - 0s 440us/sample - loss: 0.1655 - accuracy: 0.9348 - val_loss: 0.3316 - val_accuracy: 0.8750\n",
            "Epoch 87/200\n",
            "92/92 [==============================] - 0s 416us/sample - loss: 0.1336 - accuracy: 0.9565 - val_loss: 0.3415 - val_accuracy: 0.8750\n",
            "Epoch 88/200\n",
            "92/92 [==============================] - 0s 429us/sample - loss: 0.1849 - accuracy: 0.9348 - val_loss: 0.3613 - val_accuracy: 0.8333\n",
            "Epoch 89/200\n",
            "92/92 [==============================] - 0s 395us/sample - loss: 0.1736 - accuracy: 0.9348 - val_loss: 0.3549 - val_accuracy: 0.9167\n",
            "Epoch 90/200\n",
            "92/92 [==============================] - 0s 452us/sample - loss: 0.1615 - accuracy: 0.9457 - val_loss: 0.3291 - val_accuracy: 0.9167\n",
            "Epoch 91/200\n",
            "92/92 [==============================] - 0s 444us/sample - loss: 0.1596 - accuracy: 0.9239 - val_loss: 0.3252 - val_accuracy: 0.9167\n",
            "Epoch 92/200\n",
            "92/92 [==============================] - 0s 427us/sample - loss: 0.1759 - accuracy: 0.9457 - val_loss: 0.3346 - val_accuracy: 0.8750\n",
            "Epoch 93/200\n",
            "92/92 [==============================] - 0s 417us/sample - loss: 0.1321 - accuracy: 0.9674 - val_loss: 0.3532 - val_accuracy: 0.8750\n",
            "Epoch 94/200\n",
            "92/92 [==============================] - 0s 409us/sample - loss: 0.1474 - accuracy: 0.9130 - val_loss: 0.3476 - val_accuracy: 0.8750\n",
            "Epoch 95/200\n",
            "92/92 [==============================] - 0s 442us/sample - loss: 0.1287 - accuracy: 0.9674 - val_loss: 0.3400 - val_accuracy: 0.8750\n",
            "Epoch 96/200\n",
            "92/92 [==============================] - 0s 394us/sample - loss: 0.1545 - accuracy: 0.9130 - val_loss: 0.3412 - val_accuracy: 0.9167\n",
            "Epoch 97/200\n",
            "92/92 [==============================] - 0s 431us/sample - loss: 0.1383 - accuracy: 0.9457 - val_loss: 0.3539 - val_accuracy: 0.9167\n",
            "Epoch 98/200\n",
            "92/92 [==============================] - 0s 415us/sample - loss: 0.1320 - accuracy: 0.9565 - val_loss: 0.3758 - val_accuracy: 0.9167\n",
            "Epoch 99/200\n",
            "92/92 [==============================] - 0s 579us/sample - loss: 0.1197 - accuracy: 0.9565 - val_loss: 0.3835 - val_accuracy: 0.9167\n",
            "Epoch 100/200\n",
            "92/92 [==============================] - 0s 460us/sample - loss: 0.1440 - accuracy: 0.9457 - val_loss: 0.3739 - val_accuracy: 0.9167\n",
            "Epoch 101/200\n",
            "92/92 [==============================] - 0s 351us/sample - loss: 0.1256 - accuracy: 0.9565 - val_loss: 0.3604 - val_accuracy: 0.9167\n",
            "Epoch 102/200\n",
            "92/92 [==============================] - 0s 494us/sample - loss: 0.1177 - accuracy: 0.9674 - val_loss: 0.3718 - val_accuracy: 0.9167\n",
            "Epoch 103/200\n",
            "92/92 [==============================] - 0s 482us/sample - loss: 0.1232 - accuracy: 0.9565 - val_loss: 0.3944 - val_accuracy: 0.8750\n",
            "Epoch 104/200\n",
            "92/92 [==============================] - 0s 393us/sample - loss: 0.1193 - accuracy: 0.9565 - val_loss: 0.4101 - val_accuracy: 0.8750\n",
            "Epoch 105/200\n",
            "92/92 [==============================] - 0s 467us/sample - loss: 0.1301 - accuracy: 0.9457 - val_loss: 0.4021 - val_accuracy: 0.8750\n",
            "Epoch 106/200\n",
            "92/92 [==============================] - 0s 379us/sample - loss: 0.1707 - accuracy: 0.9239 - val_loss: 0.4074 - val_accuracy: 0.8750\n",
            "Epoch 107/200\n",
            "92/92 [==============================] - 0s 448us/sample - loss: 0.1296 - accuracy: 0.9457 - val_loss: 0.3945 - val_accuracy: 0.8750\n",
            "Epoch 108/200\n",
            "92/92 [==============================] - 0s 411us/sample - loss: 0.1389 - accuracy: 0.9457 - val_loss: 0.3844 - val_accuracy: 0.9167\n",
            "Epoch 109/200\n",
            "92/92 [==============================] - 0s 407us/sample - loss: 0.1502 - accuracy: 0.9457 - val_loss: 0.3812 - val_accuracy: 0.9167\n",
            "Epoch 110/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.1220 - accuracy: 0.9783 - val_loss: 0.4080 - val_accuracy: 0.8750\n",
            "Epoch 111/200\n",
            "92/92 [==============================] - 0s 478us/sample - loss: 0.1283 - accuracy: 0.9348 - val_loss: 0.4230 - val_accuracy: 0.8750\n",
            "Epoch 112/200\n",
            "92/92 [==============================] - 0s 405us/sample - loss: 0.1270 - accuracy: 0.9457 - val_loss: 0.4060 - val_accuracy: 0.9167\n",
            "Epoch 113/200\n",
            "92/92 [==============================] - 0s 442us/sample - loss: 0.1480 - accuracy: 0.9457 - val_loss: 0.3896 - val_accuracy: 0.9167\n",
            "Epoch 114/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.1382 - accuracy: 0.9348 - val_loss: 0.4178 - val_accuracy: 0.8333\n",
            "Epoch 115/200\n",
            "92/92 [==============================] - 0s 410us/sample - loss: 0.0955 - accuracy: 0.9674 - val_loss: 0.4535 - val_accuracy: 0.8333\n",
            "Epoch 116/200\n",
            "92/92 [==============================] - 0s 428us/sample - loss: 0.1202 - accuracy: 0.9565 - val_loss: 0.4582 - val_accuracy: 0.8333\n",
            "Epoch 117/200\n",
            "92/92 [==============================] - 0s 360us/sample - loss: 0.1541 - accuracy: 0.9348 - val_loss: 0.3985 - val_accuracy: 0.8750\n",
            "Epoch 118/200\n",
            "92/92 [==============================] - 0s 360us/sample - loss: 0.1247 - accuracy: 0.9457 - val_loss: 0.3593 - val_accuracy: 0.9167\n",
            "Epoch 119/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.1296 - accuracy: 0.9565 - val_loss: 0.3452 - val_accuracy: 0.9167\n",
            "Epoch 120/200\n",
            "92/92 [==============================] - 0s 464us/sample - loss: 0.1381 - accuracy: 0.9457 - val_loss: 0.3451 - val_accuracy: 0.9167\n",
            "Epoch 121/200\n",
            "92/92 [==============================] - 0s 507us/sample - loss: 0.1135 - accuracy: 0.9565 - val_loss: 0.3728 - val_accuracy: 0.8750\n",
            "Epoch 122/200\n",
            "92/92 [==============================] - 0s 468us/sample - loss: 0.1030 - accuracy: 0.9674 - val_loss: 0.4043 - val_accuracy: 0.8750\n",
            "Epoch 123/200\n",
            "92/92 [==============================] - 0s 433us/sample - loss: 0.1014 - accuracy: 0.9565 - val_loss: 0.4358 - val_accuracy: 0.8750\n",
            "Epoch 124/200\n",
            "92/92 [==============================] - 0s 499us/sample - loss: 0.1634 - accuracy: 0.9457 - val_loss: 0.4416 - val_accuracy: 0.8750\n",
            "Epoch 125/200\n",
            "92/92 [==============================] - 0s 502us/sample - loss: 0.0743 - accuracy: 0.9783 - val_loss: 0.4463 - val_accuracy: 0.8750\n",
            "Epoch 126/200\n",
            "92/92 [==============================] - 0s 349us/sample - loss: 0.1271 - accuracy: 0.9348 - val_loss: 0.4628 - val_accuracy: 0.8750\n",
            "Epoch 127/200\n",
            "92/92 [==============================] - 0s 529us/sample - loss: 0.1240 - accuracy: 0.9674 - val_loss: 0.4674 - val_accuracy: 0.8750\n",
            "Epoch 128/200\n",
            "92/92 [==============================] - 0s 510us/sample - loss: 0.0820 - accuracy: 0.9783 - val_loss: 0.4569 - val_accuracy: 0.8750\n",
            "Epoch 129/200\n",
            "92/92 [==============================] - 0s 510us/sample - loss: 0.1307 - accuracy: 0.9674 - val_loss: 0.4420 - val_accuracy: 0.8750\n",
            "Epoch 130/200\n",
            "92/92 [==============================] - 0s 446us/sample - loss: 0.0955 - accuracy: 0.9783 - val_loss: 0.4378 - val_accuracy: 0.8750\n",
            "Epoch 131/200\n",
            "92/92 [==============================] - 0s 516us/sample - loss: 0.0971 - accuracy: 0.9674 - val_loss: 0.4560 - val_accuracy: 0.8333\n",
            "Epoch 132/200\n",
            "92/92 [==============================] - 0s 420us/sample - loss: 0.1169 - accuracy: 0.9457 - val_loss: 0.4717 - val_accuracy: 0.8333\n",
            "Epoch 133/200\n",
            "92/92 [==============================] - 0s 411us/sample - loss: 0.0908 - accuracy: 0.9674 - val_loss: 0.4587 - val_accuracy: 0.8750\n",
            "Epoch 134/200\n",
            "92/92 [==============================] - 0s 413us/sample - loss: 0.0879 - accuracy: 0.9891 - val_loss: 0.4498 - val_accuracy: 0.8750\n",
            "Epoch 135/200\n",
            "92/92 [==============================] - 0s 432us/sample - loss: 0.0975 - accuracy: 0.9674 - val_loss: 0.4465 - val_accuracy: 0.8750\n",
            "Epoch 136/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.1004 - accuracy: 0.9565 - val_loss: 0.4474 - val_accuracy: 0.8750\n",
            "Epoch 137/200\n",
            "92/92 [==============================] - 0s 417us/sample - loss: 0.0880 - accuracy: 0.9783 - val_loss: 0.4572 - val_accuracy: 0.8750\n",
            "Epoch 138/200\n",
            "92/92 [==============================] - 0s 411us/sample - loss: 0.0825 - accuracy: 0.9565 - val_loss: 0.4432 - val_accuracy: 0.8750\n",
            "Epoch 139/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.1014 - accuracy: 0.9674 - val_loss: 0.4473 - val_accuracy: 0.8750\n",
            "Epoch 140/200\n",
            "92/92 [==============================] - 0s 449us/sample - loss: 0.0695 - accuracy: 0.9891 - val_loss: 0.4255 - val_accuracy: 0.8750\n",
            "Epoch 141/200\n",
            "92/92 [==============================] - 0s 393us/sample - loss: 0.0652 - accuracy: 0.9783 - val_loss: 0.4176 - val_accuracy: 0.8750\n",
            "Epoch 142/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.0777 - accuracy: 0.9674 - val_loss: 0.4100 - val_accuracy: 0.8750\n",
            "Epoch 143/200\n",
            "92/92 [==============================] - 0s 362us/sample - loss: 0.0753 - accuracy: 0.9783 - val_loss: 0.4120 - val_accuracy: 0.8750\n",
            "Epoch 144/200\n",
            "92/92 [==============================] - 0s 405us/sample - loss: 0.0842 - accuracy: 0.9565 - val_loss: 0.4389 - val_accuracy: 0.8750\n",
            "Epoch 145/200\n",
            "92/92 [==============================] - 0s 379us/sample - loss: 0.0650 - accuracy: 0.9891 - val_loss: 0.4620 - val_accuracy: 0.8750\n",
            "Epoch 146/200\n",
            "92/92 [==============================] - 0s 449us/sample - loss: 0.0858 - accuracy: 0.9674 - val_loss: 0.4593 - val_accuracy: 0.8750\n",
            "Epoch 147/200\n",
            "92/92 [==============================] - 0s 468us/sample - loss: 0.0939 - accuracy: 0.9674 - val_loss: 0.4504 - val_accuracy: 0.8750\n",
            "Epoch 148/200\n",
            "92/92 [==============================] - 0s 540us/sample - loss: 0.0527 - accuracy: 1.0000 - val_loss: 0.4729 - val_accuracy: 0.8750\n",
            "Epoch 149/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.0897 - accuracy: 0.9674 - val_loss: 0.4914 - val_accuracy: 0.8750\n",
            "Epoch 150/200\n",
            "92/92 [==============================] - 0s 371us/sample - loss: 0.0719 - accuracy: 0.9891 - val_loss: 0.5405 - val_accuracy: 0.8333\n",
            "Epoch 151/200\n",
            "92/92 [==============================] - 0s 426us/sample - loss: 0.0611 - accuracy: 0.9783 - val_loss: 0.6005 - val_accuracy: 0.8333\n",
            "Epoch 152/200\n",
            "92/92 [==============================] - 0s 504us/sample - loss: 0.0536 - accuracy: 0.9891 - val_loss: 0.6062 - val_accuracy: 0.8333\n",
            "Epoch 153/200\n",
            "92/92 [==============================] - 0s 445us/sample - loss: 0.0616 - accuracy: 0.9891 - val_loss: 0.5877 - val_accuracy: 0.8333\n",
            "Epoch 154/200\n",
            "92/92 [==============================] - 0s 405us/sample - loss: 0.0607 - accuracy: 0.9783 - val_loss: 0.5486 - val_accuracy: 0.8750\n",
            "Epoch 155/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.0603 - accuracy: 0.9891 - val_loss: 0.5219 - val_accuracy: 0.8750\n",
            "Epoch 156/200\n",
            "92/92 [==============================] - 0s 430us/sample - loss: 0.0880 - accuracy: 0.9674 - val_loss: 0.5075 - val_accuracy: 0.8750\n",
            "Epoch 157/200\n",
            "92/92 [==============================] - 0s 454us/sample - loss: 0.0575 - accuracy: 0.9891 - val_loss: 0.5321 - val_accuracy: 0.8333\n",
            "Epoch 158/200\n",
            "92/92 [==============================] - 0s 395us/sample - loss: 0.0718 - accuracy: 0.9674 - val_loss: 0.5625 - val_accuracy: 0.8333\n",
            "Epoch 159/200\n",
            "92/92 [==============================] - 0s 416us/sample - loss: 0.0963 - accuracy: 0.9783 - val_loss: 0.5712 - val_accuracy: 0.8333\n",
            "Epoch 160/200\n",
            "92/92 [==============================] - 0s 413us/sample - loss: 0.0651 - accuracy: 0.9891 - val_loss: 0.5619 - val_accuracy: 0.8750\n",
            "Epoch 161/200\n",
            "92/92 [==============================] - 0s 427us/sample - loss: 0.0711 - accuracy: 0.9674 - val_loss: 0.5476 - val_accuracy: 0.8750\n",
            "Epoch 162/200\n",
            "92/92 [==============================] - 0s 471us/sample - loss: 0.0884 - accuracy: 0.9783 - val_loss: 0.5566 - val_accuracy: 0.8333\n",
            "Epoch 163/200\n",
            "92/92 [==============================] - 0s 523us/sample - loss: 0.0678 - accuracy: 0.9891 - val_loss: 0.5847 - val_accuracy: 0.8333\n",
            "Epoch 164/200\n",
            "92/92 [==============================] - 0s 599us/sample - loss: 0.0753 - accuracy: 0.9783 - val_loss: 0.5727 - val_accuracy: 0.8333\n",
            "Epoch 165/200\n",
            "92/92 [==============================] - 0s 464us/sample - loss: 0.0597 - accuracy: 0.9891 - val_loss: 0.5715 - val_accuracy: 0.8333\n",
            "Epoch 166/200\n",
            "92/92 [==============================] - 0s 422us/sample - loss: 0.0600 - accuracy: 0.9891 - val_loss: 0.5879 - val_accuracy: 0.8333\n",
            "Epoch 167/200\n",
            "92/92 [==============================] - 0s 454us/sample - loss: 0.0535 - accuracy: 0.9891 - val_loss: 0.5906 - val_accuracy: 0.8750\n",
            "Epoch 168/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.0571 - accuracy: 0.9783 - val_loss: 0.5878 - val_accuracy: 0.8750\n",
            "Epoch 169/200\n",
            "92/92 [==============================] - 0s 478us/sample - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.5865 - val_accuracy: 0.8750\n",
            "Epoch 170/200\n",
            "92/92 [==============================] - 0s 377us/sample - loss: 0.0700 - accuracy: 0.9674 - val_loss: 0.6005 - val_accuracy: 0.8750\n",
            "Epoch 171/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 0.8750\n",
            "Epoch 172/200\n",
            "92/92 [==============================] - 0s 325us/sample - loss: 0.0377 - accuracy: 0.9891 - val_loss: 0.5877 - val_accuracy: 0.8750\n",
            "Epoch 173/200\n",
            "92/92 [==============================] - 0s 459us/sample - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.5923 - val_accuracy: 0.8750\n",
            "Epoch 174/200\n",
            "92/92 [==============================] - 0s 339us/sample - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 0.8333\n",
            "Epoch 175/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.0639 - accuracy: 0.9783 - val_loss: 0.5805 - val_accuracy: 0.8750\n",
            "Epoch 176/200\n",
            "92/92 [==============================] - 0s 412us/sample - loss: 0.0520 - accuracy: 0.9783 - val_loss: 0.5551 - val_accuracy: 0.8750\n",
            "Epoch 177/200\n",
            "92/92 [==============================] - 0s 386us/sample - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.5455 - val_accuracy: 0.8750\n",
            "Epoch 178/200\n",
            "92/92 [==============================] - 0s 348us/sample - loss: 0.0622 - accuracy: 0.9783 - val_loss: 0.5694 - val_accuracy: 0.8750\n",
            "Epoch 179/200\n",
            "92/92 [==============================] - 0s 330us/sample - loss: 0.0333 - accuracy: 0.9891 - val_loss: 0.6279 - val_accuracy: 0.8333\n",
            "Epoch 180/200\n",
            "92/92 [==============================] - 0s 332us/sample - loss: 0.0959 - accuracy: 0.9674 - val_loss: 0.6582 - val_accuracy: 0.8333\n",
            "Epoch 181/200\n",
            "92/92 [==============================] - 0s 326us/sample - loss: 0.0429 - accuracy: 0.9891 - val_loss: 0.6616 - val_accuracy: 0.8333\n",
            "Epoch 182/200\n",
            "92/92 [==============================] - 0s 360us/sample - loss: 0.0721 - accuracy: 0.9565 - val_loss: 0.6226 - val_accuracy: 0.8333\n",
            "Epoch 183/200\n",
            "92/92 [==============================] - 0s 498us/sample - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.5484 - val_accuracy: 0.8750\n",
            "Epoch 184/200\n",
            "92/92 [==============================] - 0s 347us/sample - loss: 0.0445 - accuracy: 0.9891 - val_loss: 0.5345 - val_accuracy: 0.8750\n",
            "Epoch 185/200\n",
            "92/92 [==============================] - 0s 511us/sample - loss: 0.0385 - accuracy: 1.0000 - val_loss: 0.5376 - val_accuracy: 0.8750\n",
            "Epoch 186/200\n",
            "92/92 [==============================] - 0s 444us/sample - loss: 0.0380 - accuracy: 0.9783 - val_loss: 0.5581 - val_accuracy: 0.8750\n",
            "Epoch 187/200\n",
            "92/92 [==============================] - 0s 425us/sample - loss: 0.0711 - accuracy: 0.9783 - val_loss: 0.5744 - val_accuracy: 0.8333\n",
            "Epoch 188/200\n",
            "92/92 [==============================] - 0s 364us/sample - loss: 0.0715 - accuracy: 0.9674 - val_loss: 0.5410 - val_accuracy: 0.8750\n",
            "Epoch 189/200\n",
            "92/92 [==============================] - 0s 424us/sample - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.5126 - val_accuracy: 0.8750\n",
            "Epoch 190/200\n",
            "92/92 [==============================] - 0s 394us/sample - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.5149 - val_accuracy: 0.8750\n",
            "Epoch 191/200\n",
            "92/92 [==============================] - 0s 427us/sample - loss: 0.0758 - accuracy: 0.9783 - val_loss: 0.4967 - val_accuracy: 0.8750\n",
            "Epoch 192/200\n",
            "92/92 [==============================] - 0s 416us/sample - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.8750\n",
            "Epoch 193/200\n",
            "92/92 [==============================] - 0s 464us/sample - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.4645 - val_accuracy: 0.8750\n",
            "Epoch 194/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.0450 - accuracy: 0.9891 - val_loss: 0.4759 - val_accuracy: 0.8750\n",
            "Epoch 195/200\n",
            "92/92 [==============================] - 0s 478us/sample - loss: 0.0644 - accuracy: 0.9674 - val_loss: 0.5009 - val_accuracy: 0.8750\n",
            "Epoch 196/200\n",
            "92/92 [==============================] - 0s 472us/sample - loss: 0.0545 - accuracy: 0.9783 - val_loss: 0.5482 - val_accuracy: 0.8750\n",
            "Epoch 197/200\n",
            "92/92 [==============================] - 0s 534us/sample - loss: 0.0700 - accuracy: 0.9783 - val_loss: 0.5801 - val_accuracy: 0.8750\n",
            "Epoch 198/200\n",
            "92/92 [==============================] - 0s 382us/sample - loss: 0.0477 - accuracy: 0.9891 - val_loss: 0.5976 - val_accuracy: 0.8333\n",
            "Epoch 199/200\n",
            "92/92 [==============================] - 0s 498us/sample - loss: 0.0640 - accuracy: 0.9783 - val_loss: 0.5962 - val_accuracy: 0.8333\n",
            "Epoch 200/200\n",
            "92/92 [==============================] - 0s 424us/sample - loss: 0.0739 - accuracy: 0.9783 - val_loss: 0.5690 - val_accuracy: 0.8333\n",
            ">#4: 83.333\n",
            "Train on 92 samples, validate on 24 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dc9c5e18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dc9c5e18>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dc9c5e18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dc9c5e18>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 1s 6ms/sample - loss: 0.6871 - accuracy: 0.5217 - val_loss: 0.6057 - val_accuracy: 0.5833\n",
            "Epoch 2/200\n",
            "92/92 [==============================] - 0s 458us/sample - loss: 0.6625 - accuracy: 0.6087 - val_loss: 0.5667 - val_accuracy: 0.7083\n",
            "Epoch 3/200\n",
            "92/92 [==============================] - 0s 413us/sample - loss: 0.6531 - accuracy: 0.5870 - val_loss: 0.5333 - val_accuracy: 0.7500\n",
            "Epoch 4/200\n",
            "92/92 [==============================] - 0s 408us/sample - loss: 0.6407 - accuracy: 0.6630 - val_loss: 0.5069 - val_accuracy: 0.9167\n",
            "Epoch 5/200\n",
            "92/92 [==============================] - 0s 426us/sample - loss: 0.6185 - accuracy: 0.7283 - val_loss: 0.4785 - val_accuracy: 0.9583\n",
            "Epoch 6/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.6177 - accuracy: 0.7283 - val_loss: 0.4497 - val_accuracy: 0.9583\n",
            "Epoch 7/200\n",
            "92/92 [==============================] - 0s 380us/sample - loss: 0.5892 - accuracy: 0.7283 - val_loss: 0.4219 - val_accuracy: 0.9583\n",
            "Epoch 8/200\n",
            "92/92 [==============================] - 0s 380us/sample - loss: 0.5806 - accuracy: 0.7065 - val_loss: 0.3951 - val_accuracy: 0.9583\n",
            "Epoch 9/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.5661 - accuracy: 0.6957 - val_loss: 0.3700 - val_accuracy: 0.9583\n",
            "Epoch 10/200\n",
            "92/92 [==============================] - 0s 375us/sample - loss: 0.5755 - accuracy: 0.7391 - val_loss: 0.3484 - val_accuracy: 0.9583\n",
            "Epoch 11/200\n",
            "92/92 [==============================] - 0s 383us/sample - loss: 0.5409 - accuracy: 0.7174 - val_loss: 0.3332 - val_accuracy: 0.9583\n",
            "Epoch 12/200\n",
            "92/92 [==============================] - 0s 398us/sample - loss: 0.5314 - accuracy: 0.7609 - val_loss: 0.3189 - val_accuracy: 0.9583\n",
            "Epoch 13/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.5352 - accuracy: 0.7065 - val_loss: 0.3071 - val_accuracy: 0.9583\n",
            "Epoch 14/200\n",
            "92/92 [==============================] - 0s 386us/sample - loss: 0.4848 - accuracy: 0.7609 - val_loss: 0.2928 - val_accuracy: 0.9583\n",
            "Epoch 15/200\n",
            "92/92 [==============================] - 0s 440us/sample - loss: 0.5033 - accuracy: 0.7717 - val_loss: 0.2780 - val_accuracy: 0.9583\n",
            "Epoch 16/200\n",
            "92/92 [==============================] - 0s 410us/sample - loss: 0.4799 - accuracy: 0.7717 - val_loss: 0.2710 - val_accuracy: 0.9583\n",
            "Epoch 17/200\n",
            "92/92 [==============================] - 0s 408us/sample - loss: 0.5257 - accuracy: 0.7391 - val_loss: 0.2663 - val_accuracy: 0.9583\n",
            "Epoch 18/200\n",
            "92/92 [==============================] - 0s 475us/sample - loss: 0.4635 - accuracy: 0.7717 - val_loss: 0.2647 - val_accuracy: 0.9583\n",
            "Epoch 19/200\n",
            "92/92 [==============================] - 0s 469us/sample - loss: 0.4706 - accuracy: 0.7717 - val_loss: 0.2658 - val_accuracy: 0.9583\n",
            "Epoch 20/200\n",
            "92/92 [==============================] - 0s 373us/sample - loss: 0.4643 - accuracy: 0.8043 - val_loss: 0.2706 - val_accuracy: 0.9167\n",
            "Epoch 21/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.4452 - accuracy: 0.8043 - val_loss: 0.2743 - val_accuracy: 0.9167\n",
            "Epoch 22/200\n",
            "92/92 [==============================] - 0s 443us/sample - loss: 0.4654 - accuracy: 0.7717 - val_loss: 0.2698 - val_accuracy: 0.9583\n",
            "Epoch 23/200\n",
            "92/92 [==============================] - 0s 476us/sample - loss: 0.4528 - accuracy: 0.7826 - val_loss: 0.2707 - val_accuracy: 0.9583\n",
            "Epoch 24/200\n",
            "92/92 [==============================] - 0s 459us/sample - loss: 0.4274 - accuracy: 0.8043 - val_loss: 0.2728 - val_accuracy: 0.9583\n",
            "Epoch 25/200\n",
            "92/92 [==============================] - 0s 372us/sample - loss: 0.4016 - accuracy: 0.8478 - val_loss: 0.2819 - val_accuracy: 0.9167\n",
            "Epoch 26/200\n",
            "92/92 [==============================] - 0s 473us/sample - loss: 0.4393 - accuracy: 0.8152 - val_loss: 0.3017 - val_accuracy: 0.8750\n",
            "Epoch 27/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.4513 - accuracy: 0.7935 - val_loss: 0.2909 - val_accuracy: 0.8750\n",
            "Epoch 28/200\n",
            "92/92 [==============================] - 0s 360us/sample - loss: 0.4180 - accuracy: 0.7935 - val_loss: 0.2667 - val_accuracy: 0.9583\n",
            "Epoch 29/200\n",
            "92/92 [==============================] - 0s 438us/sample - loss: 0.4109 - accuracy: 0.8478 - val_loss: 0.2687 - val_accuracy: 0.9583\n",
            "Epoch 30/200\n",
            "92/92 [==============================] - 0s 510us/sample - loss: 0.4394 - accuracy: 0.7826 - val_loss: 0.2685 - val_accuracy: 0.9583\n",
            "Epoch 31/200\n",
            "92/92 [==============================] - 0s 534us/sample - loss: 0.3908 - accuracy: 0.8261 - val_loss: 0.2729 - val_accuracy: 0.9167\n",
            "Epoch 32/200\n",
            "92/92 [==============================] - 0s 387us/sample - loss: 0.4261 - accuracy: 0.8261 - val_loss: 0.2808 - val_accuracy: 0.8750\n",
            "Epoch 33/200\n",
            "92/92 [==============================] - 0s 451us/sample - loss: 0.3766 - accuracy: 0.8478 - val_loss: 0.2774 - val_accuracy: 0.8750\n",
            "Epoch 34/200\n",
            "92/92 [==============================] - 0s 360us/sample - loss: 0.3370 - accuracy: 0.8913 - val_loss: 0.2716 - val_accuracy: 0.9167\n",
            "Epoch 35/200\n",
            "92/92 [==============================] - 0s 389us/sample - loss: 0.3752 - accuracy: 0.8696 - val_loss: 0.2701 - val_accuracy: 0.9167\n",
            "Epoch 36/200\n",
            "92/92 [==============================] - 0s 465us/sample - loss: 0.3440 - accuracy: 0.8370 - val_loss: 0.2651 - val_accuracy: 0.9167\n",
            "Epoch 37/200\n",
            "92/92 [==============================] - 0s 509us/sample - loss: 0.3827 - accuracy: 0.8696 - val_loss: 0.2617 - val_accuracy: 0.9167\n",
            "Epoch 38/200\n",
            "92/92 [==============================] - 0s 426us/sample - loss: 0.3357 - accuracy: 0.8587 - val_loss: 0.2673 - val_accuracy: 0.9167\n",
            "Epoch 39/200\n",
            "92/92 [==============================] - 0s 386us/sample - loss: 0.3754 - accuracy: 0.8043 - val_loss: 0.2716 - val_accuracy: 0.9167\n",
            "Epoch 40/200\n",
            "92/92 [==============================] - 0s 360us/sample - loss: 0.3817 - accuracy: 0.8804 - val_loss: 0.2672 - val_accuracy: 0.8750\n",
            "Epoch 41/200\n",
            "92/92 [==============================] - 0s 427us/sample - loss: 0.3307 - accuracy: 0.8261 - val_loss: 0.2669 - val_accuracy: 0.8750\n",
            "Epoch 42/200\n",
            "92/92 [==============================] - 0s 487us/sample - loss: 0.2939 - accuracy: 0.9022 - val_loss: 0.2640 - val_accuracy: 0.9167\n",
            "Epoch 43/200\n",
            "92/92 [==============================] - 0s 458us/sample - loss: 0.3147 - accuracy: 0.8696 - val_loss: 0.2650 - val_accuracy: 0.8750\n",
            "Epoch 44/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.3252 - accuracy: 0.8696 - val_loss: 0.2655 - val_accuracy: 0.9167\n",
            "Epoch 45/200\n",
            "92/92 [==============================] - 0s 392us/sample - loss: 0.3555 - accuracy: 0.8478 - val_loss: 0.2686 - val_accuracy: 0.9167\n",
            "Epoch 46/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.3149 - accuracy: 0.8696 - val_loss: 0.2613 - val_accuracy: 0.9167\n",
            "Epoch 47/200\n",
            "92/92 [==============================] - 0s 321us/sample - loss: 0.3024 - accuracy: 0.8804 - val_loss: 0.2630 - val_accuracy: 0.8750\n",
            "Epoch 48/200\n",
            "92/92 [==============================] - 0s 370us/sample - loss: 0.3130 - accuracy: 0.8696 - val_loss: 0.2610 - val_accuracy: 0.8750\n",
            "Epoch 49/200\n",
            "92/92 [==============================] - 0s 325us/sample - loss: 0.3194 - accuracy: 0.8913 - val_loss: 0.2628 - val_accuracy: 0.8750\n",
            "Epoch 50/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.2784 - accuracy: 0.9022 - val_loss: 0.2622 - val_accuracy: 0.8750\n",
            "Epoch 51/200\n",
            "92/92 [==============================] - 0s 380us/sample - loss: 0.3280 - accuracy: 0.8696 - val_loss: 0.2585 - val_accuracy: 0.9167\n",
            "Epoch 52/200\n",
            "92/92 [==============================] - 0s 406us/sample - loss: 0.3491 - accuracy: 0.8478 - val_loss: 0.2557 - val_accuracy: 0.9167\n",
            "Epoch 53/200\n",
            "92/92 [==============================] - 0s 373us/sample - loss: 0.3022 - accuracy: 0.8804 - val_loss: 0.2637 - val_accuracy: 0.8750\n",
            "Epoch 54/200\n",
            "92/92 [==============================] - 0s 364us/sample - loss: 0.2698 - accuracy: 0.8804 - val_loss: 0.2629 - val_accuracy: 0.8750\n",
            "Epoch 55/200\n",
            "92/92 [==============================] - 0s 424us/sample - loss: 0.2927 - accuracy: 0.8804 - val_loss: 0.2626 - val_accuracy: 0.8750\n",
            "Epoch 56/200\n",
            "92/92 [==============================] - 0s 379us/sample - loss: 0.3044 - accuracy: 0.9130 - val_loss: 0.2570 - val_accuracy: 0.9167\n",
            "Epoch 57/200\n",
            "92/92 [==============================] - 0s 565us/sample - loss: 0.2708 - accuracy: 0.8913 - val_loss: 0.2554 - val_accuracy: 0.9167\n",
            "Epoch 58/200\n",
            "92/92 [==============================] - 0s 424us/sample - loss: 0.2637 - accuracy: 0.9130 - val_loss: 0.2615 - val_accuracy: 0.9167\n",
            "Epoch 59/200\n",
            "92/92 [==============================] - 0s 390us/sample - loss: 0.2875 - accuracy: 0.8913 - val_loss: 0.2618 - val_accuracy: 0.9167\n",
            "Epoch 60/200\n",
            "92/92 [==============================] - 0s 329us/sample - loss: 0.2624 - accuracy: 0.9022 - val_loss: 0.2635 - val_accuracy: 0.9167\n",
            "Epoch 61/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.2368 - accuracy: 0.9348 - val_loss: 0.2665 - val_accuracy: 0.9167\n",
            "Epoch 62/200\n",
            "92/92 [==============================] - 0s 403us/sample - loss: 0.2796 - accuracy: 0.9130 - val_loss: 0.2704 - val_accuracy: 0.8750\n",
            "Epoch 63/200\n",
            "92/92 [==============================] - 0s 345us/sample - loss: 0.2452 - accuracy: 0.9130 - val_loss: 0.2750 - val_accuracy: 0.8750\n",
            "Epoch 64/200\n",
            "92/92 [==============================] - 0s 435us/sample - loss: 0.2508 - accuracy: 0.9130 - val_loss: 0.2762 - val_accuracy: 0.8750\n",
            "Epoch 65/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.1973 - accuracy: 0.9348 - val_loss: 0.2691 - val_accuracy: 0.8750\n",
            "Epoch 66/200\n",
            "92/92 [==============================] - 0s 443us/sample - loss: 0.2714 - accuracy: 0.8478 - val_loss: 0.2691 - val_accuracy: 0.9167\n",
            "Epoch 67/200\n",
            "92/92 [==============================] - 0s 440us/sample - loss: 0.2511 - accuracy: 0.9348 - val_loss: 0.2735 - val_accuracy: 0.9167\n",
            "Epoch 68/200\n",
            "92/92 [==============================] - 0s 479us/sample - loss: 0.2500 - accuracy: 0.9130 - val_loss: 0.2715 - val_accuracy: 0.9167\n",
            "Epoch 69/200\n",
            "92/92 [==============================] - 0s 394us/sample - loss: 0.2254 - accuracy: 0.9348 - val_loss: 0.2730 - val_accuracy: 0.8750\n",
            "Epoch 70/200\n",
            "92/92 [==============================] - 0s 426us/sample - loss: 0.2088 - accuracy: 0.9239 - val_loss: 0.2900 - val_accuracy: 0.8750\n",
            "Epoch 71/200\n",
            "92/92 [==============================] - 0s 419us/sample - loss: 0.1837 - accuracy: 0.9348 - val_loss: 0.2873 - val_accuracy: 0.8750\n",
            "Epoch 72/200\n",
            "92/92 [==============================] - 0s 429us/sample - loss: 0.2200 - accuracy: 0.9239 - val_loss: 0.2973 - val_accuracy: 0.8750\n",
            "Epoch 73/200\n",
            "92/92 [==============================] - 0s 463us/sample - loss: 0.2470 - accuracy: 0.8913 - val_loss: 0.2810 - val_accuracy: 0.8750\n",
            "Epoch 74/200\n",
            "92/92 [==============================] - 0s 524us/sample - loss: 0.2238 - accuracy: 0.9130 - val_loss: 0.2658 - val_accuracy: 0.9167\n",
            "Epoch 75/200\n",
            "92/92 [==============================] - 0s 421us/sample - loss: 0.2389 - accuracy: 0.8804 - val_loss: 0.2671 - val_accuracy: 0.8750\n",
            "Epoch 76/200\n",
            "92/92 [==============================] - 0s 364us/sample - loss: 0.2104 - accuracy: 0.9022 - val_loss: 0.2722 - val_accuracy: 0.8750\n",
            "Epoch 77/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.1890 - accuracy: 0.9239 - val_loss: 0.2730 - val_accuracy: 0.8750\n",
            "Epoch 78/200\n",
            "92/92 [==============================] - 0s 361us/sample - loss: 0.1787 - accuracy: 0.9565 - val_loss: 0.2778 - val_accuracy: 0.8750\n",
            "Epoch 79/200\n",
            "92/92 [==============================] - 0s 339us/sample - loss: 0.1875 - accuracy: 0.9457 - val_loss: 0.2857 - val_accuracy: 0.8333\n",
            "Epoch 80/200\n",
            "92/92 [==============================] - 0s 369us/sample - loss: 0.1863 - accuracy: 0.9674 - val_loss: 0.2988 - val_accuracy: 0.8750\n",
            "Epoch 81/200\n",
            "92/92 [==============================] - 0s 369us/sample - loss: 0.2120 - accuracy: 0.9239 - val_loss: 0.2889 - val_accuracy: 0.8750\n",
            "Epoch 82/200\n",
            "92/92 [==============================] - 0s 487us/sample - loss: 0.1659 - accuracy: 0.9457 - val_loss: 0.2779 - val_accuracy: 0.8750\n",
            "Epoch 83/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.2074 - accuracy: 0.9457 - val_loss: 0.2984 - val_accuracy: 0.8750\n",
            "Epoch 84/200\n",
            "92/92 [==============================] - 0s 452us/sample - loss: 0.1605 - accuracy: 0.9348 - val_loss: 0.3156 - val_accuracy: 0.8750\n",
            "Epoch 85/200\n",
            "92/92 [==============================] - 0s 393us/sample - loss: 0.1993 - accuracy: 0.9348 - val_loss: 0.3018 - val_accuracy: 0.8750\n",
            "Epoch 86/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.1534 - accuracy: 0.9239 - val_loss: 0.2895 - val_accuracy: 0.9167\n",
            "Epoch 87/200\n",
            "92/92 [==============================] - 0s 344us/sample - loss: 0.1643 - accuracy: 0.9457 - val_loss: 0.2965 - val_accuracy: 0.9167\n",
            "Epoch 88/200\n",
            "92/92 [==============================] - 0s 407us/sample - loss: 0.1765 - accuracy: 0.9457 - val_loss: 0.3263 - val_accuracy: 0.8750\n",
            "Epoch 89/200\n",
            "92/92 [==============================] - 0s 409us/sample - loss: 0.1495 - accuracy: 0.9457 - val_loss: 0.3495 - val_accuracy: 0.8750\n",
            "Epoch 90/200\n",
            "92/92 [==============================] - 0s 354us/sample - loss: 0.1526 - accuracy: 0.9565 - val_loss: 0.3849 - val_accuracy: 0.8333\n",
            "Epoch 91/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.1228 - accuracy: 0.9783 - val_loss: 0.3622 - val_accuracy: 0.8750\n",
            "Epoch 92/200\n",
            "92/92 [==============================] - 0s 361us/sample - loss: 0.1726 - accuracy: 0.9457 - val_loss: 0.3587 - val_accuracy: 0.8750\n",
            "Epoch 93/200\n",
            "92/92 [==============================] - 0s 352us/sample - loss: 0.1903 - accuracy: 0.9565 - val_loss: 0.3403 - val_accuracy: 0.8750\n",
            "Epoch 94/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.2060 - accuracy: 0.9348 - val_loss: 0.3409 - val_accuracy: 0.8750\n",
            "Epoch 95/200\n",
            "92/92 [==============================] - 0s 346us/sample - loss: 0.1225 - accuracy: 0.9565 - val_loss: 0.3626 - val_accuracy: 0.8750\n",
            "Epoch 96/200\n",
            "92/92 [==============================] - 0s 390us/sample - loss: 0.1305 - accuracy: 0.9457 - val_loss: 0.4075 - val_accuracy: 0.8333\n",
            "Epoch 97/200\n",
            "92/92 [==============================] - 0s 395us/sample - loss: 0.1554 - accuracy: 0.9022 - val_loss: 0.3653 - val_accuracy: 0.8750\n",
            "Epoch 98/200\n",
            "92/92 [==============================] - 0s 348us/sample - loss: 0.1445 - accuracy: 0.9348 - val_loss: 0.3125 - val_accuracy: 0.8750\n",
            "Epoch 99/200\n",
            "92/92 [==============================] - 0s 354us/sample - loss: 0.1750 - accuracy: 0.9239 - val_loss: 0.3042 - val_accuracy: 0.9167\n",
            "Epoch 100/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.1948 - accuracy: 0.8913 - val_loss: 0.3165 - val_accuracy: 0.9167\n",
            "Epoch 101/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.1427 - accuracy: 0.9348 - val_loss: 0.3425 - val_accuracy: 0.8750\n",
            "Epoch 102/200\n",
            "92/92 [==============================] - 0s 406us/sample - loss: 0.2004 - accuracy: 0.8913 - val_loss: 0.3745 - val_accuracy: 0.8333\n",
            "Epoch 103/200\n",
            "92/92 [==============================] - 0s 326us/sample - loss: 0.1713 - accuracy: 0.9239 - val_loss: 0.3559 - val_accuracy: 0.8333\n",
            "Epoch 104/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.1880 - accuracy: 0.9130 - val_loss: 0.3299 - val_accuracy: 0.8750\n",
            "Epoch 105/200\n",
            "92/92 [==============================] - 0s 347us/sample - loss: 0.1033 - accuracy: 0.9891 - val_loss: 0.3226 - val_accuracy: 0.8750\n",
            "Epoch 106/200\n",
            "92/92 [==============================] - 0s 344us/sample - loss: 0.1137 - accuracy: 0.9674 - val_loss: 0.3326 - val_accuracy: 0.8750\n",
            "Epoch 107/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.1323 - accuracy: 0.9348 - val_loss: 0.3640 - val_accuracy: 0.8750\n",
            "Epoch 108/200\n",
            "92/92 [==============================] - 0s 348us/sample - loss: 0.1168 - accuracy: 0.9891 - val_loss: 0.3807 - val_accuracy: 0.8750\n",
            "Epoch 109/200\n",
            "92/92 [==============================] - 0s 498us/sample - loss: 0.1065 - accuracy: 0.9891 - val_loss: 0.3895 - val_accuracy: 0.8750\n",
            "Epoch 110/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.1196 - accuracy: 0.9348 - val_loss: 0.3681 - val_accuracy: 0.8750\n",
            "Epoch 111/200\n",
            "92/92 [==============================] - 0s 379us/sample - loss: 0.1416 - accuracy: 0.9130 - val_loss: 0.3750 - val_accuracy: 0.8750\n",
            "Epoch 112/200\n",
            "92/92 [==============================] - 0s 373us/sample - loss: 0.1252 - accuracy: 0.9783 - val_loss: 0.3929 - val_accuracy: 0.8333\n",
            "Epoch 113/200\n",
            "92/92 [==============================] - 0s 425us/sample - loss: 0.1444 - accuracy: 0.9130 - val_loss: 0.4194 - val_accuracy: 0.8750\n",
            "Epoch 114/200\n",
            "92/92 [==============================] - 0s 339us/sample - loss: 0.0970 - accuracy: 0.9565 - val_loss: 0.4339 - val_accuracy: 0.8750\n",
            "Epoch 115/200\n",
            "92/92 [==============================] - 0s 381us/sample - loss: 0.1349 - accuracy: 0.9565 - val_loss: 0.4448 - val_accuracy: 0.8750\n",
            "Epoch 116/200\n",
            "92/92 [==============================] - 0s 391us/sample - loss: 0.1714 - accuracy: 0.8913 - val_loss: 0.4743 - val_accuracy: 0.8750\n",
            "Epoch 117/200\n",
            "92/92 [==============================] - 0s 429us/sample - loss: 0.1197 - accuracy: 0.9565 - val_loss: 0.4714 - val_accuracy: 0.8750\n",
            "Epoch 118/200\n",
            "92/92 [==============================] - 0s 376us/sample - loss: 0.1200 - accuracy: 0.9565 - val_loss: 0.5005 - val_accuracy: 0.8333\n",
            "Epoch 119/200\n",
            "92/92 [==============================] - 0s 341us/sample - loss: 0.0962 - accuracy: 0.9674 - val_loss: 0.5143 - val_accuracy: 0.8333\n",
            "Epoch 120/200\n",
            "92/92 [==============================] - 0s 363us/sample - loss: 0.1297 - accuracy: 0.9565 - val_loss: 0.4768 - val_accuracy: 0.8333\n",
            "Epoch 121/200\n",
            "92/92 [==============================] - 0s 351us/sample - loss: 0.1285 - accuracy: 0.9565 - val_loss: 0.4499 - val_accuracy: 0.8333\n",
            "Epoch 122/200\n",
            "92/92 [==============================] - 0s 434us/sample - loss: 0.1279 - accuracy: 0.9674 - val_loss: 0.4494 - val_accuracy: 0.8333\n",
            "Epoch 123/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.1416 - accuracy: 0.9457 - val_loss: 0.4596 - val_accuracy: 0.8333\n",
            "Epoch 124/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.1247 - accuracy: 0.9565 - val_loss: 0.4756 - val_accuracy: 0.8333\n",
            "Epoch 125/200\n",
            "92/92 [==============================] - 0s 364us/sample - loss: 0.1001 - accuracy: 0.9674 - val_loss: 0.5150 - val_accuracy: 0.8333\n",
            "Epoch 126/200\n",
            "92/92 [==============================] - 0s 356us/sample - loss: 0.1050 - accuracy: 0.9783 - val_loss: 0.4960 - val_accuracy: 0.8750\n",
            "Epoch 127/200\n",
            "92/92 [==============================] - 0s 362us/sample - loss: 0.0994 - accuracy: 0.9783 - val_loss: 0.4518 - val_accuracy: 0.8750\n",
            "Epoch 128/200\n",
            "92/92 [==============================] - 0s 370us/sample - loss: 0.0927 - accuracy: 0.9565 - val_loss: 0.4284 - val_accuracy: 0.8750\n",
            "Epoch 129/200\n",
            "92/92 [==============================] - 0s 397us/sample - loss: 0.1438 - accuracy: 0.9565 - val_loss: 0.4312 - val_accuracy: 0.8750\n",
            "Epoch 130/200\n",
            "92/92 [==============================] - 0s 333us/sample - loss: 0.1065 - accuracy: 0.9565 - val_loss: 0.4568 - val_accuracy: 0.8750\n",
            "Epoch 131/200\n",
            "92/92 [==============================] - 0s 354us/sample - loss: 0.0944 - accuracy: 0.9674 - val_loss: 0.4829 - val_accuracy: 0.8333\n",
            "Epoch 132/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.0795 - accuracy: 0.9783 - val_loss: 0.4564 - val_accuracy: 0.8750\n",
            "Epoch 133/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.1219 - accuracy: 0.9674 - val_loss: 0.4118 - val_accuracy: 0.8750\n",
            "Epoch 134/200\n",
            "92/92 [==============================] - 0s 383us/sample - loss: 0.1369 - accuracy: 0.9348 - val_loss: 0.3904 - val_accuracy: 0.8750\n",
            "Epoch 135/200\n",
            "92/92 [==============================] - 0s 407us/sample - loss: 0.0869 - accuracy: 1.0000 - val_loss: 0.4033 - val_accuracy: 0.8750\n",
            "Epoch 136/200\n",
            "92/92 [==============================] - 0s 489us/sample - loss: 0.0844 - accuracy: 0.9783 - val_loss: 0.4460 - val_accuracy: 0.8333\n",
            "Epoch 137/200\n",
            "92/92 [==============================] - 0s 423us/sample - loss: 0.0824 - accuracy: 0.9783 - val_loss: 0.4868 - val_accuracy: 0.8333\n",
            "Epoch 138/200\n",
            "92/92 [==============================] - 0s 358us/sample - loss: 0.0931 - accuracy: 0.9565 - val_loss: 0.4869 - val_accuracy: 0.8333\n",
            "Epoch 139/200\n",
            "92/92 [==============================] - 0s 334us/sample - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.4640 - val_accuracy: 0.8750\n",
            "Epoch 140/200\n",
            "92/92 [==============================] - 0s 394us/sample - loss: 0.0670 - accuracy: 0.9783 - val_loss: 0.4633 - val_accuracy: 0.8750\n",
            "Epoch 141/200\n",
            "92/92 [==============================] - 0s 535us/sample - loss: 0.0720 - accuracy: 0.9891 - val_loss: 0.4783 - val_accuracy: 0.8750\n",
            "Epoch 142/200\n",
            "92/92 [==============================] - 0s 411us/sample - loss: 0.0788 - accuracy: 0.9783 - val_loss: 0.4899 - val_accuracy: 0.8750\n",
            "Epoch 143/200\n",
            "92/92 [==============================] - 0s 359us/sample - loss: 0.0428 - accuracy: 0.9891 - val_loss: 0.4967 - val_accuracy: 0.8750\n",
            "Epoch 144/200\n",
            "92/92 [==============================] - 0s 391us/sample - loss: 0.0790 - accuracy: 0.9674 - val_loss: 0.5147 - val_accuracy: 0.8750\n",
            "Epoch 145/200\n",
            "92/92 [==============================] - 0s 392us/sample - loss: 0.0596 - accuracy: 0.9783 - val_loss: 0.5381 - val_accuracy: 0.8750\n",
            "Epoch 146/200\n",
            "92/92 [==============================] - 0s 461us/sample - loss: 0.0554 - accuracy: 1.0000 - val_loss: 0.5345 - val_accuracy: 0.8750\n",
            "Epoch 147/200\n",
            "92/92 [==============================] - 0s 446us/sample - loss: 0.0751 - accuracy: 0.9674 - val_loss: 0.5701 - val_accuracy: 0.8333\n",
            "Epoch 148/200\n",
            "92/92 [==============================] - 0s 327us/sample - loss: 0.0545 - accuracy: 0.9891 - val_loss: 0.5668 - val_accuracy: 0.8333\n",
            "Epoch 149/200\n",
            "92/92 [==============================] - 0s 406us/sample - loss: 0.0827 - accuracy: 0.9783 - val_loss: 0.5278 - val_accuracy: 0.8750\n",
            "Epoch 150/200\n",
            "92/92 [==============================] - 0s 344us/sample - loss: 0.0575 - accuracy: 0.9783 - val_loss: 0.4999 - val_accuracy: 0.8750\n",
            "Epoch 151/200\n",
            "92/92 [==============================] - 0s 441us/sample - loss: 0.0774 - accuracy: 0.9891 - val_loss: 0.4982 - val_accuracy: 0.8750\n",
            "Epoch 152/200\n",
            "92/92 [==============================] - 0s 388us/sample - loss: 0.1256 - accuracy: 0.9457 - val_loss: 0.5170 - val_accuracy: 0.8750\n",
            "Epoch 153/200\n",
            "92/92 [==============================] - 0s 365us/sample - loss: 0.1090 - accuracy: 0.9565 - val_loss: 0.5924 - val_accuracy: 0.7917\n",
            "Epoch 154/200\n",
            "92/92 [==============================] - 0s 514us/sample - loss: 0.0521 - accuracy: 0.9891 - val_loss: 0.6572 - val_accuracy: 0.7917\n",
            "Epoch 155/200\n",
            "92/92 [==============================] - 0s 459us/sample - loss: 0.0927 - accuracy: 0.9674 - val_loss: 0.5919 - val_accuracy: 0.8333\n",
            "Epoch 156/200\n",
            "92/92 [==============================] - 0s 412us/sample - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.5301 - val_accuracy: 0.8333\n",
            "Epoch 157/200\n",
            "92/92 [==============================] - 0s 364us/sample - loss: 0.0616 - accuracy: 0.9674 - val_loss: 0.5246 - val_accuracy: 0.8750\n",
            "Epoch 158/200\n",
            "92/92 [==============================] - 0s 315us/sample - loss: 0.0480 - accuracy: 1.0000 - val_loss: 0.5494 - val_accuracy: 0.8333\n",
            "Epoch 159/200\n",
            "92/92 [==============================] - 0s 327us/sample - loss: 0.0745 - accuracy: 0.9783 - val_loss: 0.5811 - val_accuracy: 0.8333\n",
            "Epoch 160/200\n",
            "92/92 [==============================] - 0s 318us/sample - loss: 0.1509 - accuracy: 0.9565 - val_loss: 0.5927 - val_accuracy: 0.7917\n",
            "Epoch 161/200\n",
            "92/92 [==============================] - 0s 587us/sample - loss: 0.0551 - accuracy: 0.9891 - val_loss: 0.5685 - val_accuracy: 0.8750\n",
            "Epoch 162/200\n",
            "92/92 [==============================] - 0s 401us/sample - loss: 0.0537 - accuracy: 0.9783 - val_loss: 0.5453 - val_accuracy: 0.8750\n",
            "Epoch 163/200\n",
            "92/92 [==============================] - 0s 384us/sample - loss: 0.0712 - accuracy: 0.9565 - val_loss: 0.5541 - val_accuracy: 0.8750\n",
            "Epoch 164/200\n",
            "92/92 [==============================] - 0s 484us/sample - loss: 0.0806 - accuracy: 0.9674 - val_loss: 0.6103 - val_accuracy: 0.8333\n",
            "Epoch 165/200\n",
            "92/92 [==============================] - 0s 342us/sample - loss: 0.1112 - accuracy: 0.9674 - val_loss: 0.6506 - val_accuracy: 0.8333\n",
            "Epoch 166/200\n",
            "92/92 [==============================] - 0s 407us/sample - loss: 0.0891 - accuracy: 0.9674 - val_loss: 0.6004 - val_accuracy: 0.8333\n",
            "Epoch 167/200\n",
            "92/92 [==============================] - 0s 352us/sample - loss: 0.0958 - accuracy: 0.9674 - val_loss: 0.5027 - val_accuracy: 0.8333\n",
            "Epoch 168/200\n",
            "92/92 [==============================] - 0s 406us/sample - loss: 0.0767 - accuracy: 0.9674 - val_loss: 0.4759 - val_accuracy: 0.9167\n",
            "Epoch 169/200\n",
            "92/92 [==============================] - 0s 374us/sample - loss: 0.0871 - accuracy: 0.9783 - val_loss: 0.4870 - val_accuracy: 0.8333\n",
            "Epoch 170/200\n",
            "92/92 [==============================] - 0s 398us/sample - loss: 0.0983 - accuracy: 0.9674 - val_loss: 0.5271 - val_accuracy: 0.8333\n",
            "Epoch 171/200\n",
            "92/92 [==============================] - 0s 321us/sample - loss: 0.0510 - accuracy: 1.0000 - val_loss: 0.5604 - val_accuracy: 0.8333\n",
            "Epoch 172/200\n",
            "92/92 [==============================] - 0s 380us/sample - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.5663 - val_accuracy: 0.8333\n",
            "Epoch 173/200\n",
            "92/92 [==============================] - 0s 346us/sample - loss: 0.0703 - accuracy: 0.9891 - val_loss: 0.5522 - val_accuracy: 0.8333\n",
            "Epoch 174/200\n",
            "92/92 [==============================] - 0s 350us/sample - loss: 0.0737 - accuracy: 0.9783 - val_loss: 0.5605 - val_accuracy: 0.8333\n",
            "Epoch 175/200\n",
            "92/92 [==============================] - 0s 406us/sample - loss: 0.0706 - accuracy: 0.9674 - val_loss: 0.5604 - val_accuracy: 0.8333\n",
            "Epoch 176/200\n",
            "92/92 [==============================] - 0s 323us/sample - loss: 0.0559 - accuracy: 0.9891 - val_loss: 0.5614 - val_accuracy: 0.8750\n",
            "Epoch 177/200\n",
            "92/92 [==============================] - 0s 328us/sample - loss: 0.0453 - accuracy: 0.9891 - val_loss: 0.5504 - val_accuracy: 0.8750\n",
            "Epoch 178/200\n",
            "92/92 [==============================] - 0s 329us/sample - loss: 0.0472 - accuracy: 0.9783 - val_loss: 0.5442 - val_accuracy: 0.8750\n",
            "Epoch 179/200\n",
            "92/92 [==============================] - 0s 386us/sample - loss: 0.0509 - accuracy: 0.9674 - val_loss: 0.5522 - val_accuracy: 0.8750\n",
            "Epoch 180/200\n",
            "92/92 [==============================] - 0s 346us/sample - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.5859 - val_accuracy: 0.8750\n",
            "Epoch 181/200\n",
            "92/92 [==============================] - 0s 482us/sample - loss: 0.0635 - accuracy: 0.9783 - val_loss: 0.6114 - val_accuracy: 0.8750\n",
            "Epoch 182/200\n",
            "92/92 [==============================] - 0s 444us/sample - loss: 0.0553 - accuracy: 0.9783 - val_loss: 0.6451 - val_accuracy: 0.8333\n",
            "Epoch 183/200\n",
            "92/92 [==============================] - 0s 444us/sample - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.6735 - val_accuracy: 0.7917\n",
            "Epoch 184/200\n",
            "92/92 [==============================] - 0s 485us/sample - loss: 0.0249 - accuracy: 0.9891 - val_loss: 0.6834 - val_accuracy: 0.8750\n",
            "Epoch 185/200\n",
            "92/92 [==============================] - 0s 444us/sample - loss: 0.0410 - accuracy: 0.9783 - val_loss: 0.6936 - val_accuracy: 0.8750\n",
            "Epoch 186/200\n",
            "92/92 [==============================] - 0s 411us/sample - loss: 0.0514 - accuracy: 0.9891 - val_loss: 0.6858 - val_accuracy: 0.8750\n",
            "Epoch 187/200\n",
            "92/92 [==============================] - 0s 635us/sample - loss: 0.0525 - accuracy: 1.0000 - val_loss: 0.6822 - val_accuracy: 0.8333\n",
            "Epoch 188/200\n",
            "92/92 [==============================] - 0s 419us/sample - loss: 0.0564 - accuracy: 0.9891 - val_loss: 0.6509 - val_accuracy: 0.8750\n",
            "Epoch 189/200\n",
            "92/92 [==============================] - 0s 420us/sample - loss: 0.0535 - accuracy: 0.9891 - val_loss: 0.6607 - val_accuracy: 0.8333\n",
            "Epoch 190/200\n",
            "92/92 [==============================] - 0s 378us/sample - loss: 0.0586 - accuracy: 0.9783 - val_loss: 0.7112 - val_accuracy: 0.8333\n",
            "Epoch 191/200\n",
            "92/92 [==============================] - 0s 341us/sample - loss: 0.0514 - accuracy: 0.9891 - val_loss: 0.7386 - val_accuracy: 0.8333\n",
            "Epoch 192/200\n",
            "92/92 [==============================] - 0s 322us/sample - loss: 0.0486 - accuracy: 0.9783 - val_loss: 0.7258 - val_accuracy: 0.8333\n",
            "Epoch 193/200\n",
            "92/92 [==============================] - 0s 385us/sample - loss: 0.0491 - accuracy: 0.9783 - val_loss: 0.7295 - val_accuracy: 0.8333\n",
            "Epoch 194/200\n",
            "92/92 [==============================] - 0s 436us/sample - loss: 0.0833 - accuracy: 0.9783 - val_loss: 0.6976 - val_accuracy: 0.8750\n",
            "Epoch 195/200\n",
            "92/92 [==============================] - 0s 459us/sample - loss: 0.0362 - accuracy: 0.9891 - val_loss: 0.6832 - val_accuracy: 0.8750\n",
            "Epoch 196/200\n",
            "92/92 [==============================] - 0s 413us/sample - loss: 0.0431 - accuracy: 0.9891 - val_loss: 0.6685 - val_accuracy: 0.8750\n",
            "Epoch 197/200\n",
            "92/92 [==============================] - 0s 369us/sample - loss: 0.0987 - accuracy: 0.9565 - val_loss: 0.6664 - val_accuracy: 0.8750\n",
            "Epoch 198/200\n",
            "92/92 [==============================] - 0s 357us/sample - loss: 0.0432 - accuracy: 0.9783 - val_loss: 0.6915 - val_accuracy: 0.8750\n",
            "Epoch 199/200\n",
            "92/92 [==============================] - 0s 373us/sample - loss: 0.0778 - accuracy: 0.9674 - val_loss: 0.7105 - val_accuracy: 0.8333\n",
            "Epoch 200/200\n",
            "92/92 [==============================] - 0s 430us/sample - loss: 0.0407 - accuracy: 1.0000 - val_loss: 0.6835 - val_accuracy: 0.8333\n",
            ">#5: 83.333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1umZ22j5xvUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b89253b-fd5c-4547-c646-110dded65b50"
      },
      "source": [
        "'''\n",
        "def mean(x):\n",
        "\ts = 0;\n",
        "\tfor i in x:\n",
        "\t\ts = s+ i\n",
        "\treturn s / len(x)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef mean(x):\\n\\ts = 0;\\n\\tfor i in x:\\n\\t\\ts = s+ i\\n\\treturn s / len(x)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmOuzuK7jzkj"
      },
      "source": [
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-Mud2PpjOPs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1052fd21-f47b-49bf-f6b3-3cdf115bb6d1"
      },
      "source": [
        "\n",
        "# run an experiment\n",
        "def run_experiment(repeats=10):\n",
        "\t# load data\n",
        "\t# repeat experiment\n",
        "\tscores = list()\n",
        "\tfor r in range(repeats):\n",
        "\t\tscore = evaluate_model(x_train, y_train, x_test, y_test)\n",
        "\t\tscore = score * 100.0\n",
        "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
        "\t\tscores.append(score)\n",
        "\t# summarize results\n",
        "summarize_results(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[100.0, 83.33333134651184, 95.83333134651184, 95.83333134651184, 91.66666865348816, 95.83333134651184, 91.66666865348816, 91.66666865348816, 95.83333134651184, 87.5]\n",
            "Accuracy: 92.917% (+/-4.583)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-qbfQCtmike",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f996f4b0-9921-44a7-8ee1-cacc2bfe5270"
      },
      "source": [
        "run_experiment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 92 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbfb5d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbfb5d08>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbfb5d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbfb5d08>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 0s 5ms/sample - loss: 0.6962 - accuracy: 0.4674\n",
            "Epoch 2/10\n",
            "92/92 [==============================] - 0s 78us/sample - loss: 0.6814 - accuracy: 0.6087\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 0s 147us/sample - loss: 0.6723 - accuracy: 0.5761\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 0s 54us/sample - loss: 0.6528 - accuracy: 0.5978\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 0s 62us/sample - loss: 0.6493 - accuracy: 0.6304\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 0s 62us/sample - loss: 0.6491 - accuracy: 0.6304\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 0s 76us/sample - loss: 0.6444 - accuracy: 0.6304\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 0s 61us/sample - loss: 0.6291 - accuracy: 0.6630\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 0s 65us/sample - loss: 0.6340 - accuracy: 0.7065\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 0s 76us/sample - loss: 0.6304 - accuracy: 0.6739\n",
            ">#1: 91.667\n",
            "Train on 92 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56da0b2268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56da0b2268>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56da0b2268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56da0b2268>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 0s 5ms/sample - loss: 0.6687 - accuracy: 0.6087\n",
            "Epoch 2/10\n",
            "92/92 [==============================] - 0s 84us/sample - loss: 0.6568 - accuracy: 0.6304\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 0s 74us/sample - loss: 0.6481 - accuracy: 0.6630\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 0s 67us/sample - loss: 0.6467 - accuracy: 0.6413\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 0s 85us/sample - loss: 0.6269 - accuracy: 0.6196\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 0s 101us/sample - loss: 0.6224 - accuracy: 0.6739\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 0s 83us/sample - loss: 0.6141 - accuracy: 0.6630\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 0s 73us/sample - loss: 0.6207 - accuracy: 0.6304\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 0s 77us/sample - loss: 0.6009 - accuracy: 0.7065\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 0s 57us/sample - loss: 0.5949 - accuracy: 0.7283\n",
            ">#2: 95.833\n",
            "Train on 92 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d2432158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d2432158>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d2432158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d2432158>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 0s 5ms/sample - loss: 0.6966 - accuracy: 0.5652\n",
            "Epoch 2/10\n",
            "92/92 [==============================] - 0s 80us/sample - loss: 0.6824 - accuracy: 0.5870\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 0s 99us/sample - loss: 0.6649 - accuracy: 0.5978\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 0s 75us/sample - loss: 0.6587 - accuracy: 0.5761\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 0s 64us/sample - loss: 0.6516 - accuracy: 0.6087\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 0s 55us/sample - loss: 0.6449 - accuracy: 0.6630\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 0s 87us/sample - loss: 0.6522 - accuracy: 0.5870\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 0s 56us/sample - loss: 0.6298 - accuracy: 0.6957\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 0s 81us/sample - loss: 0.6113 - accuracy: 0.7174\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 0s 67us/sample - loss: 0.6108 - accuracy: 0.6957\n",
            ">#3: 95.833\n",
            "Train on 92 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d70560d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d70560d0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d70560d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d70560d0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 0s 5ms/sample - loss: 0.7408 - accuracy: 0.4348\n",
            "Epoch 2/10\n",
            "92/92 [==============================] - 0s 79us/sample - loss: 0.7091 - accuracy: 0.4130\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 0s 67us/sample - loss: 0.6932 - accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 0s 54us/sample - loss: 0.6823 - accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 0s 64us/sample - loss: 0.6578 - accuracy: 0.6413\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 0s 85us/sample - loss: 0.6578 - accuracy: 0.6087\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 0s 95us/sample - loss: 0.6567 - accuracy: 0.5652\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 0s 70us/sample - loss: 0.6528 - accuracy: 0.5870\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 0s 67us/sample - loss: 0.6427 - accuracy: 0.6304\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 0s 64us/sample - loss: 0.6349 - accuracy: 0.6413\n",
            ">#4: 70.833\n",
            "Train on 92 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d60a26a8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d60a26a8>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d60a26a8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d60a26a8>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 0s 5ms/sample - loss: 0.7227 - accuracy: 0.4457\n",
            "Epoch 2/10\n",
            "92/92 [==============================] - 0s 106us/sample - loss: 0.6934 - accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 0s 72us/sample - loss: 0.6794 - accuracy: 0.5761\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 0s 64us/sample - loss: 0.6608 - accuracy: 0.5652\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 0s 75us/sample - loss: 0.6721 - accuracy: 0.5870\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 0s 69us/sample - loss: 0.6412 - accuracy: 0.6630\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 0s 62us/sample - loss: 0.6403 - accuracy: 0.7065\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 0s 58us/sample - loss: 0.6392 - accuracy: 0.6630\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 0s 57us/sample - loss: 0.6390 - accuracy: 0.6522\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 0s 65us/sample - loss: 0.6263 - accuracy: 0.6413\n",
            ">#5: 79.167\n",
            "Train on 92 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e291a158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e291a158>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e291a158> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e291a158>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 0s 5ms/sample - loss: 0.7068 - accuracy: 0.5217\n",
            "Epoch 2/10\n",
            "92/92 [==============================] - 0s 89us/sample - loss: 0.6892 - accuracy: 0.5435\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 0s 76us/sample - loss: 0.6781 - accuracy: 0.5978\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 0s 77us/sample - loss: 0.6462 - accuracy: 0.6196\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 0s 58us/sample - loss: 0.6580 - accuracy: 0.6087\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 0s 66us/sample - loss: 0.6330 - accuracy: 0.6522\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 0s 106us/sample - loss: 0.6345 - accuracy: 0.6087\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 0s 80us/sample - loss: 0.6285 - accuracy: 0.5978\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 0s 76us/sample - loss: 0.6239 - accuracy: 0.6196\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 0s 106us/sample - loss: 0.6198 - accuracy: 0.6413\n",
            ">#6: 91.667\n",
            "Train on 92 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d47050d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d47050d0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d47050d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d47050d0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 0s 5ms/sample - loss: 0.7050 - accuracy: 0.4565\n",
            "Epoch 2/10\n",
            "92/92 [==============================] - 0s 79us/sample - loss: 0.6826 - accuracy: 0.5870\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 0s 79us/sample - loss: 0.6787 - accuracy: 0.5543\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 0s 75us/sample - loss: 0.6659 - accuracy: 0.6196\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 0s 60us/sample - loss: 0.6642 - accuracy: 0.6087\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 0s 61us/sample - loss: 0.6654 - accuracy: 0.6196\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 0s 78us/sample - loss: 0.6472 - accuracy: 0.6304\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 0s 52us/sample - loss: 0.6381 - accuracy: 0.6304\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 0s 80us/sample - loss: 0.6367 - accuracy: 0.6413\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 0s 60us/sample - loss: 0.6246 - accuracy: 0.6957\n",
            ">#7: 87.500\n",
            "Train on 92 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e7791488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e7791488>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e7791488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56e7791488>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 0s 5ms/sample - loss: 0.6929 - accuracy: 0.5761\n",
            "Epoch 2/10\n",
            "92/92 [==============================] - 0s 119us/sample - loss: 0.6819 - accuracy: 0.5543\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 0s 89us/sample - loss: 0.6685 - accuracy: 0.5761\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 0s 99us/sample - loss: 0.6671 - accuracy: 0.5870\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 0s 60us/sample - loss: 0.6465 - accuracy: 0.6413\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 0s 97us/sample - loss: 0.6503 - accuracy: 0.5978\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 0s 80us/sample - loss: 0.6523 - accuracy: 0.5978\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 0s 84us/sample - loss: 0.6352 - accuracy: 0.5978\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 0s 85us/sample - loss: 0.6276 - accuracy: 0.6848\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 0s 92us/sample - loss: 0.6187 - accuracy: 0.6413\n",
            ">#8: 95.833\n",
            "Train on 92 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbcffd08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbcffd08>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbcffd08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56dbcffd08>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 0s 5ms/sample - loss: 0.6990 - accuracy: 0.5109\n",
            "Epoch 2/10\n",
            "92/92 [==============================] - 0s 91us/sample - loss: 0.6839 - accuracy: 0.5652\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 0s 118us/sample - loss: 0.6762 - accuracy: 0.5870\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 0s 95us/sample - loss: 0.6585 - accuracy: 0.6304\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 0s 96us/sample - loss: 0.6466 - accuracy: 0.6196\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 0s 94us/sample - loss: 0.6439 - accuracy: 0.6196\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 0s 76us/sample - loss: 0.6421 - accuracy: 0.6304\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 0s 76us/sample - loss: 0.6259 - accuracy: 0.6522\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 0s 125us/sample - loss: 0.6202 - accuracy: 0.6957\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 0s 85us/sample - loss: 0.6188 - accuracy: 0.7283\n",
            ">#9: 95.833\n",
            "Train on 92 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d48a60d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d48a60d0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d48a60d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f56d48a60d0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
            "92/92 [==============================] - 0s 5ms/sample - loss: 0.6978 - accuracy: 0.5326\n",
            "Epoch 2/10\n",
            "92/92 [==============================] - 0s 100us/sample - loss: 0.6740 - accuracy: 0.5870\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 0s 90us/sample - loss: 0.6665 - accuracy: 0.6304\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 0s 90us/sample - loss: 0.6591 - accuracy: 0.5978\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 0s 86us/sample - loss: 0.6441 - accuracy: 0.6957\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 0s 70us/sample - loss: 0.6327 - accuracy: 0.6630\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 0s 70us/sample - loss: 0.6235 - accuracy: 0.7391\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 0s 132us/sample - loss: 0.6208 - accuracy: 0.7391\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 0s 84us/sample - loss: 0.6005 - accuracy: 0.7065\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 0s 85us/sample - loss: 0.5855 - accuracy: 0.7717\n",
            ">#10: 95.833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyaW5V84hHEt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0e841cea-1838-4520-b142-699b64ef0419"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "predict = model.predict_classes(x_test)\n",
        "cm = confusion_matrix(y_test, predict)\n",
        "print(cm)\n",
        "\n",
        "\n",
        "TP = cm[0][0]\n",
        "TN = cm[1][1]\n",
        "FP = cm[1][0]\n",
        "FN = cm[0][1] \n",
        "\n",
        "print('Testing Accuracy: ',(TP+TN)/(TP+TN+FN+FP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 9  2]\n",
            " [ 1 12]]\n",
            "Testing Accuracy:  0.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSHL-tuNltWa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "3dbfa927-cd6e-4fe4-e960-e682ce5fec32"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "Y_pred = model.predict(x_test)\n",
        "Y_pred = np.argmax(Y_pred, axis=1)\n",
        "Y_pred = model.predict_classes(x_test)\n",
        "\n",
        "new_y = y_test.reshape(24,1,1)\n",
        "\n",
        "print(y_test)\n",
        "\n",
        "p = model.predict_proba(x_test)\n",
        "\n",
        "target_names = ['Class 1(Control)', 'Class 2(Cancer)']\n",
        "print(classification_report(np.argmax(new_y, axis=1),Y_pred,target_names=target_names))\n",
        "print(confusion_matrix(np.argmax(new_y, axis=1),Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0]\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Class 1(Control)       1.00      0.42      0.59        24\n",
            " Class 2(Cancer)       0.00      0.00      0.00         0\n",
            "\n",
            "        accuracy                           0.42        24\n",
            "       macro avg       0.50      0.21      0.29        24\n",
            "    weighted avg       1.00      0.42      0.59        24\n",
            "\n",
            "[[10 14]\n",
            " [ 0  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5NTEieJ6kGQ"
      },
      "source": [
        "def predict(model, x_aux, y_aux):\n",
        "\n",
        "    prediction = model.predict(x_aux)\n",
        "    print(prediction)\n",
        "\n",
        "    prediction_index = np.argmax(prediction,axis=1)\n",
        "    print(\" Prediction index: {}\".format( prediction_index))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mALukMDHzpy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "71647ef4-b679-44c1-de08-fa825d5e2de6"
      },
      "source": [
        "test_isolado = np.asarray(test_isolado)\n",
        "print(test_isolado)\n",
        "\n",
        "#36\t28,5766758494031\t86\t4,345\t0,921719333333333\t15,1248\t8,6\t9,1539\t534,224\n",
        "\n",
        "atila = [[36,28.5766758494031,86,4.345,0.921719333333333,15.1248,\t8.6,9.1539,534.224]]\n",
        "\n",
        "print(atila)\n",
        "atila = scaler.transform(atila)\n",
        "print(atila)\n",
        "atila = atila.reshape(1,9,1)\n",
        "print(atila)\n",
        "atila_y = 0\n",
        "atila\n",
        "predict(model, atila, atila_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 78.        25.3       60.         3.508      0.519184   6.633\n",
            "  10.567295   4.6638   209.749   ]\n",
            "[[36, 28.5766758494031, 86, 4.345, 0.921719333333333, 15.1248, 8.6, 9.1539, 534.224]]\n",
            "[[-1.31056198  0.24332226 -0.52978825 -0.52215874 -0.43538139 -0.57787387\n",
            "  -0.27215301 -0.47680335  0.00608853]]\n",
            "[[[-1.31056198]\n",
            "  [ 0.24332226]\n",
            "  [-0.52978825]\n",
            "  [-0.52215874]\n",
            "  [-0.43538139]\n",
            "  [-0.57787387]\n",
            "  [-0.27215301]\n",
            "  [-0.47680335]\n",
            "  [ 0.00608853]]]\n",
            "[[0.8379102  0.16208978]]\n",
            " Prediction index: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhHDgYI44-TM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ce85892d-497c-48fc-bc49-dae318e3d37d"
      },
      "source": [
        "x_aux = x_test[2]\n",
        "x_aux\n",
        "print(x_aux)\n",
        "\n",
        "x_aux = x_aux.reshape(1,9, 1)\n",
        "print(x_aux)\n",
        "\n",
        "y_aux = y_test[2]\n",
        "print(y_aux)\n",
        "predict(model, x_aux, y_aux)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.28105086e+00]\n",
            " [-4.28229801e-01]\n",
            " [-1.82290405e+00]\n",
            " [-6.06770235e-01]\n",
            " [-5.44698050e-01]\n",
            " [-1.02439202e+00]\n",
            " [-5.72133378e-04]\n",
            " [-8.29524843e-01]\n",
            " [-9.21193560e-01]]\n",
            "[[[ 1.28105086e+00]\n",
            "  [-4.28229801e-01]\n",
            "  [-1.82290405e+00]\n",
            "  [-6.06770235e-01]\n",
            "  [-5.44698050e-01]\n",
            "  [-1.02439202e+00]\n",
            "  [-5.72133378e-04]\n",
            "  [-8.29524843e-01]\n",
            "  [-9.21193560e-01]]]\n",
            "0\n",
            "[[0.99015146 0.00984849]]\n",
            " Prediction index: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}